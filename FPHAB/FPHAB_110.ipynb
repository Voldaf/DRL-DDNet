{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import sys \n",
    "# sys.path\n",
    "# sys.path.append(\"~/anaconda3/lib/python3.8/site-packages/tensorflow_estimator\")\n",
    "# sys.path\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 20 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 45 # the number of class\n",
    "        self.feat_d = 190\n",
    "        self.filters = 64\n",
    "        self.data_dir = '/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/data/FPHAB/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
    "    x = tf.image.resize(x,size=[H,W]) \n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=20,joint_d=3,feat_d=231,filters=64):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,3)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,1)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,3) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,1) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DD_Net(frame_l=32,joint_n=20,joint_d=3,feat_d=231,clc_num=45,filters=64):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))   #32,231\n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) #32,22,3\n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters) #32,22,3,231,16\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD_Net = build_DD_Net(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters) #32,22,3,231,14,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 190)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 20, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1733376     M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65536       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 128)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 45)           5805        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,822,125\n",
      "Trainable params: 1,816,493\n",
      "Non-trainable params: 5,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DD_Net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 600/600 [00:05<00:00, 114.15it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,20,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['label'][i]-1] = 1  \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  #(1960,32,231)\n",
    "X_1 = np.stack(X_1)  #(1960,32,22,3)\n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 575/575 [00:05<00:00, 112.22it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,20,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 575 samples\n",
      "Epoch 1/600\n",
      "600/600 [==============================] - 6s 11ms/step - loss: 4.4882 - accuracy: 0.0200 - val_loss: 5.2804 - val_accuracy: 0.0261\n",
      "Epoch 2/600\n",
      "600/600 [==============================] - 0s 183us/step - loss: 4.1483 - accuracy: 0.0367 - val_loss: 5.5843 - val_accuracy: 0.0243\n",
      "Epoch 3/600\n",
      "600/600 [==============================] - 0s 139us/step - loss: 3.9727 - accuracy: 0.0600 - val_loss: 6.0138 - val_accuracy: 0.0470\n",
      "Epoch 4/600\n",
      "600/600 [==============================] - 0s 135us/step - loss: 3.6822 - accuracy: 0.1117 - val_loss: 6.3721 - val_accuracy: 0.0261\n",
      "Epoch 5/600\n",
      "600/600 [==============================] - 0s 135us/step - loss: 3.7117 - accuracy: 0.0917 - val_loss: 6.9146 - val_accuracy: 0.0209\n",
      "Epoch 6/600\n",
      "600/600 [==============================] - 0s 134us/step - loss: 3.5343 - accuracy: 0.0883 - val_loss: 7.0025 - val_accuracy: 0.0209\n",
      "Epoch 7/600\n",
      "600/600 [==============================] - 0s 131us/step - loss: 3.4528 - accuracy: 0.1317 - val_loss: 6.8467 - val_accuracy: 0.0209\n",
      "Epoch 8/600\n",
      "600/600 [==============================] - 0s 138us/step - loss: 3.3391 - accuracy: 0.1467 - val_loss: 6.3645 - val_accuracy: 0.0209\n",
      "Epoch 9/600\n",
      "600/600 [==============================] - 0s 139us/step - loss: 3.2729 - accuracy: 0.1700 - val_loss: 5.8257 - val_accuracy: 0.0243\n",
      "Epoch 10/600\n",
      "600/600 [==============================] - 0s 148us/step - loss: 3.1685 - accuracy: 0.1850 - val_loss: 5.4003 - val_accuracy: 0.0400\n",
      "Epoch 11/600\n",
      "600/600 [==============================] - 0s 148us/step - loss: 3.0399 - accuracy: 0.1967 - val_loss: 5.0275 - val_accuracy: 0.0452\n",
      "Epoch 12/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 2.9390 - accuracy: 0.2217 - val_loss: 4.7156 - val_accuracy: 0.0557\n",
      "Epoch 13/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 2.9642 - accuracy: 0.2283 - val_loss: 4.4699 - val_accuracy: 0.0678\n",
      "Epoch 14/600\n",
      "600/600 [==============================] - 0s 145us/step - loss: 2.7998 - accuracy: 0.2700 - val_loss: 4.2285 - val_accuracy: 0.0730\n",
      "Epoch 15/600\n",
      "600/600 [==============================] - 0s 151us/step - loss: 2.7883 - accuracy: 0.3033 - val_loss: 4.0232 - val_accuracy: 0.0783\n",
      "Epoch 16/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 2.6440 - accuracy: 0.3217 - val_loss: 3.8387 - val_accuracy: 0.0939\n",
      "Epoch 17/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 2.5685 - accuracy: 0.3183 - val_loss: 3.6906 - val_accuracy: 0.0991\n",
      "Epoch 18/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 2.5846 - accuracy: 0.3350 - val_loss: 3.6005 - val_accuracy: 0.1165\n",
      "Epoch 19/600\n",
      "600/600 [==============================] - 0s 151us/step - loss: 2.4711 - accuracy: 0.3600 - val_loss: 3.5463 - val_accuracy: 0.1374\n",
      "Epoch 20/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 2.4790 - accuracy: 0.3617 - val_loss: 3.5172 - val_accuracy: 0.1426\n",
      "Epoch 21/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 2.4011 - accuracy: 0.3633 - val_loss: 3.5230 - val_accuracy: 0.1513\n",
      "Epoch 22/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 2.2656 - accuracy: 0.4283 - val_loss: 3.5289 - val_accuracy: 0.1722\n",
      "Epoch 23/600\n",
      "600/600 [==============================] - 0s 149us/step - loss: 2.2973 - accuracy: 0.3900 - val_loss: 3.5213 - val_accuracy: 0.1913\n",
      "Epoch 24/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 2.1559 - accuracy: 0.4383 - val_loss: 3.4905 - val_accuracy: 0.2035\n",
      "Epoch 25/600\n",
      "600/600 [==============================] - 0s 147us/step - loss: 2.0769 - accuracy: 0.4717 - val_loss: 3.4502 - val_accuracy: 0.2139\n",
      "Epoch 26/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 2.0766 - accuracy: 0.4817 - val_loss: 3.4004 - val_accuracy: 0.2261\n",
      "Epoch 27/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 2.0432 - accuracy: 0.4867 - val_loss: 3.3384 - val_accuracy: 0.2417\n",
      "Epoch 28/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 1.9682 - accuracy: 0.5417 - val_loss: 3.2859 - val_accuracy: 0.2417\n",
      "Epoch 29/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 1.8410 - accuracy: 0.5633 - val_loss: 3.2379 - val_accuracy: 0.2417\n",
      "Epoch 30/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 1.8571 - accuracy: 0.5617 - val_loss: 3.2226 - val_accuracy: 0.2435\n",
      "Epoch 31/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 1.8018 - accuracy: 0.5700 - val_loss: 3.2015 - val_accuracy: 0.2417\n",
      "Epoch 32/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 1.7648 - accuracy: 0.5783 - val_loss: 3.2255 - val_accuracy: 0.2330\n",
      "Epoch 33/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 1.7884 - accuracy: 0.5600 - val_loss: 3.2583 - val_accuracy: 0.2278\n",
      "Epoch 34/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 1.6822 - accuracy: 0.6017 - val_loss: 3.2792 - val_accuracy: 0.2243\n",
      "Epoch 35/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 1.6286 - accuracy: 0.6217 - val_loss: 3.2545 - val_accuracy: 0.2209\n",
      "Epoch 36/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 1.6130 - accuracy: 0.6350 - val_loss: 3.1999 - val_accuracy: 0.2296\n",
      "Epoch 37/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 1.6087 - accuracy: 0.6200 - val_loss: 3.1390 - val_accuracy: 0.2435\n",
      "Epoch 38/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 1.5145 - accuracy: 0.6517 - val_loss: 3.1068 - val_accuracy: 0.2313\n",
      "Epoch 39/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 1.4891 - accuracy: 0.6667 - val_loss: 3.0831 - val_accuracy: 0.2330\n",
      "Epoch 40/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 1.4799 - accuracy: 0.6733 - val_loss: 3.0597 - val_accuracy: 0.2330\n",
      "Epoch 41/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 1.3962 - accuracy: 0.6833 - val_loss: 3.0348 - val_accuracy: 0.2330\n",
      "Epoch 42/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 1.3854 - accuracy: 0.6783 - val_loss: 3.0148 - val_accuracy: 0.2417\n",
      "Epoch 43/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 1.3469 - accuracy: 0.6983 - val_loss: 2.9821 - val_accuracy: 0.2452\n",
      "Epoch 44/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 1.2534 - accuracy: 0.7450 - val_loss: 2.9496 - val_accuracy: 0.2609\n",
      "Epoch 45/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 1.2728 - accuracy: 0.7217 - val_loss: 2.9091 - val_accuracy: 0.2609\n",
      "Epoch 46/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 1.2249 - accuracy: 0.7483 - val_loss: 2.8811 - val_accuracy: 0.2661\n",
      "Epoch 47/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 1.1809 - accuracy: 0.7550 - val_loss: 2.8558 - val_accuracy: 0.2817\n",
      "Epoch 48/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 1.1761 - accuracy: 0.7767 - val_loss: 2.8128 - val_accuracy: 0.3009\n",
      "Epoch 49/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 1.1456 - accuracy: 0.7950 - val_loss: 2.7571 - val_accuracy: 0.3357\n",
      "Epoch 50/600\n",
      "600/600 [==============================] - 0s 151us/step - loss: 1.1149 - accuracy: 0.7900 - val_loss: 2.7151 - val_accuracy: 0.3809\n",
      "Epoch 51/600\n",
      "600/600 [==============================] - 0s 168us/step - loss: 1.0725 - accuracy: 0.7933 - val_loss: 2.6764 - val_accuracy: 0.3826\n",
      "Epoch 52/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 1.0780 - accuracy: 0.7783 - val_loss: 2.6505 - val_accuracy: 0.4017\n",
      "Epoch 53/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 1.0669 - accuracy: 0.7983 - val_loss: 2.6430 - val_accuracy: 0.4087\n",
      "Epoch 54/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 1.0056 - accuracy: 0.8033 - val_loss: 2.6298 - val_accuracy: 0.4191\n",
      "Epoch 55/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.9449 - accuracy: 0.8150 - val_loss: 2.6305 - val_accuracy: 0.4052\n",
      "Epoch 56/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.9308 - accuracy: 0.8350 - val_loss: 2.6423 - val_accuracy: 0.4035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.9129 - accuracy: 0.8383 - val_loss: 2.6514 - val_accuracy: 0.3930\n",
      "Epoch 58/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.8763 - accuracy: 0.8333 - val_loss: 2.6392 - val_accuracy: 0.3896\n",
      "Epoch 59/600\n",
      "600/600 [==============================] - 0s 144us/step - loss: 0.8713 - accuracy: 0.8467 - val_loss: 2.6020 - val_accuracy: 0.4104\n",
      "Epoch 60/600\n",
      "600/600 [==============================] - 0s 148us/step - loss: 0.8490 - accuracy: 0.8667 - val_loss: 2.5614 - val_accuracy: 0.4313\n",
      "Epoch 61/600\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.7998 - accuracy: 0.8600 - val_loss: 2.5047 - val_accuracy: 0.4817\n",
      "Epoch 62/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.8359 - accuracy: 0.8617 - val_loss: 2.4566 - val_accuracy: 0.5130\n",
      "Epoch 63/600\n",
      "600/600 [==============================] - 0s 146us/step - loss: 0.7277 - accuracy: 0.8783 - val_loss: 2.4441 - val_accuracy: 0.5078\n",
      "Epoch 64/600\n",
      "600/600 [==============================] - 0s 148us/step - loss: 0.7678 - accuracy: 0.8617 - val_loss: 2.4279 - val_accuracy: 0.5009\n",
      "Epoch 65/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.7459 - accuracy: 0.8583 - val_loss: 2.4346 - val_accuracy: 0.4748\n",
      "Epoch 66/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 0.7136 - accuracy: 0.8900 - val_loss: 2.4439 - val_accuracy: 0.4417\n",
      "Epoch 67/600\n",
      "600/600 [==============================] - 0s 172us/step - loss: 0.6969 - accuracy: 0.8733 - val_loss: 2.4500 - val_accuracy: 0.4261\n",
      "Epoch 68/600\n",
      "600/600 [==============================] - 0s 146us/step - loss: 0.6714 - accuracy: 0.8983 - val_loss: 2.4766 - val_accuracy: 0.4087\n",
      "Epoch 69/600\n",
      "600/600 [==============================] - 0s 143us/step - loss: 0.7033 - accuracy: 0.9000 - val_loss: 2.4971 - val_accuracy: 0.3930\n",
      "Epoch 70/600\n",
      "600/600 [==============================] - 0s 149us/step - loss: 0.6849 - accuracy: 0.8867 - val_loss: 2.5340 - val_accuracy: 0.3861\n",
      "Epoch 71/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.6697 - accuracy: 0.9000 - val_loss: 2.5553 - val_accuracy: 0.3948\n",
      "Epoch 72/600\n",
      "600/600 [==============================] - 0s 180us/step - loss: 0.6058 - accuracy: 0.8917 - val_loss: 2.5642 - val_accuracy: 0.4104\n",
      "Epoch 73/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.5946 - accuracy: 0.9000 - val_loss: 2.5279 - val_accuracy: 0.4174\n",
      "Epoch 74/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.6001 - accuracy: 0.9050 - val_loss: 2.4786 - val_accuracy: 0.4365\n",
      "Epoch 75/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.5535 - accuracy: 0.9283 - val_loss: 2.4335 - val_accuracy: 0.4574\n",
      "Epoch 76/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 0.5552 - accuracy: 0.9250 - val_loss: 2.3949 - val_accuracy: 0.4643\n",
      "Epoch 77/600\n",
      "600/600 [==============================] - 0s 149us/step - loss: 0.5599 - accuracy: 0.9150 - val_loss: 2.3677 - val_accuracy: 0.4678\n",
      "Epoch 78/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.5753 - accuracy: 0.9100 - val_loss: 2.3514 - val_accuracy: 0.4678\n",
      "Epoch 79/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 0.5349 - accuracy: 0.9133 - val_loss: 2.3434 - val_accuracy: 0.4609\n",
      "Epoch 80/600\n",
      "600/600 [==============================] - 0s 151us/step - loss: 0.5137 - accuracy: 0.9233 - val_loss: 2.3459 - val_accuracy: 0.4487\n",
      "Epoch 81/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.5341 - accuracy: 0.9250 - val_loss: 2.3510 - val_accuracy: 0.4365\n",
      "Epoch 82/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.5185 - accuracy: 0.9367 - val_loss: 2.3472 - val_accuracy: 0.4348\n",
      "Epoch 83/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.4753 - accuracy: 0.9450 - val_loss: 2.3286 - val_accuracy: 0.4278\n",
      "Epoch 84/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.4834 - accuracy: 0.9333 - val_loss: 2.3031 - val_accuracy: 0.4296\n",
      "Epoch 85/600\n",
      "600/600 [==============================] - 0s 163us/step - loss: 0.4273 - accuracy: 0.9550 - val_loss: 2.2480 - val_accuracy: 0.4609\n",
      "Epoch 86/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.4545 - accuracy: 0.9500 - val_loss: 2.1899 - val_accuracy: 0.4783\n",
      "Epoch 87/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.3979 - accuracy: 0.9617 - val_loss: 2.1389 - val_accuracy: 0.5043\n",
      "Epoch 88/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.4010 - accuracy: 0.9550 - val_loss: 2.0833 - val_accuracy: 0.5217\n",
      "Epoch 89/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.4401 - accuracy: 0.9417 - val_loss: 2.0480 - val_accuracy: 0.5357\n",
      "Epoch 90/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.4121 - accuracy: 0.9550 - val_loss: 2.0195 - val_accuracy: 0.5461\n",
      "Epoch 91/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.4103 - accuracy: 0.9500 - val_loss: 2.0234 - val_accuracy: 0.5565\n",
      "Epoch 92/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.3803 - accuracy: 0.9683 - val_loss: 2.0032 - val_accuracy: 0.5565\n",
      "Epoch 93/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.3832 - accuracy: 0.9567 - val_loss: 1.9908 - val_accuracy: 0.5652\n",
      "Epoch 94/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.3560 - accuracy: 0.9567 - val_loss: 1.9847 - val_accuracy: 0.5670\n",
      "Epoch 95/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.3328 - accuracy: 0.9767 - val_loss: 1.9708 - val_accuracy: 0.5670\n",
      "Epoch 96/600\n",
      "600/600 [==============================] - 0s 166us/step - loss: 0.3723 - accuracy: 0.9633 - val_loss: 1.9434 - val_accuracy: 0.5704\n",
      "Epoch 97/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.3768 - accuracy: 0.9583 - val_loss: 1.9022 - val_accuracy: 0.5878\n",
      "Epoch 98/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.3604 - accuracy: 0.9600 - val_loss: 1.8613 - val_accuracy: 0.6035\n",
      "Epoch 99/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.3123 - accuracy: 0.9800 - val_loss: 1.8424 - val_accuracy: 0.6104\n",
      "Epoch 100/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.3386 - accuracy: 0.9750 - val_loss: 1.8618 - val_accuracy: 0.6000\n",
      "Epoch 101/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.3311 - accuracy: 0.9717 - val_loss: 1.9330 - val_accuracy: 0.5670\n",
      "Epoch 102/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.3041 - accuracy: 0.9817 - val_loss: 1.9638 - val_accuracy: 0.5443\n",
      "Epoch 103/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.3037 - accuracy: 0.9700 - val_loss: 2.0102 - val_accuracy: 0.5304\n",
      "Epoch 104/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2834 - accuracy: 0.9617 - val_loss: 2.0216 - val_accuracy: 0.5200\n",
      "Epoch 105/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.2811 - accuracy: 0.9750 - val_loss: 2.0140 - val_accuracy: 0.5287\n",
      "Epoch 106/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 0.2915 - accuracy: 0.9783 - val_loss: 1.9973 - val_accuracy: 0.5304\n",
      "Epoch 107/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.2808 - accuracy: 0.9683 - val_loss: 1.9437 - val_accuracy: 0.5478\n",
      "Epoch 108/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2641 - accuracy: 0.9850 - val_loss: 1.8583 - val_accuracy: 0.5791\n",
      "Epoch 109/600\n",
      "600/600 [==============================] - 0s 163us/step - loss: 0.2747 - accuracy: 0.9867 - val_loss: 1.7559 - val_accuracy: 0.6070\n",
      "Epoch 110/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.2769 - accuracy: 0.9833 - val_loss: 1.6644 - val_accuracy: 0.6365\n",
      "Epoch 111/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.2889 - accuracy: 0.9667 - val_loss: 1.6353 - val_accuracy: 0.6435\n",
      "Epoch 112/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.2581 - accuracy: 0.9800 - val_loss: 1.6252 - val_accuracy: 0.6522\n",
      "Epoch 113/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 173us/step - loss: 0.2573 - accuracy: 0.9767 - val_loss: 1.6259 - val_accuracy: 0.6487\n",
      "Epoch 114/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 0.2646 - accuracy: 0.9767 - val_loss: 1.6058 - val_accuracy: 0.6452\n",
      "Epoch 115/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.2520 - accuracy: 0.9833 - val_loss: 1.6107 - val_accuracy: 0.6400\n",
      "Epoch 116/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.2446 - accuracy: 0.9800 - val_loss: 1.6177 - val_accuracy: 0.6365\n",
      "Epoch 117/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.2441 - accuracy: 0.9700 - val_loss: 1.6093 - val_accuracy: 0.6383\n",
      "Epoch 118/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.2410 - accuracy: 0.9783 - val_loss: 1.5985 - val_accuracy: 0.6348\n",
      "Epoch 119/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.2344 - accuracy: 0.9833 - val_loss: 1.5714 - val_accuracy: 0.6452\n",
      "Epoch 120/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.2273 - accuracy: 0.9817 - val_loss: 1.4938 - val_accuracy: 0.6591\n",
      "Epoch 121/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2272 - accuracy: 0.9833 - val_loss: 1.4242 - val_accuracy: 0.6713\n",
      "Epoch 122/600\n",
      "600/600 [==============================] - 0s 166us/step - loss: 0.2341 - accuracy: 0.9750 - val_loss: 1.3544 - val_accuracy: 0.6904\n",
      "Epoch 123/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2367 - accuracy: 0.9733 - val_loss: 1.2815 - val_accuracy: 0.6991\n",
      "Epoch 124/600\n",
      "600/600 [==============================] - 0s 172us/step - loss: 0.2358 - accuracy: 0.9833 - val_loss: 1.2177 - val_accuracy: 0.7078\n",
      "Epoch 125/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.2080 - accuracy: 0.9833 - val_loss: 1.1737 - val_accuracy: 0.7148\n",
      "Epoch 126/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1904 - accuracy: 0.9800 - val_loss: 1.1093 - val_accuracy: 0.7374\n",
      "Epoch 127/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2501 - accuracy: 0.9700 - val_loss: 1.1118 - val_accuracy: 0.7339\n",
      "Epoch 128/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.2364 - accuracy: 0.9817 - val_loss: 1.1415 - val_accuracy: 0.7443\n",
      "Epoch 129/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.2101 - accuracy: 0.9867 - val_loss: 1.2158 - val_accuracy: 0.7183\n",
      "Epoch 130/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1950 - accuracy: 0.9900 - val_loss: 1.2935 - val_accuracy: 0.6974\n",
      "Epoch 131/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1916 - accuracy: 0.9833 - val_loss: 1.3628 - val_accuracy: 0.6887\n",
      "Epoch 132/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1880 - accuracy: 0.9867 - val_loss: 1.3700 - val_accuracy: 0.6922\n",
      "Epoch 133/600\n",
      "600/600 [==============================] - 0s 163us/step - loss: 0.2039 - accuracy: 0.9850 - val_loss: 1.3560 - val_accuracy: 0.6939\n",
      "Epoch 134/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.2018 - accuracy: 0.9833 - val_loss: 1.3102 - val_accuracy: 0.6991\n",
      "Epoch 135/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.1896 - accuracy: 0.9900 - val_loss: 1.2497 - val_accuracy: 0.7078\n",
      "Epoch 136/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1721 - accuracy: 0.9867 - val_loss: 1.1972 - val_accuracy: 0.7183\n",
      "Epoch 137/600\n",
      "600/600 [==============================] - 0s 151us/step - loss: 0.2066 - accuracy: 0.9750 - val_loss: 1.1697 - val_accuracy: 0.7270\n",
      "Epoch 138/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1900 - accuracy: 0.9850 - val_loss: 1.1532 - val_accuracy: 0.7252\n",
      "Epoch 139/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1737 - accuracy: 0.9950 - val_loss: 1.1468 - val_accuracy: 0.7287\n",
      "Epoch 140/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.1792 - accuracy: 0.9917 - val_loss: 1.1420 - val_accuracy: 0.7252\n",
      "Epoch 141/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.1684 - accuracy: 0.9950 - val_loss: 1.1363 - val_accuracy: 0.7270\n",
      "Epoch 142/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1749 - accuracy: 0.9900 - val_loss: 1.1496 - val_accuracy: 0.7270\n",
      "Epoch 143/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1878 - accuracy: 0.9817 - val_loss: 1.1659 - val_accuracy: 0.7235\n",
      "Epoch 144/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1901 - accuracy: 0.9867 - val_loss: 1.1882 - val_accuracy: 0.7130\n",
      "Epoch 145/600\n",
      "600/600 [==============================] - 0s 163us/step - loss: 0.1802 - accuracy: 0.9867 - val_loss: 1.1840 - val_accuracy: 0.7235\n",
      "Epoch 146/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1597 - accuracy: 0.9867 - val_loss: 1.1718 - val_accuracy: 0.7252\n",
      "Epoch 147/600\n",
      "600/600 [==============================] - 0s 164us/step - loss: 0.1560 - accuracy: 0.9900 - val_loss: 1.1586 - val_accuracy: 0.7322\n",
      "Epoch 148/600\n",
      "600/600 [==============================] - 0s 165us/step - loss: 0.1802 - accuracy: 0.9883 - val_loss: 1.1455 - val_accuracy: 0.7357\n",
      "Epoch 149/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1681 - accuracy: 0.9867 - val_loss: 1.1218 - val_accuracy: 0.7391\n",
      "Epoch 150/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1616 - accuracy: 0.9950 - val_loss: 1.0888 - val_accuracy: 0.7426\n",
      "Epoch 151/600\n",
      "600/600 [==============================] - 0s 152us/step - loss: 0.1659 - accuracy: 0.9917 - val_loss: 1.0512 - val_accuracy: 0.7530\n",
      "Epoch 152/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1682 - accuracy: 0.9883 - val_loss: 1.0069 - val_accuracy: 0.7617\n",
      "Epoch 153/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1619 - accuracy: 0.9900 - val_loss: 0.9823 - val_accuracy: 0.7670\n",
      "Epoch 154/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1538 - accuracy: 0.9967 - val_loss: 0.9576 - val_accuracy: 0.7704\n",
      "Epoch 155/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1467 - accuracy: 0.9917 - val_loss: 0.9320 - val_accuracy: 0.7809\n",
      "Epoch 156/600\n",
      "600/600 [==============================] - 0s 164us/step - loss: 0.1716 - accuracy: 0.9783 - val_loss: 0.9082 - val_accuracy: 0.7843\n",
      "Epoch 157/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1527 - accuracy: 0.9867 - val_loss: 0.8859 - val_accuracy: 0.7861\n",
      "Epoch 158/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1675 - accuracy: 0.9883 - val_loss: 0.8618 - val_accuracy: 0.7948\n",
      "Epoch 159/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1557 - accuracy: 0.9850 - val_loss: 0.8379 - val_accuracy: 0.7983\n",
      "Epoch 160/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1402 - accuracy: 0.9933 - val_loss: 0.8207 - val_accuracy: 0.8000\n",
      "Epoch 161/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1636 - accuracy: 0.9933 - val_loss: 0.8090 - val_accuracy: 0.8035\n",
      "Epoch 162/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1582 - accuracy: 0.9900 - val_loss: 0.7966 - val_accuracy: 0.8052\n",
      "Epoch 163/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1589 - accuracy: 0.9917 - val_loss: 0.7831 - val_accuracy: 0.8052\n",
      "Epoch 164/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1546 - accuracy: 0.9933 - val_loss: 0.7659 - val_accuracy: 0.8035\n",
      "Epoch 165/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1652 - accuracy: 0.9883 - val_loss: 0.7493 - val_accuracy: 0.8104\n",
      "Epoch 166/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1481 - accuracy: 0.9917 - val_loss: 0.7339 - val_accuracy: 0.8174\n",
      "Epoch 167/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1555 - accuracy: 0.9900 - val_loss: 0.7196 - val_accuracy: 0.8243\n",
      "Epoch 168/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1612 - accuracy: 0.9917 - val_loss: 0.7070 - val_accuracy: 0.8243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/600\n",
      "600/600 [==============================] - 0s 169us/step - loss: 0.1580 - accuracy: 0.9883 - val_loss: 0.6949 - val_accuracy: 0.8261\n",
      "Epoch 170/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1484 - accuracy: 0.9933 - val_loss: 0.6838 - val_accuracy: 0.8243\n",
      "Epoch 171/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1463 - accuracy: 0.9933 - val_loss: 0.6751 - val_accuracy: 0.8261\n",
      "Epoch 172/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1350 - accuracy: 0.9933 - val_loss: 0.6665 - val_accuracy: 0.8330\n",
      "Epoch 173/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1450 - accuracy: 0.9900 - val_loss: 0.6585 - val_accuracy: 0.8383\n",
      "Epoch 174/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1598 - accuracy: 0.9950 - val_loss: 0.6517 - val_accuracy: 0.8400\n",
      "Epoch 175/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1652 - accuracy: 0.9867 - val_loss: 0.6462 - val_accuracy: 0.8365\n",
      "Epoch 176/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1348 - accuracy: 0.9950 - val_loss: 0.6398 - val_accuracy: 0.8383\n",
      "Epoch 177/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1498 - accuracy: 0.9900 - val_loss: 0.6337 - val_accuracy: 0.8400\n",
      "Epoch 178/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1579 - accuracy: 0.9917 - val_loss: 0.6270 - val_accuracy: 0.8417\n",
      "Epoch 179/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1550 - accuracy: 0.9900 - val_loss: 0.6214 - val_accuracy: 0.8417\n",
      "Epoch 180/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1425 - accuracy: 0.9917 - val_loss: 0.6148 - val_accuracy: 0.8435\n",
      "Epoch 181/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1458 - accuracy: 0.9883 - val_loss: 0.6089 - val_accuracy: 0.8435\n",
      "Epoch 182/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1622 - accuracy: 0.9883 - val_loss: 0.6015 - val_accuracy: 0.8470\n",
      "Epoch 183/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1524 - accuracy: 0.9917 - val_loss: 0.5949 - val_accuracy: 0.8522\n",
      "Epoch 184/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1415 - accuracy: 1.0000 - val_loss: 0.5876 - val_accuracy: 0.8504\n",
      "Epoch 185/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1279 - accuracy: 0.9950 - val_loss: 0.5810 - val_accuracy: 0.8504\n",
      "Epoch 186/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1395 - accuracy: 0.9933 - val_loss: 0.5741 - val_accuracy: 0.8557\n",
      "Epoch 187/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1464 - accuracy: 0.9900 - val_loss: 0.5679 - val_accuracy: 0.8609\n",
      "Epoch 188/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 0.1381 - accuracy: 0.9950 - val_loss: 0.5621 - val_accuracy: 0.8626\n",
      "Epoch 189/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1604 - accuracy: 0.9883 - val_loss: 0.5559 - val_accuracy: 0.8643\n",
      "Epoch 190/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1574 - accuracy: 0.9900 - val_loss: 0.5505 - val_accuracy: 0.8626\n",
      "Epoch 191/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1670 - accuracy: 0.9817 - val_loss: 0.5444 - val_accuracy: 0.8643\n",
      "Epoch 192/600\n",
      "600/600 [==============================] - 0s 163us/step - loss: 0.1295 - accuracy: 0.9917 - val_loss: 0.5383 - val_accuracy: 0.8661\n",
      "Epoch 193/600\n",
      "600/600 [==============================] - 0s 162us/step - loss: 0.1450 - accuracy: 0.9900 - val_loss: 0.5320 - val_accuracy: 0.8696\n",
      "Epoch 194/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1357 - accuracy: 0.9900 - val_loss: 0.5256 - val_accuracy: 0.8713\n",
      "Epoch 195/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.1242 - accuracy: 0.9883 - val_loss: 0.5192 - val_accuracy: 0.8713\n",
      "Epoch 196/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1365 - accuracy: 0.9967 - val_loss: 0.5133 - val_accuracy: 0.8713\n",
      "Epoch 197/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1373 - accuracy: 0.9983 - val_loss: 0.5072 - val_accuracy: 0.8713\n",
      "Epoch 198/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1313 - accuracy: 0.9950 - val_loss: 0.5013 - val_accuracy: 0.8730\n",
      "Epoch 199/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1401 - accuracy: 0.9933 - val_loss: 0.4953 - val_accuracy: 0.8765\n",
      "Epoch 200/600\n",
      "600/600 [==============================] - 0s 165us/step - loss: 0.1510 - accuracy: 0.9867 - val_loss: 0.4898 - val_accuracy: 0.8765\n",
      "Epoch 201/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1473 - accuracy: 0.9867 - val_loss: 0.4847 - val_accuracy: 0.8765\n",
      "Epoch 202/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1507 - accuracy: 0.9900 - val_loss: 0.4798 - val_accuracy: 0.8783\n",
      "Epoch 203/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1442 - accuracy: 0.9950 - val_loss: 0.4750 - val_accuracy: 0.8817\n",
      "Epoch 204/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1362 - accuracy: 0.9933 - val_loss: 0.4703 - val_accuracy: 0.8852\n",
      "Epoch 205/600\n",
      "600/600 [==============================] - 0s 161us/step - loss: 0.1460 - accuracy: 0.9867 - val_loss: 0.4659 - val_accuracy: 0.8852\n",
      "Epoch 206/600\n",
      "600/600 [==============================] - 0s 153us/step - loss: 0.1474 - accuracy: 0.9933 - val_loss: 0.4614 - val_accuracy: 0.8852\n",
      "Epoch 207/600\n",
      "600/600 [==============================] - 0s 157us/step - loss: 0.1352 - accuracy: 0.9983 - val_loss: 0.4570 - val_accuracy: 0.8870\n",
      "Epoch 208/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1321 - accuracy: 0.9933 - val_loss: 0.4529 - val_accuracy: 0.8887\n",
      "Epoch 209/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1473 - accuracy: 0.9867 - val_loss: 0.4489 - val_accuracy: 0.8887\n",
      "Epoch 210/600\n",
      "600/600 [==============================] - 0s 159us/step - loss: 0.1325 - accuracy: 0.9933 - val_loss: 0.4450 - val_accuracy: 0.8887\n",
      "Epoch 211/600\n",
      "600/600 [==============================] - 0s 160us/step - loss: 0.1445 - accuracy: 0.9950 - val_loss: 0.4412 - val_accuracy: 0.8887\n",
      "Epoch 212/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1386 - accuracy: 0.9933 - val_loss: 0.4374 - val_accuracy: 0.8887\n",
      "Epoch 213/600\n",
      "600/600 [==============================] - 0s 150us/step - loss: 0.1510 - accuracy: 0.9900 - val_loss: 0.4338 - val_accuracy: 0.8904\n",
      "Epoch 214/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1398 - accuracy: 0.9967 - val_loss: 0.4302 - val_accuracy: 0.8922\n",
      "Epoch 215/600\n",
      "600/600 [==============================] - 0s 156us/step - loss: 0.1420 - accuracy: 0.9867 - val_loss: 0.4268 - val_accuracy: 0.8939\n",
      "Epoch 216/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1471 - accuracy: 0.9950 - val_loss: 0.4233 - val_accuracy: 0.8939\n",
      "Epoch 217/600\n",
      "600/600 [==============================] - 0s 168us/step - loss: 0.1421 - accuracy: 0.9900 - val_loss: 0.4200 - val_accuracy: 0.8939\n",
      "Epoch 218/600\n",
      "600/600 [==============================] - 0s 158us/step - loss: 0.1598 - accuracy: 0.9900 - val_loss: 0.4168 - val_accuracy: 0.8939\n",
      "Epoch 219/600\n",
      "600/600 [==============================] - 0s 154us/step - loss: 0.1502 - accuracy: 0.9917 - val_loss: 0.4136 - val_accuracy: 0.8939\n",
      "Epoch 220/600\n",
      "600/600 [==============================] - 0s 166us/step - loss: 0.1266 - accuracy: 0.9950 - val_loss: 0.4104 - val_accuracy: 0.8939\n",
      "Epoch 221/600\n",
      "600/600 [==============================] - 0s 155us/step - loss: 0.1212 - accuracy: 0.9983 - val_loss: 0.4074 - val_accuracy: 0.8939\n",
      "Epoch 222/600\n",
      "600/600 [==============================] - 0s 165us/step - loss: 0.1501 - accuracy: 0.9850 - val_loss: 0.4043 - val_accuracy: 0.8957\n",
      "Epoch 223/600\n"
     ]
    }
   ],
   "source": [
    "# it may takes several times to reach the reported performance\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_0,X_1],Y,  \n",
    "            batch_size=len(Y),\n",
    "            epochs=600, #600\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_110_test1.png\")\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD_Net.save_weights('weights/coarse_heavy.h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test2.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20  #20\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    \n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,20,3])\n",
    "        p = sampling_frame(p,C)\n",
    "       \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    DD_Net_model = DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test3.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient for frame selection  -------train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuvlckHKrJQs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.nn import Linear,ReLU,CrossEntropyLoss,Sequential,Conv2d,MaxPool2d,Module,Softmax,BatchNorm2d,Dropout\n",
    "from torch.optim import Adam,SGD\n",
    "\n",
    "from numpy.random import default_rng\n",
    "from utils import *\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "gamma = 0.7\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_size, learning_rate=1e-4):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.n_actions = n_actions \n",
    "\n",
    "        self.cnn_layers = Sequential(Conv2d(1, 32, kernel_size=2),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),\n",
    "                                     Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),\n",
    "                                     Conv2d(64, 128, kernel_size=2, padding=1),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),) \n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(7680,3)) #32 8448  #7680\n",
    "#         optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = x.float()\n",
    "        x = self.cnn_layers(x)\n",
    "        # x=x.size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x \n",
    "    \n",
    "    def select_action(self, state):  \n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        pr = self.forward(Variable(state))   \n",
    "        act = np.random.choice(self.n_actions, p=np.squeeze(pr.detach().numpy())) \n",
    "        log_pr = torch.log(pr.squeeze(0)[act]) \n",
    "        return act, log_pr\n",
    "  \n",
    "            \n",
    "def update_policy(policy_network, rewards, log_probs): \n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        count = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + gamma**count * r   \n",
    "            count +=count\n",
    "        discounted_rewards.append(Gt)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-4) \n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    policy_gradient = torch.stack(policy_gradient).sum()\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()\n",
    "\n",
    "\n",
    "def calculate_reward(Probs, Probs_history , true_class): \n",
    "  ## Probs is the outcome of softmax layer from classifier CNN # Probs : N_classes * 1\n",
    "  ## Probs_history i the output of previous iteration\n",
    "  ## true_class is an integer from [1-10]\n",
    "  ## iteration is the number of iterations passed from the beginning\n",
    "  omega = 5 # a measure of how strong are the punishments and stimulations\n",
    "  predicted_class = np.argmax(Probs) + 1\n",
    "  prev_predicted_class = np.argmax(Probs_history) + 1  ## +1 is bcz classes are from 1 to 10\n",
    "  \n",
    "  \n",
    "  if (predicted_class == true_class and not(prev_predicted_class == true_class) ):\n",
    "    reward = omega  ## stimulation\n",
    "  elif ( not(predicted_class == true_class) and (prev_predicted_class == true_class) ):\n",
    "    reward = - omega ## punishment\n",
    "  else:\n",
    "    true_class = int(true_class)\n",
    "    reward = (np.sign(Probs[true_class - 1]  - Probs_history[true_class - 1])) ## -1 is bcz classes are from 1 to 10\n",
    "\n",
    "  return reward   \n",
    " \n",
    "def train(oridata,labels): \n",
    "    n_states=32 \n",
    "    n_actions=3 \n",
    "    # env.seed(random_seed)\n",
    "    policy_net = Policy(n_states, n_actions, 128) \n",
    "    max_episode_num =  1\n",
    "    max_steps = 32  \n",
    "    all_rewards = []\n",
    "\n",
    "    label_new=[]\n",
    "    final_ind=[]\n",
    "    for episode in tqdm(range(max_episode_num)):        \n",
    "        rewards2=[]\n",
    "        x_tr=[]\n",
    "        for v in tqdm(range(len(oridata))):\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "#             x_tr_1=[]\n",
    "            rng = default_rng()\n",
    "            Action=[]\n",
    "            if len(oridata[v])>=32:\n",
    "#                 rng = default_rng()\n",
    "                MM=np.sort(rng.choice(len(oridata[v]),size=32,replace=False))   \n",
    "            else:\n",
    "                MM=np.sort(rng.choice(len(oridata[v]),size=32,replace=True))  \n",
    "            XX=oridata[v] \n",
    "            original_fr=XX.clone().detach() \n",
    "            original_label=labels[v].numpy()\n",
    "            \n",
    "            for steps in range(len(MM)):\n",
    "                state = original_fr[MM[steps]]\n",
    "                state = state.reshape(1, 1, 20, 3)\n",
    "                state = torch.Tensor(state)\n",
    "                action, log_prob = policy_net.select_action(state)\n",
    "                Action.append(action)\n",
    "\n",
    "                if action == 0 :\n",
    "                    if steps == 0:\n",
    "                        a = 0\n",
    "                    else:\n",
    "                        a = math.ceil(((MM[steps - 1]) + MM[steps]) / 2)\n",
    "                    d = min(1, MM[steps] - a)\n",
    "                    MM[steps] = MM[steps] - d\n",
    "                if action == 1:\n",
    "                    MM[steps] = MM[steps]\n",
    "                if action == 2:\n",
    "                    if steps == len(MM) - 1:\n",
    "                        a = len(original_fr)\n",
    "                    else:\n",
    "                        a = math.ceil((MM[steps] + MM[steps + 1]) / 2)\n",
    "                    d = min(1, a - MM[steps] - 1)\n",
    "                    MM[steps] = MM[steps] + d\n",
    "\n",
    "                xm1 = XX[MM]\n",
    "                xmm = xm1\n",
    "\n",
    "                q = np.array(xm1)\n",
    "                q = zoom(q,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "\n",
    "\n",
    "                M = get_CG(q,C)  \n",
    "                q = q.reshape(1,32,20,3)\n",
    "                M = M.reshape(1,32,190)\n",
    "\n",
    "                original_fr = XX\n",
    "                output = DD_Net.predict([M,q])       \n",
    "                prob = output\n",
    "                prediction = np.argmax(output,axis=1)\n",
    "\n",
    "                prob=prob[0]\n",
    "                if (steps == 0):\n",
    "                  reward = 1 if prediction==original_label else -1  \n",
    "                else:\n",
    "                  reward=calculate_reward(prob, Probs_history ,original_label)\n",
    "                Probs_history=prob \n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "            \n",
    "                if steps==len(MM) - 1:\n",
    "                    xm1_step = XX[MM]\n",
    "            x_tr.append(xm1_step)\n",
    "            rewards2.append(np.mean(rewards))\n",
    "            update_policy(policy_net, rewards, log_probs)    \n",
    "            label_new.append(original_label)\n",
    "\n",
    "        x_tr = torch.stack(x_tr)\n",
    "        \n",
    "        if episode==max_episode_num-1:\n",
    "          final_ind.append(Action)\n",
    "            \n",
    "\n",
    "        \n",
    "        R=np.sum((rewards2))\n",
    "        all_rewards.append(R)\n",
    " \n",
    "    return all_rewards,x_tr,policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\",\"rb\"))\n",
    "\n",
    "oridata = []\n",
    "\n",
    "for i in tqdm(range(len(Train['pose']))):\n",
    "    ori = np.copy(Train['pose'][i]).reshape([-1,20,3]) \n",
    "    ori = torch.tensor(ori)\n",
    "    oridata.append(ori)\n",
    "    \n",
    "labels = [int(Train['label'][i]) for i in (range(len(Train['pose'])))]\n",
    "labels = torch.Tensor(labels)\n",
    "labels_original = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew,policy_net=train(oridata,labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "wF3Umjarn2RI",
    "outputId": "ec5ca4b1-6545-4c75-debc-808c4d0993ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_rewards)\n",
    "plt.plot(all_rewards)\n",
    "plt.xlabel('number of epoches')\n",
    "plt.ylabel('the average reward')\n",
    "plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_110_test4.png\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew))):\n",
    "    w_p = np.copy(x_trnew[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it may takes several times to reach the reported performance\n",
    "\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  #history使得训练结果可视化\n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test5.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_2,policy_net=train(x_trnew,labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_rewards)\n",
    "#plt.plot(all_rewards)\n",
    "#plt.xlabel('number of epoches')\n",
    "#plt.ylabel('the average reward')\n",
    "#plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_110_test1.png\")\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_2))):\n",
    "    w_p = np.copy(x_trnew_2[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl, \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test6.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_3,policy_net=train(x_trnew_2,labels_original)\n",
    "print(all_rewards)\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_3))):\n",
    "    w_p = np.copy(x_trnew_3[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test7.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_4,policy_net=train(x_trnew_3,labels_original)\n",
    "print(all_rewards)\n",
    "\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_4))):\n",
    "    w_p = np.copy(x_trnew_4[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test8.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_5,policy_net=train(x_trnew_4,labels_original)\n",
    "print(all_rewards)\n",
    "\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_5))):\n",
    "    w_p = np.copy(x_trnew_5[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_110_test8.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
