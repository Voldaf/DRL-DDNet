{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import sys \n",
    "# sys.path\n",
    "# sys.path.append(\"~/anaconda3/lib/python3.8/site-packages/tensorflow_estimator\")\n",
    "# sys.path\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1,2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 20 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 45 # the number of class\n",
    "        self.feat_d = 190\n",
    "        self.filters = 64\n",
    "        self.data_dir = '/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/data/FPHAB/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
    "    x = tf.image.resize(x,size=[H,W]) \n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=20,joint_d=3,feat_d=231,filters=64):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,3)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,1)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,3) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,1) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DD_Net(frame_l=32,joint_n=20,joint_d=3,feat_d=231,clc_num=45,filters=64):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))   #32,231\n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) #32,22,3\n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters) #32,22,3,231,16\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD_Net = build_DD_Net(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters) #32,22,3,231,14,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 190)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 20, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1733376     M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65536       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 128)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 45)           5805        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,822,125\n",
      "Trainable params: 1,816,493\n",
      "Non-trainable params: 5,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DD_Net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train_310.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test_310.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 783/783 [00:06<00:00, 122.81it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,20,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['label'][i]-1] = 1  \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  #(1960,32,231) ndarray\n",
    "X_1 = np.stack(X_1)  #(1960,32,22,3)\n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 392/392 [00:03<00:00, 114.70it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,20,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 783 samples, validate on 392 samples\n",
      "Epoch 1/600\n",
      "783/783 [==============================] - 6s 8ms/step - loss: 4.4883 - accuracy: 0.0294 - val_loss: 6.0630 - val_accuracy: 0.0357\n",
      "Epoch 2/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 4.2182 - accuracy: 0.0307 - val_loss: 6.0063 - val_accuracy: 0.0357\n",
      "Epoch 3/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 3.9547 - accuracy: 0.0511 - val_loss: 7.6821 - val_accuracy: 0.0255\n",
      "Epoch 4/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 3.7856 - accuracy: 0.0702 - val_loss: 9.6514 - val_accuracy: 0.0255\n",
      "Epoch 5/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 3.6266 - accuracy: 0.0805 - val_loss: 11.2712 - val_accuracy: 0.0255\n",
      "Epoch 6/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 3.4167 - accuracy: 0.1111 - val_loss: 11.8926 - val_accuracy: 0.0536\n",
      "Epoch 7/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 3.3490 - accuracy: 0.1379 - val_loss: 11.9566 - val_accuracy: 0.0485\n",
      "Epoch 8/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 3.3247 - accuracy: 0.1507 - val_loss: 11.7502 - val_accuracy: 0.0434\n",
      "Epoch 9/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 3.2457 - accuracy: 0.1367 - val_loss: 11.4408 - val_accuracy: 0.0357\n",
      "Epoch 10/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 3.1444 - accuracy: 0.1775 - val_loss: 11.1541 - val_accuracy: 0.0357\n",
      "Epoch 11/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 3.0694 - accuracy: 0.1954 - val_loss: 10.7973 - val_accuracy: 0.0357\n",
      "Epoch 12/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 3.0043 - accuracy: 0.2005 - val_loss: 10.0990 - val_accuracy: 0.0332\n",
      "Epoch 13/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 2.9144 - accuracy: 0.2375 - val_loss: 9.0419 - val_accuracy: 0.0306\n",
      "Epoch 14/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 2.7538 - accuracy: 0.2912 - val_loss: 7.9792 - val_accuracy: 0.0281\n",
      "Epoch 15/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 2.7079 - accuracy: 0.2950 - val_loss: 6.8726 - val_accuracy: 0.0383\n",
      "Epoch 16/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 2.7004 - accuracy: 0.2771 - val_loss: 6.0526 - val_accuracy: 0.0536\n",
      "Epoch 17/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 2.6163 - accuracy: 0.3193 - val_loss: 5.4321 - val_accuracy: 0.0638\n",
      "Epoch 18/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 2.5231 - accuracy: 0.3359 - val_loss: 4.9424 - val_accuracy: 0.0714\n",
      "Epoch 19/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 2.4532 - accuracy: 0.3410 - val_loss: 4.5749 - val_accuracy: 0.0740\n",
      "Epoch 20/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 2.4225 - accuracy: 0.3563 - val_loss: 4.2963 - val_accuracy: 0.0765\n",
      "Epoch 21/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 2.3640 - accuracy: 0.3640 - val_loss: 4.0393 - val_accuracy: 0.0816\n",
      "Epoch 22/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 2.3220 - accuracy: 0.3997 - val_loss: 3.8523 - val_accuracy: 0.0893\n",
      "Epoch 23/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 2.1864 - accuracy: 0.4483 - val_loss: 3.7510 - val_accuracy: 0.0995\n",
      "Epoch 24/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 2.1669 - accuracy: 0.4215 - val_loss: 3.6804 - val_accuracy: 0.1199\n",
      "Epoch 25/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 2.1396 - accuracy: 0.4598 - val_loss: 3.6282 - val_accuracy: 0.1301\n",
      "Epoch 26/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 2.0926 - accuracy: 0.4521 - val_loss: 3.5751 - val_accuracy: 0.1378\n",
      "Epoch 27/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 2.0498 - accuracy: 0.4904 - val_loss: 3.5273 - val_accuracy: 0.1429\n",
      "Epoch 28/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 1.9720 - accuracy: 0.5070 - val_loss: 3.5015 - val_accuracy: 0.1403\n",
      "Epoch 29/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 1.9162 - accuracy: 0.5236 - val_loss: 3.4949 - val_accuracy: 0.1352\n",
      "Epoch 30/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 1.9066 - accuracy: 0.5236 - val_loss: 3.5079 - val_accuracy: 0.1327\n",
      "Epoch 31/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 1.8021 - accuracy: 0.5517 - val_loss: 3.5381 - val_accuracy: 0.1276\n",
      "Epoch 32/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 1.7821 - accuracy: 0.5568 - val_loss: 3.5558 - val_accuracy: 0.1276\n",
      "Epoch 33/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 1.7301 - accuracy: 0.5811 - val_loss: 3.5690 - val_accuracy: 0.1327\n",
      "Epoch 34/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 1.7155 - accuracy: 0.6041 - val_loss: 3.5804 - val_accuracy: 0.1327\n",
      "Epoch 35/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 1.6742 - accuracy: 0.5900 - val_loss: 3.5721 - val_accuracy: 0.1199\n",
      "Epoch 36/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 1.6119 - accuracy: 0.6130 - val_loss: 3.5386 - val_accuracy: 0.1224\n",
      "Epoch 37/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 1.5769 - accuracy: 0.6054 - val_loss: 3.4965 - val_accuracy: 0.1301\n",
      "Epoch 38/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 1.5132 - accuracy: 0.6398 - val_loss: 3.4329 - val_accuracy: 0.1429\n",
      "Epoch 39/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 1.4510 - accuracy: 0.6654 - val_loss: 3.3830 - val_accuracy: 0.1454\n",
      "Epoch 40/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 1.4544 - accuracy: 0.6590 - val_loss: 3.3412 - val_accuracy: 0.1531\n",
      "Epoch 41/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 1.4106 - accuracy: 0.6654 - val_loss: 3.3145 - val_accuracy: 0.1582\n",
      "Epoch 42/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 1.3566 - accuracy: 0.6858 - val_loss: 3.2940 - val_accuracy: 0.1709\n",
      "Epoch 43/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 1.3420 - accuracy: 0.7190 - val_loss: 3.2838 - val_accuracy: 0.1735\n",
      "Epoch 44/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 1.3711 - accuracy: 0.6807 - val_loss: 3.2747 - val_accuracy: 0.1735\n",
      "Epoch 45/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 1.2552 - accuracy: 0.7254 - val_loss: 3.2712 - val_accuracy: 0.1786\n",
      "Epoch 46/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 1.2530 - accuracy: 0.7318 - val_loss: 3.2477 - val_accuracy: 0.1964\n",
      "Epoch 47/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 1.1933 - accuracy: 0.7612 - val_loss: 3.2193 - val_accuracy: 0.1964\n",
      "Epoch 48/600\n",
      "783/783 [==============================] - 0s 151us/step - loss: 1.2056 - accuracy: 0.7216 - val_loss: 3.1942 - val_accuracy: 0.2015\n",
      "Epoch 49/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 1.1611 - accuracy: 0.7535 - val_loss: 3.1630 - val_accuracy: 0.1964\n",
      "Epoch 50/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 1.1411 - accuracy: 0.7688 - val_loss: 3.1314 - val_accuracy: 0.1939\n",
      "Epoch 51/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 1.0590 - accuracy: 0.7944 - val_loss: 3.0907 - val_accuracy: 0.2117\n",
      "Epoch 52/600\n",
      "783/783 [==============================] - 0s 159us/step - loss: 1.0780 - accuracy: 0.7663 - val_loss: 3.0456 - val_accuracy: 0.2270\n",
      "Epoch 53/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 1.0581 - accuracy: 0.7803 - val_loss: 2.9727 - val_accuracy: 0.2526\n",
      "Epoch 54/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 1.0279 - accuracy: 0.7944 - val_loss: 2.9010 - val_accuracy: 0.2806\n",
      "Epoch 55/600\n",
      "783/783 [==============================] - 0s 142us/step - loss: 0.9675 - accuracy: 0.8238 - val_loss: 2.8324 - val_accuracy: 0.3112\n",
      "Epoch 56/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.9569 - accuracy: 0.8059 - val_loss: 2.7720 - val_accuracy: 0.3240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.9554 - accuracy: 0.8072 - val_loss: 2.7101 - val_accuracy: 0.3393\n",
      "Epoch 58/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.9145 - accuracy: 0.8110 - val_loss: 2.6208 - val_accuracy: 0.3878\n",
      "Epoch 59/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.9028 - accuracy: 0.8314 - val_loss: 2.5261 - val_accuracy: 0.4235\n",
      "Epoch 60/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.8630 - accuracy: 0.8238 - val_loss: 2.4377 - val_accuracy: 0.4694\n",
      "Epoch 61/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.8346 - accuracy: 0.8531 - val_loss: 2.3629 - val_accuracy: 0.4974\n",
      "Epoch 62/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.8153 - accuracy: 0.8633 - val_loss: 2.3144 - val_accuracy: 0.5077\n",
      "Epoch 63/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.7861 - accuracy: 0.8544 - val_loss: 2.2639 - val_accuracy: 0.5204\n",
      "Epoch 64/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.7862 - accuracy: 0.8519 - val_loss: 2.1972 - val_accuracy: 0.5408\n",
      "Epoch 65/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.7746 - accuracy: 0.8685 - val_loss: 2.1197 - val_accuracy: 0.5842\n",
      "Epoch 66/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.7803 - accuracy: 0.8685 - val_loss: 2.0683 - val_accuracy: 0.6020\n",
      "Epoch 67/600\n",
      "783/783 [==============================] - 0s 146us/step - loss: 0.7292 - accuracy: 0.8723 - val_loss: 2.0556 - val_accuracy: 0.5995\n",
      "Epoch 68/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.7000 - accuracy: 0.8812 - val_loss: 2.0630 - val_accuracy: 0.6020\n",
      "Epoch 69/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.6797 - accuracy: 0.8902 - val_loss: 2.0733 - val_accuracy: 0.6097\n",
      "Epoch 70/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.7016 - accuracy: 0.8710 - val_loss: 2.1060 - val_accuracy: 0.5791\n",
      "Epoch 71/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.6454 - accuracy: 0.8876 - val_loss: 2.1398 - val_accuracy: 0.5638\n",
      "Epoch 72/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.6253 - accuracy: 0.8940 - val_loss: 2.1600 - val_accuracy: 0.5485\n",
      "Epoch 73/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.6104 - accuracy: 0.9080 - val_loss: 2.1698 - val_accuracy: 0.5332\n",
      "Epoch 74/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.6534 - accuracy: 0.9004 - val_loss: 2.1585 - val_accuracy: 0.5332\n",
      "Epoch 75/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.6093 - accuracy: 0.8927 - val_loss: 2.1547 - val_accuracy: 0.5357\n",
      "Epoch 76/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.5568 - accuracy: 0.9183 - val_loss: 2.1586 - val_accuracy: 0.5510\n",
      "Epoch 77/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.5558 - accuracy: 0.9387 - val_loss: 2.1155 - val_accuracy: 0.5714\n",
      "Epoch 78/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.5277 - accuracy: 0.9246 - val_loss: 2.0679 - val_accuracy: 0.5944\n",
      "Epoch 79/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.5442 - accuracy: 0.9183 - val_loss: 2.0319 - val_accuracy: 0.5867\n",
      "Epoch 80/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.4875 - accuracy: 0.9400 - val_loss: 2.0212 - val_accuracy: 0.5816\n",
      "Epoch 81/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.5234 - accuracy: 0.9170 - val_loss: 2.0316 - val_accuracy: 0.5816\n",
      "Epoch 82/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.4967 - accuracy: 0.9246 - val_loss: 2.0033 - val_accuracy: 0.5765\n",
      "Epoch 83/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.4850 - accuracy: 0.9195 - val_loss: 1.9379 - val_accuracy: 0.5893\n",
      "Epoch 84/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.4458 - accuracy: 0.9387 - val_loss: 1.8641 - val_accuracy: 0.6148\n",
      "Epoch 85/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.4574 - accuracy: 0.9464 - val_loss: 1.8103 - val_accuracy: 0.6301\n",
      "Epoch 86/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.4624 - accuracy: 0.9310 - val_loss: 1.7861 - val_accuracy: 0.6505\n",
      "Epoch 87/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.4499 - accuracy: 0.9451 - val_loss: 1.7857 - val_accuracy: 0.6352\n",
      "Epoch 88/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.4127 - accuracy: 0.9400 - val_loss: 1.7545 - val_accuracy: 0.6378\n",
      "Epoch 89/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.4196 - accuracy: 0.9476 - val_loss: 1.7157 - val_accuracy: 0.6403\n",
      "Epoch 90/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.4037 - accuracy: 0.9540 - val_loss: 1.6881 - val_accuracy: 0.6505\n",
      "Epoch 91/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.3797 - accuracy: 0.9540 - val_loss: 1.6632 - val_accuracy: 0.6429\n",
      "Epoch 92/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.4032 - accuracy: 0.9553 - val_loss: 1.6513 - val_accuracy: 0.6505\n",
      "Epoch 93/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.3735 - accuracy: 0.9668 - val_loss: 1.6374 - val_accuracy: 0.6480\n",
      "Epoch 94/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.4022 - accuracy: 0.9515 - val_loss: 1.6076 - val_accuracy: 0.6658\n",
      "Epoch 95/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.3621 - accuracy: 0.9579 - val_loss: 1.5720 - val_accuracy: 0.6786\n",
      "Epoch 96/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.3696 - accuracy: 0.9476 - val_loss: 1.5056 - val_accuracy: 0.6888\n",
      "Epoch 97/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.3486 - accuracy: 0.9630 - val_loss: 1.4547 - val_accuracy: 0.7066\n",
      "Epoch 98/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.3308 - accuracy: 0.9681 - val_loss: 1.3899 - val_accuracy: 0.7296\n",
      "Epoch 99/600\n",
      "783/783 [==============================] - 0s 109us/step - loss: 0.3335 - accuracy: 0.9591 - val_loss: 1.3321 - val_accuracy: 0.7296\n",
      "Epoch 100/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.3659 - accuracy: 0.9566 - val_loss: 1.3025 - val_accuracy: 0.7372\n",
      "Epoch 101/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.3354 - accuracy: 0.9668 - val_loss: 1.2486 - val_accuracy: 0.7449\n",
      "Epoch 102/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.3118 - accuracy: 0.9655 - val_loss: 1.1971 - val_accuracy: 0.7449\n",
      "Epoch 103/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.2890 - accuracy: 0.9681 - val_loss: 1.1648 - val_accuracy: 0.7577\n",
      "Epoch 104/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.2808 - accuracy: 0.9732 - val_loss: 1.1543 - val_accuracy: 0.7679\n",
      "Epoch 105/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.2887 - accuracy: 0.9681 - val_loss: 1.1804 - val_accuracy: 0.7577\n",
      "Epoch 106/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.2493 - accuracy: 0.9821 - val_loss: 1.2518 - val_accuracy: 0.7321\n",
      "Epoch 107/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.2860 - accuracy: 0.9745 - val_loss: 1.3611 - val_accuracy: 0.7015\n",
      "Epoch 108/600\n",
      "783/783 [==============================] - 0s 109us/step - loss: 0.3129 - accuracy: 0.9668 - val_loss: 1.4350 - val_accuracy: 0.6633\n",
      "Epoch 109/600\n",
      "783/783 [==============================] - 0s 108us/step - loss: 0.2930 - accuracy: 0.9630 - val_loss: 1.4756 - val_accuracy: 0.6403\n",
      "Epoch 110/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2893 - accuracy: 0.9745 - val_loss: 1.4481 - val_accuracy: 0.6505\n",
      "Epoch 111/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2619 - accuracy: 0.9847 - val_loss: 1.3698 - val_accuracy: 0.6658\n",
      "Epoch 112/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2698 - accuracy: 0.9617 - val_loss: 1.3120 - val_accuracy: 0.6735\n",
      "Epoch 113/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 0s 114us/step - loss: 0.2695 - accuracy: 0.9693 - val_loss: 1.2738 - val_accuracy: 0.6837\n",
      "Epoch 114/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2429 - accuracy: 0.9770 - val_loss: 1.2337 - val_accuracy: 0.7092\n",
      "Epoch 115/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.2564 - accuracy: 0.9770 - val_loss: 1.1929 - val_accuracy: 0.7219\n",
      "Epoch 116/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2553 - accuracy: 0.9783 - val_loss: 1.1466 - val_accuracy: 0.7347\n",
      "Epoch 117/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.2337 - accuracy: 0.9847 - val_loss: 1.0896 - val_accuracy: 0.7398\n",
      "Epoch 118/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.2535 - accuracy: 0.9693 - val_loss: 1.0600 - val_accuracy: 0.7577\n",
      "Epoch 119/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.2267 - accuracy: 0.9860 - val_loss: 1.0416 - val_accuracy: 0.7628\n",
      "Epoch 120/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2554 - accuracy: 0.9808 - val_loss: 1.0394 - val_accuracy: 0.7704\n",
      "Epoch 121/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.2205 - accuracy: 0.9860 - val_loss: 1.0392 - val_accuracy: 0.7755\n",
      "Epoch 122/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.2522 - accuracy: 0.9757 - val_loss: 1.0309 - val_accuracy: 0.7781\n",
      "Epoch 123/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.2269 - accuracy: 0.9757 - val_loss: 1.0184 - val_accuracy: 0.7806\n",
      "Epoch 124/600\n",
      "783/783 [==============================] - 0s 151us/step - loss: 0.2336 - accuracy: 0.9821 - val_loss: 0.9970 - val_accuracy: 0.7832\n",
      "Epoch 125/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2244 - accuracy: 0.9872 - val_loss: 0.9754 - val_accuracy: 0.7883\n",
      "Epoch 126/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.2545 - accuracy: 0.9757 - val_loss: 0.9738 - val_accuracy: 0.7857\n",
      "Epoch 127/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.2383 - accuracy: 0.9783 - val_loss: 0.9609 - val_accuracy: 0.7883\n",
      "Epoch 128/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2224 - accuracy: 0.9808 - val_loss: 0.9425 - val_accuracy: 0.7959\n",
      "Epoch 129/600\n",
      "783/783 [==============================] - 0s 108us/step - loss: 0.2236 - accuracy: 0.9847 - val_loss: 0.9234 - val_accuracy: 0.8010\n",
      "Epoch 130/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2245 - accuracy: 0.9808 - val_loss: 0.9059 - val_accuracy: 0.7985\n",
      "Epoch 131/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2107 - accuracy: 0.9770 - val_loss: 0.8942 - val_accuracy: 0.7985\n",
      "Epoch 132/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.2333 - accuracy: 0.9834 - val_loss: 0.8882 - val_accuracy: 0.7985\n",
      "Epoch 133/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2329 - accuracy: 0.9745 - val_loss: 0.8843 - val_accuracy: 0.8061\n",
      "Epoch 134/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2166 - accuracy: 0.9821 - val_loss: 0.8754 - val_accuracy: 0.8061\n",
      "Epoch 135/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.2197 - accuracy: 0.9719 - val_loss: 0.8644 - val_accuracy: 0.8061\n",
      "Epoch 136/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.2043 - accuracy: 0.9808 - val_loss: 0.8513 - val_accuracy: 0.8061\n",
      "Epoch 137/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.2178 - accuracy: 0.9860 - val_loss: 0.8406 - val_accuracy: 0.8061\n",
      "Epoch 138/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.2224 - accuracy: 0.9783 - val_loss: 0.8346 - val_accuracy: 0.8087\n",
      "Epoch 139/600\n",
      "783/783 [==============================] - 0s 109us/step - loss: 0.1784 - accuracy: 0.9872 - val_loss: 0.8309 - val_accuracy: 0.8061\n",
      "Epoch 140/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.2000 - accuracy: 0.9834 - val_loss: 0.8267 - val_accuracy: 0.8138\n",
      "Epoch 141/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2100 - accuracy: 0.9796 - val_loss: 0.8205 - val_accuracy: 0.8163\n",
      "Epoch 142/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2053 - accuracy: 0.9860 - val_loss: 0.8142 - val_accuracy: 0.8087\n",
      "Epoch 143/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.2025 - accuracy: 0.9847 - val_loss: 0.8117 - val_accuracy: 0.8036\n",
      "Epoch 144/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2031 - accuracy: 0.9872 - val_loss: 0.8072 - val_accuracy: 0.8061\n",
      "Epoch 145/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1978 - accuracy: 0.9796 - val_loss: 0.7961 - val_accuracy: 0.8061\n",
      "Epoch 146/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2112 - accuracy: 0.9821 - val_loss: 0.7810 - val_accuracy: 0.8087\n",
      "Epoch 147/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1959 - accuracy: 0.9847 - val_loss: 0.7678 - val_accuracy: 0.8138\n",
      "Epoch 148/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2044 - accuracy: 0.9796 - val_loss: 0.7517 - val_accuracy: 0.8189\n",
      "Epoch 149/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2073 - accuracy: 0.9860 - val_loss: 0.7332 - val_accuracy: 0.8214\n",
      "Epoch 150/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.2030 - accuracy: 0.9821 - val_loss: 0.7142 - val_accuracy: 0.8240\n",
      "Epoch 151/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1990 - accuracy: 0.9821 - val_loss: 0.6972 - val_accuracy: 0.8342\n",
      "Epoch 152/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1876 - accuracy: 0.9872 - val_loss: 0.6802 - val_accuracy: 0.8418\n",
      "Epoch 153/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2044 - accuracy: 0.9821 - val_loss: 0.6677 - val_accuracy: 0.8469\n",
      "Epoch 154/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.2003 - accuracy: 0.9860 - val_loss: 0.6541 - val_accuracy: 0.8520\n",
      "Epoch 155/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1887 - accuracy: 0.9898 - val_loss: 0.6419 - val_accuracy: 0.8546\n",
      "Epoch 156/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1963 - accuracy: 0.9885 - val_loss: 0.6305 - val_accuracy: 0.8546\n",
      "Epoch 157/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2024 - accuracy: 0.9847 - val_loss: 0.6189 - val_accuracy: 0.8571\n",
      "Epoch 158/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1962 - accuracy: 0.9860 - val_loss: 0.6072 - val_accuracy: 0.8648\n",
      "Epoch 159/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1757 - accuracy: 0.9936 - val_loss: 0.5950 - val_accuracy: 0.8648\n",
      "Epoch 160/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.2017 - accuracy: 0.9872 - val_loss: 0.5828 - val_accuracy: 0.8699\n",
      "Epoch 161/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.2075 - accuracy: 0.9783 - val_loss: 0.5721 - val_accuracy: 0.8750\n",
      "Epoch 162/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.2054 - accuracy: 0.9911 - val_loss: 0.5631 - val_accuracy: 0.8750\n",
      "Epoch 163/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.2026 - accuracy: 0.9834 - val_loss: 0.5532 - val_accuracy: 0.8776\n",
      "Epoch 164/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.1861 - accuracy: 0.9911 - val_loss: 0.5432 - val_accuracy: 0.8776\n",
      "Epoch 165/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.2013 - accuracy: 0.9885 - val_loss: 0.5330 - val_accuracy: 0.8801\n",
      "Epoch 166/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1762 - accuracy: 0.9885 - val_loss: 0.5234 - val_accuracy: 0.8801\n",
      "Epoch 167/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1833 - accuracy: 0.9911 - val_loss: 0.5143 - val_accuracy: 0.8878\n",
      "Epoch 168/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1966 - accuracy: 0.9885 - val_loss: 0.5058 - val_accuracy: 0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1933 - accuracy: 0.9796 - val_loss: 0.4982 - val_accuracy: 0.8929\n",
      "Epoch 170/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1947 - accuracy: 0.9860 - val_loss: 0.4903 - val_accuracy: 0.8929\n",
      "Epoch 171/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1896 - accuracy: 0.9872 - val_loss: 0.4829 - val_accuracy: 0.8929\n",
      "Epoch 172/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1791 - accuracy: 0.9872 - val_loss: 0.4756 - val_accuracy: 0.8980\n",
      "Epoch 173/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1871 - accuracy: 0.9885 - val_loss: 0.4683 - val_accuracy: 0.8980\n",
      "Epoch 174/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.2003 - accuracy: 0.9911 - val_loss: 0.4614 - val_accuracy: 0.9031\n",
      "Epoch 175/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1857 - accuracy: 0.9898 - val_loss: 0.4543 - val_accuracy: 0.9031\n",
      "Epoch 176/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1842 - accuracy: 0.9936 - val_loss: 0.4474 - val_accuracy: 0.9056\n",
      "Epoch 177/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2110 - accuracy: 0.9783 - val_loss: 0.4408 - val_accuracy: 0.9056\n",
      "Epoch 178/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1987 - accuracy: 0.9872 - val_loss: 0.4348 - val_accuracy: 0.9056\n",
      "Epoch 179/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1973 - accuracy: 0.9847 - val_loss: 0.4288 - val_accuracy: 0.9082\n",
      "Epoch 180/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1955 - accuracy: 0.9847 - val_loss: 0.4233 - val_accuracy: 0.9107\n",
      "Epoch 181/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1970 - accuracy: 0.9847 - val_loss: 0.4179 - val_accuracy: 0.9107\n",
      "Epoch 182/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1874 - accuracy: 0.9872 - val_loss: 0.4121 - val_accuracy: 0.9107\n",
      "Epoch 183/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1792 - accuracy: 0.9923 - val_loss: 0.4064 - val_accuracy: 0.9107\n",
      "Epoch 184/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1745 - accuracy: 0.9898 - val_loss: 0.4009 - val_accuracy: 0.9107\n",
      "Epoch 185/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1850 - accuracy: 0.9872 - val_loss: 0.3955 - val_accuracy: 0.9133\n",
      "Epoch 186/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1781 - accuracy: 0.9936 - val_loss: 0.3902 - val_accuracy: 0.9133\n",
      "Epoch 187/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1762 - accuracy: 0.9885 - val_loss: 0.3850 - val_accuracy: 0.9158\n",
      "Epoch 188/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1858 - accuracy: 0.9834 - val_loss: 0.3798 - val_accuracy: 0.9184\n",
      "Epoch 189/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1958 - accuracy: 0.9860 - val_loss: 0.3748 - val_accuracy: 0.9209\n",
      "Epoch 190/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1818 - accuracy: 0.9885 - val_loss: 0.3697 - val_accuracy: 0.9209\n",
      "Epoch 191/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1792 - accuracy: 0.9885 - val_loss: 0.3649 - val_accuracy: 0.9209\n",
      "Epoch 192/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1986 - accuracy: 0.9885 - val_loss: 0.3604 - val_accuracy: 0.9209\n",
      "Epoch 193/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.2020 - accuracy: 0.9847 - val_loss: 0.3560 - val_accuracy: 0.9209\n",
      "Epoch 194/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.2001 - accuracy: 0.9860 - val_loss: 0.3516 - val_accuracy: 0.9235\n",
      "Epoch 195/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.2054 - accuracy: 0.9847 - val_loss: 0.3473 - val_accuracy: 0.9235\n",
      "Epoch 196/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1884 - accuracy: 0.9898 - val_loss: 0.3433 - val_accuracy: 0.9235\n",
      "Epoch 197/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.2011 - accuracy: 0.9821 - val_loss: 0.3394 - val_accuracy: 0.9286\n",
      "Epoch 198/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.2039 - accuracy: 0.9872 - val_loss: 0.3355 - val_accuracy: 0.9286\n",
      "Epoch 199/600\n",
      "783/783 [==============================] - 0s 143us/step - loss: 0.2159 - accuracy: 0.9796 - val_loss: 0.3316 - val_accuracy: 0.9286\n",
      "Epoch 200/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1828 - accuracy: 0.9898 - val_loss: 0.3279 - val_accuracy: 0.9286\n",
      "Epoch 201/600\n",
      "783/783 [==============================] - 0s 136us/step - loss: 0.1838 - accuracy: 0.9936 - val_loss: 0.3244 - val_accuracy: 0.9311\n",
      "Epoch 202/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1840 - accuracy: 0.9860 - val_loss: 0.3209 - val_accuracy: 0.9337\n",
      "Epoch 203/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1848 - accuracy: 0.9872 - val_loss: 0.3176 - val_accuracy: 0.9337\n",
      "Epoch 204/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1951 - accuracy: 0.9847 - val_loss: 0.3144 - val_accuracy: 0.9337\n",
      "Epoch 205/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.2078 - accuracy: 0.9821 - val_loss: 0.3111 - val_accuracy: 0.9337\n",
      "Epoch 206/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1892 - accuracy: 0.9847 - val_loss: 0.3081 - val_accuracy: 0.9362\n",
      "Epoch 207/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1947 - accuracy: 0.9898 - val_loss: 0.3050 - val_accuracy: 0.9362\n",
      "Epoch 208/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1918 - accuracy: 0.9847 - val_loss: 0.3020 - val_accuracy: 0.9388\n",
      "Epoch 209/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1797 - accuracy: 0.9898 - val_loss: 0.2991 - val_accuracy: 0.9388\n",
      "Epoch 210/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1916 - accuracy: 0.9898 - val_loss: 0.2963 - val_accuracy: 0.9388\n",
      "Epoch 211/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1830 - accuracy: 0.9923 - val_loss: 0.2935 - val_accuracy: 0.9388\n",
      "Epoch 212/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1973 - accuracy: 0.9796 - val_loss: 0.2908 - val_accuracy: 0.9388\n",
      "Epoch 213/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1877 - accuracy: 0.9885 - val_loss: 0.2881 - val_accuracy: 0.9388\n",
      "Epoch 214/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.2110 - accuracy: 0.9860 - val_loss: 0.2855 - val_accuracy: 0.9388\n",
      "Epoch 215/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.2028 - accuracy: 0.9834 - val_loss: 0.2830 - val_accuracy: 0.9388\n",
      "Epoch 216/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1765 - accuracy: 0.9860 - val_loss: 0.2805 - val_accuracy: 0.9413\n",
      "Epoch 217/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1822 - accuracy: 0.9834 - val_loss: 0.2782 - val_accuracy: 0.9413\n",
      "Epoch 218/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1885 - accuracy: 0.9847 - val_loss: 0.2759 - val_accuracy: 0.9413\n",
      "Epoch 219/600\n",
      "783/783 [==============================] - 0s 146us/step - loss: 0.2005 - accuracy: 0.9834 - val_loss: 0.2738 - val_accuracy: 0.9413\n",
      "Epoch 220/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1808 - accuracy: 0.9847 - val_loss: 0.2717 - val_accuracy: 0.9413\n",
      "Epoch 221/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1785 - accuracy: 0.9860 - val_loss: 0.2696 - val_accuracy: 0.9413\n",
      "Epoch 222/600\n",
      "783/783 [==============================] - 0s 144us/step - loss: 0.1948 - accuracy: 0.9872 - val_loss: 0.2677 - val_accuracy: 0.9413\n",
      "Epoch 223/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1815 - accuracy: 0.9911 - val_loss: 0.2658 - val_accuracy: 0.9439\n",
      "Epoch 224/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1795 - accuracy: 0.9860 - val_loss: 0.2639 - val_accuracy: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1765 - accuracy: 0.9860 - val_loss: 0.2621 - val_accuracy: 0.9439\n",
      "Epoch 226/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1913 - accuracy: 0.9847 - val_loss: 0.2603 - val_accuracy: 0.9464\n",
      "Epoch 227/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.2034 - accuracy: 0.9796 - val_loss: 0.2585 - val_accuracy: 0.9490\n",
      "Epoch 228/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1843 - accuracy: 0.9911 - val_loss: 0.2568 - val_accuracy: 0.9490\n",
      "Epoch 229/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1936 - accuracy: 0.9911 - val_loss: 0.2553 - val_accuracy: 0.9490\n",
      "Epoch 230/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.2167 - accuracy: 0.9821 - val_loss: 0.2538 - val_accuracy: 0.9490\n",
      "Epoch 231/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1901 - accuracy: 0.9860 - val_loss: 0.2523 - val_accuracy: 0.9490\n",
      "Epoch 232/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.2055 - accuracy: 0.9872 - val_loss: 0.2509 - val_accuracy: 0.9490\n",
      "Epoch 233/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1815 - accuracy: 0.9923 - val_loss: 0.2495 - val_accuracy: 0.9490\n",
      "Epoch 234/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2273 - accuracy: 0.9847 - val_loss: 0.2481 - val_accuracy: 0.9490\n",
      "Epoch 235/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1924 - accuracy: 0.9885 - val_loss: 0.2468 - val_accuracy: 0.9490\n",
      "Epoch 236/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1774 - accuracy: 0.9923 - val_loss: 0.2454 - val_accuracy: 0.9490\n",
      "Epoch 237/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1902 - accuracy: 0.9860 - val_loss: 0.2441 - val_accuracy: 0.9490\n",
      "Epoch 238/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1801 - accuracy: 0.9885 - val_loss: 0.2428 - val_accuracy: 0.9490\n",
      "Epoch 239/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1899 - accuracy: 0.9860 - val_loss: 0.2416 - val_accuracy: 0.9490\n",
      "Epoch 240/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1966 - accuracy: 0.9923 - val_loss: 0.2404 - val_accuracy: 0.9490\n",
      "Epoch 241/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1913 - accuracy: 0.9834 - val_loss: 0.2392 - val_accuracy: 0.9490\n",
      "Epoch 242/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1880 - accuracy: 0.9872 - val_loss: 0.2380 - val_accuracy: 0.9490\n",
      "Epoch 243/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1814 - accuracy: 0.9936 - val_loss: 0.2369 - val_accuracy: 0.9490\n",
      "Epoch 244/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.1803 - accuracy: 0.9860 - val_loss: 0.2357 - val_accuracy: 0.9490\n",
      "Epoch 245/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1853 - accuracy: 0.9885 - val_loss: 0.2346 - val_accuracy: 0.9490\n",
      "Epoch 246/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1684 - accuracy: 0.9949 - val_loss: 0.2335 - val_accuracy: 0.9490\n",
      "Epoch 247/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1865 - accuracy: 0.9808 - val_loss: 0.2325 - val_accuracy: 0.9490\n",
      "Epoch 248/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1930 - accuracy: 0.9847 - val_loss: 0.2316 - val_accuracy: 0.9490\n",
      "Epoch 249/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.2031 - accuracy: 0.9821 - val_loss: 0.2306 - val_accuracy: 0.9490\n",
      "Epoch 250/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1918 - accuracy: 0.9898 - val_loss: 0.2296 - val_accuracy: 0.9464\n",
      "Epoch 251/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1782 - accuracy: 0.9911 - val_loss: 0.2287 - val_accuracy: 0.9464\n",
      "Epoch 252/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1825 - accuracy: 0.9885 - val_loss: 0.2279 - val_accuracy: 0.9464\n",
      "Epoch 253/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1943 - accuracy: 0.9860 - val_loss: 0.2270 - val_accuracy: 0.9464\n",
      "Epoch 254/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1848 - accuracy: 0.9885 - val_loss: 0.2261 - val_accuracy: 0.9490\n",
      "Epoch 255/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1919 - accuracy: 0.9885 - val_loss: 0.2253 - val_accuracy: 0.9490\n",
      "Epoch 256/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1943 - accuracy: 0.9872 - val_loss: 0.2245 - val_accuracy: 0.9490\n",
      "Epoch 257/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1850 - accuracy: 0.9898 - val_loss: 0.2237 - val_accuracy: 0.9490\n",
      "Epoch 258/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1903 - accuracy: 0.9860 - val_loss: 0.2229 - val_accuracy: 0.9490\n",
      "Epoch 259/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1900 - accuracy: 0.9898 - val_loss: 0.2220 - val_accuracy: 0.9490\n",
      "Epoch 260/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.1709 - accuracy: 0.9898 - val_loss: 0.2212 - val_accuracy: 0.9490\n",
      "Epoch 261/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1713 - accuracy: 0.9898 - val_loss: 0.2205 - val_accuracy: 0.9541\n",
      "Epoch 262/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1810 - accuracy: 0.9885 - val_loss: 0.2197 - val_accuracy: 0.9541\n",
      "Epoch 263/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1992 - accuracy: 0.9885 - val_loss: 0.2190 - val_accuracy: 0.9541\n",
      "Epoch 264/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1830 - accuracy: 0.9923 - val_loss: 0.2183 - val_accuracy: 0.9541\n",
      "Epoch 265/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1851 - accuracy: 0.9885 - val_loss: 0.2175 - val_accuracy: 0.9541\n",
      "Epoch 266/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.2104 - accuracy: 0.9808 - val_loss: 0.2168 - val_accuracy: 0.9566\n",
      "Epoch 267/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1788 - accuracy: 0.9860 - val_loss: 0.2161 - val_accuracy: 0.9566\n",
      "Epoch 268/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1992 - accuracy: 0.9847 - val_loss: 0.2154 - val_accuracy: 0.9566\n",
      "Epoch 269/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1806 - accuracy: 0.9860 - val_loss: 0.2147 - val_accuracy: 0.9566\n",
      "Epoch 270/600\n",
      "783/783 [==============================] - 0s 136us/step - loss: 0.2015 - accuracy: 0.9885 - val_loss: 0.2141 - val_accuracy: 0.9566\n",
      "Epoch 271/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1910 - accuracy: 0.9885 - val_loss: 0.2134 - val_accuracy: 0.9566\n",
      "Epoch 272/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1865 - accuracy: 0.9847 - val_loss: 0.2127 - val_accuracy: 0.9566\n",
      "Epoch 273/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1861 - accuracy: 0.9847 - val_loss: 0.2120 - val_accuracy: 0.9566\n",
      "Epoch 274/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1948 - accuracy: 0.9872 - val_loss: 0.2114 - val_accuracy: 0.9566\n",
      "Epoch 275/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1871 - accuracy: 0.9872 - val_loss: 0.2107 - val_accuracy: 0.9566\n",
      "Epoch 276/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.2058 - accuracy: 0.9860 - val_loss: 0.2101 - val_accuracy: 0.9566\n",
      "Epoch 277/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1903 - accuracy: 0.9911 - val_loss: 0.2095 - val_accuracy: 0.9566\n",
      "Epoch 278/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.2061 - accuracy: 0.9885 - val_loss: 0.2089 - val_accuracy: 0.9566\n",
      "Epoch 279/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1908 - accuracy: 0.9834 - val_loss: 0.2083 - val_accuracy: 0.9592\n",
      "Epoch 280/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1923 - accuracy: 0.9847 - val_loss: 0.2078 - val_accuracy: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/600\n",
      "783/783 [==============================] - 0s 141us/step - loss: 0.1905 - accuracy: 0.9885 - val_loss: 0.2073 - val_accuracy: 0.9592\n",
      "Epoch 282/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1930 - accuracy: 0.9847 - val_loss: 0.2067 - val_accuracy: 0.9592\n",
      "Epoch 283/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1913 - accuracy: 0.9847 - val_loss: 0.2061 - val_accuracy: 0.9592\n",
      "Epoch 284/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1815 - accuracy: 0.9898 - val_loss: 0.2057 - val_accuracy: 0.9592\n",
      "Epoch 285/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1793 - accuracy: 0.9860 - val_loss: 0.2052 - val_accuracy: 0.9592\n",
      "Epoch 286/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1943 - accuracy: 0.9860 - val_loss: 0.2048 - val_accuracy: 0.9566\n",
      "Epoch 287/600\n",
      "783/783 [==============================] - 0s 134us/step - loss: 0.1811 - accuracy: 0.9911 - val_loss: 0.2043 - val_accuracy: 0.9592\n",
      "Epoch 288/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1795 - accuracy: 0.9860 - val_loss: 0.2039 - val_accuracy: 0.9592\n",
      "Epoch 289/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1858 - accuracy: 0.9860 - val_loss: 0.2034 - val_accuracy: 0.9592\n",
      "Epoch 290/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1873 - accuracy: 0.9898 - val_loss: 0.2030 - val_accuracy: 0.9592\n",
      "Epoch 291/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1896 - accuracy: 0.9860 - val_loss: 0.2026 - val_accuracy: 0.9592\n",
      "Epoch 292/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1720 - accuracy: 0.9923 - val_loss: 0.2022 - val_accuracy: 0.9592\n",
      "Epoch 293/600\n",
      "783/783 [==============================] - 0s 138us/step - loss: 0.1827 - accuracy: 0.9923 - val_loss: 0.2018 - val_accuracy: 0.9592\n",
      "Epoch 294/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1769 - accuracy: 0.9923 - val_loss: 0.2014 - val_accuracy: 0.9592\n",
      "Epoch 295/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1856 - accuracy: 0.9847 - val_loss: 0.2010 - val_accuracy: 0.9592\n",
      "Epoch 296/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1900 - accuracy: 0.9834 - val_loss: 0.2006 - val_accuracy: 0.9592\n",
      "Epoch 297/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1868 - accuracy: 0.9885 - val_loss: 0.2002 - val_accuracy: 0.9592\n",
      "Epoch 298/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.2054 - accuracy: 0.9860 - val_loss: 0.1998 - val_accuracy: 0.9592\n",
      "Epoch 299/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1753 - accuracy: 0.9847 - val_loss: 0.1995 - val_accuracy: 0.9592\n",
      "Epoch 300/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1911 - accuracy: 0.9898 - val_loss: 0.1991 - val_accuracy: 0.9592\n",
      "Epoch 301/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1767 - accuracy: 0.9898 - val_loss: 0.1988 - val_accuracy: 0.9592\n",
      "Epoch 302/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1838 - accuracy: 0.9872 - val_loss: 0.1985 - val_accuracy: 0.9592\n",
      "Epoch 303/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1916 - accuracy: 0.9860 - val_loss: 0.1982 - val_accuracy: 0.9592\n",
      "Epoch 304/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1787 - accuracy: 0.9885 - val_loss: 0.1978 - val_accuracy: 0.9592\n",
      "Epoch 305/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1938 - accuracy: 0.9847 - val_loss: 0.1974 - val_accuracy: 0.9592\n",
      "Epoch 306/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1818 - accuracy: 0.9885 - val_loss: 0.1971 - val_accuracy: 0.9592\n",
      "Epoch 307/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1852 - accuracy: 0.9860 - val_loss: 0.1967 - val_accuracy: 0.9592\n",
      "Epoch 308/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1896 - accuracy: 0.9872 - val_loss: 0.1963 - val_accuracy: 0.9592\n",
      "Epoch 309/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1886 - accuracy: 0.9923 - val_loss: 0.1960 - val_accuracy: 0.9592\n",
      "Epoch 310/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1980 - accuracy: 0.9936 - val_loss: 0.1956 - val_accuracy: 0.9592\n",
      "Epoch 311/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1755 - accuracy: 0.9834 - val_loss: 0.1953 - val_accuracy: 0.9617\n",
      "Epoch 312/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1931 - accuracy: 0.9898 - val_loss: 0.1949 - val_accuracy: 0.9617\n",
      "Epoch 313/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1809 - accuracy: 0.9885 - val_loss: 0.1947 - val_accuracy: 0.9617\n",
      "Epoch 314/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1783 - accuracy: 0.9949 - val_loss: 0.1944 - val_accuracy: 0.9617\n",
      "Epoch 315/600\n",
      "783/783 [==============================] - 0s 134us/step - loss: 0.1821 - accuracy: 0.9898 - val_loss: 0.1941 - val_accuracy: 0.9617\n",
      "Epoch 316/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.2005 - accuracy: 0.9872 - val_loss: 0.1938 - val_accuracy: 0.9617\n",
      "Epoch 317/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1865 - accuracy: 0.9834 - val_loss: 0.1934 - val_accuracy: 0.9617\n",
      "Epoch 318/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1895 - accuracy: 0.9885 - val_loss: 0.1931 - val_accuracy: 0.9617\n",
      "Epoch 319/600\n",
      "783/783 [==============================] - 0s 154us/step - loss: 0.1859 - accuracy: 0.9860 - val_loss: 0.1928 - val_accuracy: 0.9617\n",
      "Epoch 320/600\n",
      "783/783 [==============================] - 0s 144us/step - loss: 0.1973 - accuracy: 0.9808 - val_loss: 0.1924 - val_accuracy: 0.9617\n",
      "Epoch 321/600\n",
      "783/783 [==============================] - 0s 136us/step - loss: 0.1880 - accuracy: 0.9911 - val_loss: 0.1921 - val_accuracy: 0.9617\n",
      "Epoch 322/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1838 - accuracy: 0.9860 - val_loss: 0.1918 - val_accuracy: 0.9617\n",
      "Epoch 323/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.2011 - accuracy: 0.9885 - val_loss: 0.1914 - val_accuracy: 0.9617\n",
      "Epoch 324/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1847 - accuracy: 0.9834 - val_loss: 0.1911 - val_accuracy: 0.9617\n",
      "Epoch 325/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1858 - accuracy: 0.9872 - val_loss: 0.1908 - val_accuracy: 0.9617\n",
      "Epoch 326/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1849 - accuracy: 0.9834 - val_loss: 0.1905 - val_accuracy: 0.9617\n",
      "Epoch 327/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1908 - accuracy: 0.9872 - val_loss: 0.1902 - val_accuracy: 0.9617\n",
      "Epoch 328/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1776 - accuracy: 0.9898 - val_loss: 0.1899 - val_accuracy: 0.9617\n",
      "Epoch 329/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1845 - accuracy: 0.9847 - val_loss: 0.1896 - val_accuracy: 0.9617\n",
      "Epoch 330/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1831 - accuracy: 0.9936 - val_loss: 0.1893 - val_accuracy: 0.9617\n",
      "Epoch 331/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1789 - accuracy: 0.9885 - val_loss: 0.1891 - val_accuracy: 0.9617\n",
      "Epoch 332/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1704 - accuracy: 0.9860 - val_loss: 0.1889 - val_accuracy: 0.9617\n",
      "Epoch 333/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1982 - accuracy: 0.9860 - val_loss: 0.1887 - val_accuracy: 0.9617\n",
      "Epoch 334/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1819 - accuracy: 0.9885 - val_loss: 0.1884 - val_accuracy: 0.9617\n",
      "Epoch 335/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.2019 - accuracy: 0.9770 - val_loss: 0.1882 - val_accuracy: 0.9617\n",
      "Epoch 336/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1846 - accuracy: 0.9898 - val_loss: 0.1880 - val_accuracy: 0.9617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1774 - accuracy: 0.9923 - val_loss: 0.1879 - val_accuracy: 0.9617\n",
      "Epoch 338/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1998 - accuracy: 0.9821 - val_loss: 0.1876 - val_accuracy: 0.9617\n",
      "Epoch 339/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1913 - accuracy: 0.9911 - val_loss: 0.1874 - val_accuracy: 0.9617\n",
      "Epoch 340/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1918 - accuracy: 0.9847 - val_loss: 0.1872 - val_accuracy: 0.9617\n",
      "Epoch 341/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.2030 - accuracy: 0.9808 - val_loss: 0.1871 - val_accuracy: 0.9617\n",
      "Epoch 342/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1900 - accuracy: 0.9821 - val_loss: 0.1869 - val_accuracy: 0.9617\n",
      "Epoch 343/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1857 - accuracy: 0.9834 - val_loss: 0.1867 - val_accuracy: 0.9617\n",
      "Epoch 344/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1738 - accuracy: 0.9885 - val_loss: 0.1865 - val_accuracy: 0.9617\n",
      "Epoch 345/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1899 - accuracy: 0.9872 - val_loss: 0.1864 - val_accuracy: 0.9617\n",
      "Epoch 346/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1786 - accuracy: 0.9860 - val_loss: 0.1862 - val_accuracy: 0.9617\n",
      "Epoch 347/600\n",
      "783/783 [==============================] - 0s 148us/step - loss: 0.1974 - accuracy: 0.9834 - val_loss: 0.1860 - val_accuracy: 0.9617\n",
      "Epoch 348/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1814 - accuracy: 0.9860 - val_loss: 0.1859 - val_accuracy: 0.9617\n",
      "Epoch 349/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1777 - accuracy: 0.9911 - val_loss: 0.1857 - val_accuracy: 0.9617\n",
      "Epoch 350/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1903 - accuracy: 0.9885 - val_loss: 0.1855 - val_accuracy: 0.9617\n",
      "Epoch 351/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1821 - accuracy: 0.9860 - val_loss: 0.1853 - val_accuracy: 0.9617\n",
      "Epoch 352/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1805 - accuracy: 0.9847 - val_loss: 0.1851 - val_accuracy: 0.9643\n",
      "Epoch 353/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1638 - accuracy: 0.9911 - val_loss: 0.1850 - val_accuracy: 0.9643\n",
      "Epoch 354/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.2050 - accuracy: 0.9847 - val_loss: 0.1848 - val_accuracy: 0.9643\n",
      "Epoch 355/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1874 - accuracy: 0.9847 - val_loss: 0.1847 - val_accuracy: 0.9643\n",
      "Epoch 356/600\n",
      "783/783 [==============================] - 0s 138us/step - loss: 0.2131 - accuracy: 0.9834 - val_loss: 0.1846 - val_accuracy: 0.9643\n",
      "Epoch 357/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1700 - accuracy: 0.9911 - val_loss: 0.1845 - val_accuracy: 0.9643\n",
      "Epoch 358/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.1911 - accuracy: 0.9898 - val_loss: 0.1843 - val_accuracy: 0.9643\n",
      "Epoch 359/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1734 - accuracy: 0.9808 - val_loss: 0.1841 - val_accuracy: 0.9643\n",
      "Epoch 360/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1823 - accuracy: 0.9936 - val_loss: 0.1840 - val_accuracy: 0.9643\n",
      "Epoch 361/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1773 - accuracy: 0.9962 - val_loss: 0.1838 - val_accuracy: 0.9643\n",
      "Epoch 362/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1730 - accuracy: 0.9872 - val_loss: 0.1836 - val_accuracy: 0.9643\n",
      "Epoch 363/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1928 - accuracy: 0.9821 - val_loss: 0.1834 - val_accuracy: 0.9643\n",
      "Epoch 364/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1874 - accuracy: 0.9860 - val_loss: 0.1833 - val_accuracy: 0.9643\n",
      "Epoch 365/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1778 - accuracy: 0.9949 - val_loss: 0.1831 - val_accuracy: 0.9643\n",
      "Epoch 366/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1626 - accuracy: 0.9898 - val_loss: 0.1830 - val_accuracy: 0.9643\n",
      "Epoch 367/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1883 - accuracy: 0.9872 - val_loss: 0.1829 - val_accuracy: 0.9643\n",
      "Epoch 368/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1688 - accuracy: 0.9860 - val_loss: 0.1828 - val_accuracy: 0.9643\n",
      "Epoch 369/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1835 - accuracy: 0.9898 - val_loss: 0.1827 - val_accuracy: 0.9643\n",
      "Epoch 370/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1803 - accuracy: 0.9885 - val_loss: 0.1826 - val_accuracy: 0.9643\n",
      "Epoch 371/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1749 - accuracy: 0.9923 - val_loss: 0.1824 - val_accuracy: 0.9643\n",
      "Epoch 372/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1784 - accuracy: 0.9911 - val_loss: 0.1823 - val_accuracy: 0.9643\n",
      "Epoch 373/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1948 - accuracy: 0.9847 - val_loss: 0.1822 - val_accuracy: 0.9643\n",
      "Epoch 374/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1793 - accuracy: 0.9911 - val_loss: 0.1821 - val_accuracy: 0.9643\n",
      "Epoch 375/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1766 - accuracy: 0.9872 - val_loss: 0.1820 - val_accuracy: 0.9643\n",
      "Epoch 376/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1881 - accuracy: 0.9911 - val_loss: 0.1819 - val_accuracy: 0.9643\n",
      "Epoch 377/600\n",
      "783/783 [==============================] - 0s 144us/step - loss: 0.1886 - accuracy: 0.9898 - val_loss: 0.1818 - val_accuracy: 0.9643\n",
      "Epoch 378/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.2008 - accuracy: 0.9898 - val_loss: 0.1817 - val_accuracy: 0.9643\n",
      "Epoch 379/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1857 - accuracy: 0.9898 - val_loss: 0.1816 - val_accuracy: 0.9643\n",
      "Epoch 380/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1958 - accuracy: 0.9796 - val_loss: 0.1815 - val_accuracy: 0.9643\n",
      "Epoch 381/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1823 - accuracy: 0.9847 - val_loss: 0.1814 - val_accuracy: 0.9643\n",
      "Epoch 382/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1769 - accuracy: 0.9860 - val_loss: 0.1814 - val_accuracy: 0.9643\n",
      "Epoch 383/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1926 - accuracy: 0.9834 - val_loss: 0.1813 - val_accuracy: 0.9643\n",
      "Epoch 384/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1805 - accuracy: 0.9872 - val_loss: 0.1812 - val_accuracy: 0.9643\n",
      "Epoch 385/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1758 - accuracy: 0.9936 - val_loss: 0.1811 - val_accuracy: 0.9643\n",
      "Epoch 386/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1721 - accuracy: 0.9936 - val_loss: 0.1810 - val_accuracy: 0.9643\n",
      "Epoch 387/600\n",
      "783/783 [==============================] - 0s 154us/step - loss: 0.1790 - accuracy: 0.9872 - val_loss: 0.1810 - val_accuracy: 0.9643\n",
      "Epoch 388/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1809 - accuracy: 0.9911 - val_loss: 0.1809 - val_accuracy: 0.9643\n",
      "Epoch 389/600\n",
      "783/783 [==============================] - 0s 142us/step - loss: 0.1779 - accuracy: 0.9923 - val_loss: 0.1808 - val_accuracy: 0.9643\n",
      "Epoch 390/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.1882 - accuracy: 0.9885 - val_loss: 0.1807 - val_accuracy: 0.9643\n",
      "Epoch 391/600\n",
      "783/783 [==============================] - 0s 141us/step - loss: 0.1846 - accuracy: 0.9949 - val_loss: 0.1806 - val_accuracy: 0.9643\n",
      "Epoch 392/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1891 - accuracy: 0.9834 - val_loss: 0.1805 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1798 - accuracy: 0.9872 - val_loss: 0.1805 - val_accuracy: 0.9643\n",
      "Epoch 394/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1833 - accuracy: 0.9834 - val_loss: 0.1804 - val_accuracy: 0.9643\n",
      "Epoch 395/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1983 - accuracy: 0.9872 - val_loss: 0.1803 - val_accuracy: 0.9643\n",
      "Epoch 396/600\n",
      "783/783 [==============================] - 0s 140us/step - loss: 0.1988 - accuracy: 0.9821 - val_loss: 0.1802 - val_accuracy: 0.9643\n",
      "Epoch 397/600\n",
      "783/783 [==============================] - 0s 142us/step - loss: 0.1704 - accuracy: 0.9898 - val_loss: 0.1801 - val_accuracy: 0.9643\n",
      "Epoch 398/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.2017 - accuracy: 0.9821 - val_loss: 0.1800 - val_accuracy: 0.9643\n",
      "Epoch 399/600\n",
      "783/783 [==============================] - 0s 152us/step - loss: 0.1657 - accuracy: 0.9936 - val_loss: 0.1799 - val_accuracy: 0.9643\n",
      "Epoch 400/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1857 - accuracy: 0.9860 - val_loss: 0.1798 - val_accuracy: 0.9643\n",
      "Epoch 401/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1966 - accuracy: 0.9885 - val_loss: 0.1797 - val_accuracy: 0.9643\n",
      "Epoch 402/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1862 - accuracy: 0.9885 - val_loss: 0.1797 - val_accuracy: 0.9643\n",
      "Epoch 403/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1869 - accuracy: 0.9872 - val_loss: 0.1796 - val_accuracy: 0.9643\n",
      "Epoch 404/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1882 - accuracy: 0.9885 - val_loss: 0.1795 - val_accuracy: 0.9643\n",
      "Epoch 405/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1847 - accuracy: 0.9923 - val_loss: 0.1794 - val_accuracy: 0.9643\n",
      "Epoch 406/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.2017 - accuracy: 0.9808 - val_loss: 0.1794 - val_accuracy: 0.9643\n",
      "Epoch 407/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.1877 - accuracy: 0.9872 - val_loss: 0.1793 - val_accuracy: 0.9643\n",
      "Epoch 408/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1764 - accuracy: 0.9885 - val_loss: 0.1792 - val_accuracy: 0.9643\n",
      "Epoch 409/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1878 - accuracy: 0.9872 - val_loss: 0.1791 - val_accuracy: 0.9643\n",
      "Epoch 410/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1776 - accuracy: 0.9911 - val_loss: 0.1790 - val_accuracy: 0.9643\n",
      "Epoch 411/600\n",
      "783/783 [==============================] - 0s 119us/step - loss: 0.1709 - accuracy: 0.9885 - val_loss: 0.1789 - val_accuracy: 0.9643\n",
      "Epoch 412/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1981 - accuracy: 0.9847 - val_loss: 0.1788 - val_accuracy: 0.9643\n",
      "Epoch 413/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1809 - accuracy: 0.9872 - val_loss: 0.1787 - val_accuracy: 0.9643\n",
      "Epoch 414/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1940 - accuracy: 0.9923 - val_loss: 0.1786 - val_accuracy: 0.9643\n",
      "Epoch 415/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.1799 - accuracy: 0.9847 - val_loss: 0.1785 - val_accuracy: 0.9643\n",
      "Epoch 416/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1699 - accuracy: 0.9872 - val_loss: 0.1785 - val_accuracy: 0.9643\n",
      "Epoch 417/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1778 - accuracy: 0.9898 - val_loss: 0.1784 - val_accuracy: 0.9643\n",
      "Epoch 418/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1828 - accuracy: 0.9847 - val_loss: 0.1783 - val_accuracy: 0.9643\n",
      "Epoch 419/600\n",
      "783/783 [==============================] - 0s 109us/step - loss: 0.1847 - accuracy: 0.9898 - val_loss: 0.1783 - val_accuracy: 0.9643\n",
      "Epoch 420/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1781 - accuracy: 0.9911 - val_loss: 0.1782 - val_accuracy: 0.9643\n",
      "Epoch 421/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.1774 - accuracy: 0.9936 - val_loss: 0.1781 - val_accuracy: 0.9643\n",
      "Epoch 422/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.2024 - accuracy: 0.9885 - val_loss: 0.1780 - val_accuracy: 0.9643\n",
      "Epoch 423/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1812 - accuracy: 0.9872 - val_loss: 0.1779 - val_accuracy: 0.9643\n",
      "Epoch 424/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1946 - accuracy: 0.9898 - val_loss: 0.1778 - val_accuracy: 0.9643\n",
      "Epoch 425/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1799 - accuracy: 0.9834 - val_loss: 0.1777 - val_accuracy: 0.9643\n",
      "Epoch 426/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.1839 - accuracy: 0.9923 - val_loss: 0.1776 - val_accuracy: 0.9643\n",
      "Epoch 427/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1893 - accuracy: 0.9872 - val_loss: 0.1776 - val_accuracy: 0.9643\n",
      "Epoch 428/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.1929 - accuracy: 0.9898 - val_loss: 0.1775 - val_accuracy: 0.9643\n",
      "Epoch 429/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1834 - accuracy: 0.9911 - val_loss: 0.1774 - val_accuracy: 0.9643\n",
      "Epoch 430/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.1895 - accuracy: 0.9860 - val_loss: 0.1774 - val_accuracy: 0.9643\n",
      "Epoch 431/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1797 - accuracy: 0.9911 - val_loss: 0.1773 - val_accuracy: 0.9643\n",
      "Epoch 432/600\n",
      "783/783 [==============================] - 0s 115us/step - loss: 0.1826 - accuracy: 0.9923 - val_loss: 0.1772 - val_accuracy: 0.9643\n",
      "Epoch 433/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1953 - accuracy: 0.9834 - val_loss: 0.1772 - val_accuracy: 0.9643\n",
      "Epoch 434/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.2007 - accuracy: 0.9770 - val_loss: 0.1771 - val_accuracy: 0.9643\n",
      "Epoch 435/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1791 - accuracy: 0.9885 - val_loss: 0.1770 - val_accuracy: 0.9643\n",
      "Epoch 436/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1836 - accuracy: 0.9898 - val_loss: 0.1770 - val_accuracy: 0.9643\n",
      "Epoch 437/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1990 - accuracy: 0.9847 - val_loss: 0.1769 - val_accuracy: 0.9643\n",
      "Epoch 438/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1967 - accuracy: 0.9808 - val_loss: 0.1768 - val_accuracy: 0.9643\n",
      "Epoch 439/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1843 - accuracy: 0.9885 - val_loss: 0.1767 - val_accuracy: 0.9643\n",
      "Epoch 440/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1916 - accuracy: 0.9885 - val_loss: 0.1766 - val_accuracy: 0.9643\n",
      "Epoch 441/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1916 - accuracy: 0.9885 - val_loss: 0.1765 - val_accuracy: 0.9643\n",
      "Epoch 442/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1863 - accuracy: 0.9872 - val_loss: 0.1764 - val_accuracy: 0.9643\n",
      "Epoch 443/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1813 - accuracy: 0.9821 - val_loss: 0.1764 - val_accuracy: 0.9643\n",
      "Epoch 444/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1968 - accuracy: 0.9796 - val_loss: 0.1763 - val_accuracy: 0.9643\n",
      "Epoch 445/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.1905 - accuracy: 0.9783 - val_loss: 0.1762 - val_accuracy: 0.9643\n",
      "Epoch 446/600\n",
      "783/783 [==============================] - 0s 108us/step - loss: 0.1861 - accuracy: 0.9847 - val_loss: 0.1762 - val_accuracy: 0.9643\n",
      "Epoch 447/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.1941 - accuracy: 0.9885 - val_loss: 0.1762 - val_accuracy: 0.9643\n",
      "Epoch 448/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1719 - accuracy: 0.9898 - val_loss: 0.1761 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/600\n",
      "783/783 [==============================] - 0s 116us/step - loss: 0.1940 - accuracy: 0.9808 - val_loss: 0.1760 - val_accuracy: 0.9643\n",
      "Epoch 450/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1949 - accuracy: 0.9821 - val_loss: 0.1760 - val_accuracy: 0.9643\n",
      "Epoch 451/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1799 - accuracy: 0.9860 - val_loss: 0.1760 - val_accuracy: 0.9643\n",
      "Epoch 452/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1967 - accuracy: 0.9847 - val_loss: 0.1759 - val_accuracy: 0.9617\n",
      "Epoch 453/600\n",
      "783/783 [==============================] - 0s 117us/step - loss: 0.1798 - accuracy: 0.9898 - val_loss: 0.1758 - val_accuracy: 0.9617\n",
      "Epoch 454/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.1888 - accuracy: 0.9860 - val_loss: 0.1758 - val_accuracy: 0.9617\n",
      "Epoch 455/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1895 - accuracy: 0.9860 - val_loss: 0.1758 - val_accuracy: 0.9617\n",
      "Epoch 456/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1768 - accuracy: 0.9923 - val_loss: 0.1758 - val_accuracy: 0.9617\n",
      "Epoch 457/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1765 - accuracy: 0.9847 - val_loss: 0.1757 - val_accuracy: 0.9617\n",
      "Epoch 458/600\n",
      "783/783 [==============================] - 0s 112us/step - loss: 0.1814 - accuracy: 0.9821 - val_loss: 0.1757 - val_accuracy: 0.9617\n",
      "Epoch 459/600\n",
      "783/783 [==============================] - 0s 113us/step - loss: 0.2013 - accuracy: 0.9898 - val_loss: 0.1756 - val_accuracy: 0.9617\n",
      "Epoch 460/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1898 - accuracy: 0.9898 - val_loss: 0.1756 - val_accuracy: 0.9617\n",
      "Epoch 461/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1788 - accuracy: 0.9898 - val_loss: 0.1755 - val_accuracy: 0.9617\n",
      "Epoch 462/600\n",
      "783/783 [==============================] - 0s 114us/step - loss: 0.1961 - accuracy: 0.9834 - val_loss: 0.1755 - val_accuracy: 0.9617\n",
      "Epoch 463/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1799 - accuracy: 0.9834 - val_loss: 0.1754 - val_accuracy: 0.9617\n",
      "Epoch 464/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1903 - accuracy: 0.9898 - val_loss: 0.1753 - val_accuracy: 0.9617\n",
      "Epoch 465/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1836 - accuracy: 0.9936 - val_loss: 0.1753 - val_accuracy: 0.9617\n",
      "Epoch 466/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1763 - accuracy: 0.9860 - val_loss: 0.1753 - val_accuracy: 0.9617\n",
      "Epoch 467/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1834 - accuracy: 0.9872 - val_loss: 0.1753 - val_accuracy: 0.9617\n",
      "Epoch 468/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2080 - accuracy: 0.9808 - val_loss: 0.1752 - val_accuracy: 0.9617\n",
      "Epoch 469/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.2008 - accuracy: 0.9808 - val_loss: 0.1752 - val_accuracy: 0.9617\n",
      "Epoch 470/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1761 - accuracy: 0.9872 - val_loss: 0.1751 - val_accuracy: 0.9617\n",
      "Epoch 471/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.1821 - accuracy: 0.9911 - val_loss: 0.1751 - val_accuracy: 0.9617\n",
      "Epoch 472/600\n",
      "783/783 [==============================] - 0s 110us/step - loss: 0.1773 - accuracy: 0.9872 - val_loss: 0.1751 - val_accuracy: 0.9617\n",
      "Epoch 473/600\n",
      "783/783 [==============================] - 0s 105us/step - loss: 0.1901 - accuracy: 0.9872 - val_loss: 0.1750 - val_accuracy: 0.9617\n",
      "Epoch 474/600\n",
      "783/783 [==============================] - 0s 111us/step - loss: 0.1690 - accuracy: 0.9847 - val_loss: 0.1750 - val_accuracy: 0.9617\n",
      "Epoch 475/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1884 - accuracy: 0.9911 - val_loss: 0.1750 - val_accuracy: 0.9617\n",
      "Epoch 476/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1748 - accuracy: 0.9911 - val_loss: 0.1750 - val_accuracy: 0.9617\n",
      "Epoch 477/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1866 - accuracy: 0.9847 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 478/600\n",
      "783/783 [==============================] - 0s 118us/step - loss: 0.1702 - accuracy: 0.9923 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 479/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1891 - accuracy: 0.9885 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 480/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1853 - accuracy: 0.9923 - val_loss: 0.1750 - val_accuracy: 0.9617\n",
      "Epoch 481/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1820 - accuracy: 0.9860 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 482/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1809 - accuracy: 0.9872 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 483/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1854 - accuracy: 0.9872 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 484/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1898 - accuracy: 0.9911 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 485/600\n",
      "783/783 [==============================] - 0s 135us/step - loss: 0.1740 - accuracy: 0.9885 - val_loss: 0.1749 - val_accuracy: 0.9617\n",
      "Epoch 486/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1835 - accuracy: 0.9860 - val_loss: 0.1748 - val_accuracy: 0.9617\n",
      "Epoch 487/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1850 - accuracy: 0.9936 - val_loss: 0.1748 - val_accuracy: 0.9617\n",
      "Epoch 488/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1870 - accuracy: 0.9936 - val_loss: 0.1748 - val_accuracy: 0.9617\n",
      "Epoch 489/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1829 - accuracy: 0.9860 - val_loss: 0.1748 - val_accuracy: 0.9617\n",
      "Epoch 490/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1687 - accuracy: 0.9949 - val_loss: 0.1747 - val_accuracy: 0.9617\n",
      "Epoch 491/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1922 - accuracy: 0.9911 - val_loss: 0.1747 - val_accuracy: 0.9617\n",
      "Epoch 492/600\n",
      "783/783 [==============================] - 0s 134us/step - loss: 0.1820 - accuracy: 0.9923 - val_loss: 0.1747 - val_accuracy: 0.9617\n",
      "Epoch 493/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1923 - accuracy: 0.9885 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 494/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1631 - accuracy: 0.9949 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 495/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1871 - accuracy: 0.9885 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 496/600\n",
      "783/783 [==============================] - 0s 120us/step - loss: 0.1675 - accuracy: 0.9847 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 497/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1757 - accuracy: 0.9923 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 498/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1851 - accuracy: 0.9847 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 499/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1857 - accuracy: 0.9872 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 500/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1699 - accuracy: 0.9936 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 501/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1777 - accuracy: 0.9885 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 502/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1785 - accuracy: 0.9860 - val_loss: 0.1746 - val_accuracy: 0.9617\n",
      "Epoch 503/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.2123 - accuracy: 0.9860 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 504/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1763 - accuracy: 0.9872 - val_loss: 0.1745 - val_accuracy: 0.9617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1931 - accuracy: 0.9821 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 506/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1999 - accuracy: 0.9783 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 507/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1784 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 508/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1932 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 509/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1614 - accuracy: 0.9923 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 510/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.2059 - accuracy: 0.9834 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 511/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.2066 - accuracy: 0.9834 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 512/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1905 - accuracy: 0.9808 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 513/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1690 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 514/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1984 - accuracy: 0.9808 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 515/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1870 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 516/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1727 - accuracy: 0.9898 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 517/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.2009 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 518/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1895 - accuracy: 0.9962 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 519/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1814 - accuracy: 0.9821 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 520/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1872 - accuracy: 0.9847 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 521/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.2045 - accuracy: 0.9834 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 522/600\n",
      "783/783 [==============================] - 0s 121us/step - loss: 0.1891 - accuracy: 0.9808 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 523/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1632 - accuracy: 0.9847 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 524/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1592 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 525/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1863 - accuracy: 0.9872 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 526/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1736 - accuracy: 0.9911 - val_loss: 0.1745 - val_accuracy: 0.9617\n",
      "Epoch 527/600\n",
      "783/783 [==============================] - 0s 141us/step - loss: 0.1858 - accuracy: 0.9885 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 528/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1683 - accuracy: 0.9872 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 529/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1837 - accuracy: 0.9860 - val_loss: 0.1744 - val_accuracy: 0.9617\n",
      "Epoch 530/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1811 - accuracy: 0.9885 - val_loss: 0.1743 - val_accuracy: 0.9617\n",
      "Epoch 531/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1862 - accuracy: 0.9796 - val_loss: 0.1743 - val_accuracy: 0.9617\n",
      "Epoch 532/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1972 - accuracy: 0.9834 - val_loss: 0.1742 - val_accuracy: 0.9617\n",
      "Epoch 533/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1815 - accuracy: 0.9911 - val_loss: 0.1742 - val_accuracy: 0.9617\n",
      "Epoch 534/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1994 - accuracy: 0.9872 - val_loss: 0.1742 - val_accuracy: 0.9617\n",
      "Epoch 535/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1893 - accuracy: 0.9860 - val_loss: 0.1742 - val_accuracy: 0.9617\n",
      "Epoch 536/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1704 - accuracy: 0.9923 - val_loss: 0.1741 - val_accuracy: 0.9617\n",
      "Epoch 537/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1738 - accuracy: 0.9872 - val_loss: 0.1740 - val_accuracy: 0.9617\n",
      "Epoch 538/600\n",
      "783/783 [==============================] - 0s 136us/step - loss: 0.1796 - accuracy: 0.9898 - val_loss: 0.1740 - val_accuracy: 0.9617\n",
      "Epoch 539/600\n",
      "783/783 [==============================] - 0s 177us/step - loss: 0.1785 - accuracy: 0.9898 - val_loss: 0.1739 - val_accuracy: 0.9617\n",
      "Epoch 540/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1641 - accuracy: 0.9898 - val_loss: 0.1738 - val_accuracy: 0.9617\n",
      "Epoch 541/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1823 - accuracy: 0.9885 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 542/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1739 - accuracy: 0.9898 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 543/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.2011 - accuracy: 0.9821 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 544/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1754 - accuracy: 0.9872 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 545/600\n",
      "783/783 [==============================] - 0s 138us/step - loss: 0.1899 - accuracy: 0.9860 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 546/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1769 - accuracy: 0.9936 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 547/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1645 - accuracy: 0.9936 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 548/600\n",
      "783/783 [==============================] - 0s 144us/step - loss: 0.1870 - accuracy: 0.9808 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 549/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1873 - accuracy: 0.9872 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 550/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1703 - accuracy: 0.9936 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 551/600\n",
      "783/783 [==============================] - 0s 138us/step - loss: 0.1681 - accuracy: 0.9898 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 552/600\n",
      "783/783 [==============================] - 0s 137us/step - loss: 0.1633 - accuracy: 0.9911 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 553/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1927 - accuracy: 0.9923 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 554/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1874 - accuracy: 0.9860 - val_loss: 0.1738 - val_accuracy: 0.9643\n",
      "Epoch 555/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1925 - accuracy: 0.9872 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 556/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.2069 - accuracy: 0.9821 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 557/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1522 - accuracy: 0.9936 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 558/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1664 - accuracy: 0.9936 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 559/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1902 - accuracy: 0.9834 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 560/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1888 - accuracy: 0.9885 - val_loss: 0.1738 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1757 - accuracy: 0.9911 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 562/600\n",
      "783/783 [==============================] - 0s 138us/step - loss: 0.1836 - accuracy: 0.9936 - val_loss: 0.1737 - val_accuracy: 0.9643\n",
      "Epoch 563/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1864 - accuracy: 0.9821 - val_loss: 0.1736 - val_accuracy: 0.9643\n",
      "Epoch 564/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1967 - accuracy: 0.9872 - val_loss: 0.1736 - val_accuracy: 0.9643\n",
      "Epoch 565/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1838 - accuracy: 0.9847 - val_loss: 0.1736 - val_accuracy: 0.9643\n",
      "Epoch 566/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1892 - accuracy: 0.9898 - val_loss: 0.1735 - val_accuracy: 0.9643\n",
      "Epoch 567/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1852 - accuracy: 0.9847 - val_loss: 0.1735 - val_accuracy: 0.9643\n",
      "Epoch 568/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1716 - accuracy: 0.9847 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 569/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1705 - accuracy: 0.9898 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 570/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1709 - accuracy: 0.9885 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 571/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1810 - accuracy: 0.9834 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 572/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1920 - accuracy: 0.9821 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 573/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1860 - accuracy: 0.9834 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 574/600\n",
      "783/783 [==============================] - 0s 147us/step - loss: 0.1852 - accuracy: 0.9847 - val_loss: 0.1734 - val_accuracy: 0.9643\n",
      "Epoch 575/600\n",
      "783/783 [==============================] - 0s 130us/step - loss: 0.1717 - accuracy: 0.9923 - val_loss: 0.1733 - val_accuracy: 0.9643\n",
      "Epoch 576/600\n",
      "783/783 [==============================] - 0s 132us/step - loss: 0.1851 - accuracy: 0.9898 - val_loss: 0.1733 - val_accuracy: 0.9643\n",
      "Epoch 577/600\n",
      "783/783 [==============================] - 0s 140us/step - loss: 0.1938 - accuracy: 0.9796 - val_loss: 0.1733 - val_accuracy: 0.9643\n",
      "Epoch 578/600\n",
      "783/783 [==============================] - 0s 146us/step - loss: 0.1765 - accuracy: 0.9885 - val_loss: 0.1733 - val_accuracy: 0.9643\n",
      "Epoch 579/600\n",
      "783/783 [==============================] - 0s 126us/step - loss: 0.1874 - accuracy: 0.9872 - val_loss: 0.1733 - val_accuracy: 0.9643\n",
      "Epoch 580/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1875 - accuracy: 0.9872 - val_loss: 0.1732 - val_accuracy: 0.9643\n",
      "Epoch 581/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1940 - accuracy: 0.9847 - val_loss: 0.1732 - val_accuracy: 0.9643\n",
      "Epoch 582/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1700 - accuracy: 0.9885 - val_loss: 0.1732 - val_accuracy: 0.9643\n",
      "Epoch 583/600\n",
      "783/783 [==============================] - 0s 139us/step - loss: 0.1751 - accuracy: 0.9911 - val_loss: 0.1731 - val_accuracy: 0.9643\n",
      "Epoch 584/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1935 - accuracy: 0.9898 - val_loss: 0.1731 - val_accuracy: 0.9643\n",
      "Epoch 585/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1738 - accuracy: 0.9949 - val_loss: 0.1730 - val_accuracy: 0.9643\n",
      "Epoch 586/600\n",
      "783/783 [==============================] - 0s 131us/step - loss: 0.1858 - accuracy: 0.9847 - val_loss: 0.1730 - val_accuracy: 0.9643\n",
      "Epoch 587/600\n",
      "783/783 [==============================] - 0s 144us/step - loss: 0.2001 - accuracy: 0.9872 - val_loss: 0.1730 - val_accuracy: 0.9643\n",
      "Epoch 588/600\n",
      "783/783 [==============================] - 0s 123us/step - loss: 0.1766 - accuracy: 0.9949 - val_loss: 0.1729 - val_accuracy: 0.9643\n",
      "Epoch 589/600\n",
      "783/783 [==============================] - 0s 122us/step - loss: 0.1854 - accuracy: 0.9936 - val_loss: 0.1729 - val_accuracy: 0.9643\n",
      "Epoch 590/600\n",
      "783/783 [==============================] - 0s 128us/step - loss: 0.1894 - accuracy: 0.9860 - val_loss: 0.1729 - val_accuracy: 0.9643\n",
      "Epoch 591/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1761 - accuracy: 0.9898 - val_loss: 0.1729 - val_accuracy: 0.9643\n",
      "Epoch 592/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1546 - accuracy: 0.9923 - val_loss: 0.1728 - val_accuracy: 0.9643\n",
      "Epoch 593/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1636 - accuracy: 0.9872 - val_loss: 0.1728 - val_accuracy: 0.9643\n",
      "Epoch 594/600\n",
      "783/783 [==============================] - 0s 124us/step - loss: 0.1845 - accuracy: 0.9872 - val_loss: 0.1728 - val_accuracy: 0.9643\n",
      "Epoch 595/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1798 - accuracy: 0.9898 - val_loss: 0.1728 - val_accuracy: 0.9643\n",
      "Epoch 596/600\n",
      "783/783 [==============================] - 0s 133us/step - loss: 0.1648 - accuracy: 0.9911 - val_loss: 0.1727 - val_accuracy: 0.9643\n",
      "Epoch 597/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1836 - accuracy: 0.9885 - val_loss: 0.1727 - val_accuracy: 0.9643\n",
      "Epoch 598/600\n",
      "783/783 [==============================] - 0s 129us/step - loss: 0.1846 - accuracy: 0.9847 - val_loss: 0.1727 - val_accuracy: 0.9643\n",
      "Epoch 599/600\n",
      "783/783 [==============================] - 0s 127us/step - loss: 0.1821 - accuracy: 0.9898 - val_loss: 0.1727 - val_accuracy: 0.9643\n",
      "Epoch 600/600\n",
      "783/783 [==============================] - 0s 125us/step - loss: 0.1623 - accuracy: 0.9936 - val_loss: 0.1727 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "# it may takes several times to reach the reported performance\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_0,X_1],Y,  \n",
    "            batch_size=len(Y),\n",
    "            epochs=600, #600\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/rlsherc_14_test1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/rlsherc_14_test1.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m], loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper left\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1022\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m-> 1023\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/figure.py:3343\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3339\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3340\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   3341\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m-> 3343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1688\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1689\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/PIL/Image.py:2317\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2320\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/rlsherc_14_test1.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_310_test1.png\")\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD_Net.save_weights('weights/coarse_heavy.h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_310_test2.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20  #20\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    \n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,20,3])\n",
    "        p = sampling_frame(p,C)\n",
    "       \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    DD_Net_model = DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_310_test3.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient for frame selection  -------train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SuvlckHKrJQs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.nn import Linear,ReLU,CrossEntropyLoss,Sequential,Conv2d,MaxPool2d,Module,Softmax,BatchNorm2d,Dropout\n",
    "from torch.optim import Adam,SGD\n",
    "\n",
    "from numpy.random import default_rng\n",
    "from utils import *\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gamma = 0.7\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_size, learning_rate=1e-4):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.n_actions = n_actions \n",
    "\n",
    "        self.cnn_layers = Sequential(Conv2d(1, 32, kernel_size=2),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),\n",
    "                                     Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),\n",
    "                                     Conv2d(64, 128, kernel_size=2, padding=1),\n",
    "                                     ReLU(inplace=True),\n",
    "                                     MaxPool2d(kernel_size=1),) \n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(7680,3)) #32 8448  #7680\n",
    "#         optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = x.float()\n",
    "        x = self.cnn_layers(x)\n",
    "        # x=x.size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x \n",
    "    \n",
    "    def select_action(self, state):  \n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        pr = self.forward(Variable(state))   \n",
    "        act = np.random.choice(self.n_actions, p=np.squeeze(pr.detach().numpy())) \n",
    "        log_pr = torch.log(pr.squeeze(0)[act]) \n",
    "        return act, log_pr\n",
    "  \n",
    "            \n",
    "def update_policy(policy_network, rewards, log_probs): \n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        count = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + gamma**count * r    \n",
    "            count +=count\n",
    "        discounted_rewards.append(Gt)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-4) \n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    policy_gradient = torch.stack(policy_gradient).sum()\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()\n",
    "\n",
    "\n",
    "def calculate_reward(Probs, Probs_history , true_class):\n",
    "  ## Probs is the outcome of softmax layer from classifier CNN # Probs : N_classes * 1\n",
    "  ## Probs_history i the output of previous iteration\n",
    "  ## true_class is an integer from [1-10]\n",
    "  ## iteration is the number of iterations passed from the beginning\n",
    "  omega = 5 # a measure of how strong are the punishments and stimulations\n",
    "  predicted_class = np.argmax(Probs) + 1\n",
    "  prev_predicted_class = np.argmax(Probs_history) + 1  ## +1 is bcz classes are from 1 to 10\n",
    "  \n",
    "  \n",
    "  if (predicted_class == true_class and not(prev_predicted_class == true_class) ):\n",
    "    reward = omega  ## stimulation\n",
    "  elif ( not(predicted_class == true_class) and (prev_predicted_class == true_class) ):\n",
    "    reward = - omega ## punishment\n",
    "  else:\n",
    "    true_class = int(true_class)\n",
    "    reward = (np.sign(Probs[true_class - 1]  - Probs_history[true_class - 1])) ## -1 is bcz classes are from 1 to 10\n",
    "\n",
    "  return reward   \n",
    "\n",
    "def train(oridata,labels): \n",
    "\n",
    "    n_states=32 \n",
    "    n_actions=3\n",
    "    # env.seed(random_seed)\n",
    "    policy_net = Policy(n_states, n_actions, 128) \n",
    "    max_episode_num =  1\n",
    "    max_steps = 32  \n",
    "#     numsteps = []\n",
    "#     avg_numsteps = []\n",
    "    all_rewards = []\n",
    "#     x_trnew=[]\n",
    "    label_new=[]\n",
    "    final_ind=[]\n",
    "    for episode in tqdm(range(max_episode_num)):        \n",
    "        rewards2=[]\n",
    "        x_tr=[]\n",
    "        for v in tqdm(range(len(oridata))):\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "#             x_tr_1=[]\n",
    "            rng = default_rng()\n",
    "            Action=[]\n",
    "            if len(oridata[v])>=32:\n",
    "#                 rng = default_rng()\n",
    "                MM=np.sort(rng.choice(len(oridata[v]),size=32,replace=False))   \n",
    "            else:\n",
    "                MM=np.sort(rng.choice(len(oridata[v]),size=32,replace=True))  \n",
    "            XX=oridata[v] \n",
    "            original_fr=XX.clone().detach() \n",
    "            original_label=labels[v].numpy()\n",
    "            \n",
    "            for steps in range(len(MM)):              \n",
    "                state = original_fr[MM[steps]]\n",
    "                state = state.reshape(1, 1, 20, 3)\n",
    "                state = torch.Tensor(state)\n",
    "                action, log_prob = policy_net.select_action(state)\n",
    "                Action.append(action)\n",
    "\n",
    "                if action == 0 :\n",
    "                    if steps == 0:\n",
    "                        a = 0\n",
    "                    else:\n",
    "                        a = math.ceil(((MM[steps - 1]) + MM[steps]) / 2)\n",
    "                    d = min(1, MM[steps] - a)\n",
    "                    MM[steps] = MM[steps] - d\n",
    "                if action == 1:\n",
    "                    MM[steps] = MM[steps]\n",
    "                if action == 2:\n",
    "                    if steps == len(MM) - 1:\n",
    "                        a = len(original_fr)\n",
    "                    else:\n",
    "                        a = math.ceil((MM[steps] + MM[steps + 1]) / 2)\n",
    "                    d = min(1, a - MM[steps] - 1)\n",
    "                    MM[steps] = MM[steps] + d\n",
    "\n",
    "                xm1 = XX[MM]\n",
    "                xmm = xm1\n",
    "                q = np.array(xm1)\n",
    "                q = zoom(q,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "\n",
    "\n",
    "                M = get_CG(q,C)  \n",
    "                q = q.reshape(1,32,20,3)\n",
    "                M = M.reshape(1,32,190)\n",
    "\n",
    "                original_fr = XX\n",
    "        #             with torch.no_grad():\n",
    "                output = DD_Net.predict([M,q])       \n",
    "                prob = output\n",
    "                prediction = np.argmax(output,axis=1)\n",
    "\n",
    "                prob=prob[0]\n",
    "                if (steps == 0):\n",
    "                  reward = 1 if prediction==original_label else -1    \n",
    "                else:\n",
    "                  reward=calculate_reward(prob, Probs_history ,original_label)\n",
    "                Probs_history=prob\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "            \n",
    "                if steps==len(MM) - 1:\n",
    "                    xm1_step = XX[MM]\n",
    "            x_tr.append(xm1_step)\n",
    "            rewards2.append(np.mean(rewards))\n",
    "            update_policy(policy_net, rewards, log_probs)    \n",
    "            label_new.append(original_label)\n",
    "\n",
    "        x_tr = torch.stack(x_tr)\n",
    "        \n",
    "        if episode==max_episode_num-1:\n",
    "          final_ind.append(Action)\n",
    "        \n",
    "        R=np.sum((rewards2))\n",
    "        all_rewards.append(R)\n",
    " \n",
    "    return all_rewards,x_tr,policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 783/783 [00:00<00:00, 19561.96it/s]\n"
     ]
    }
   ],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train_310.pkl\",\"rb\"))\n",
    "\n",
    "oridata = []\n",
    "\n",
    "for i in tqdm(range(len(Train['pose']))):\n",
    "    ori = np.copy(Train['pose'][i]).reshape([-1,20,3]) \n",
    "    ori = torch.tensor(ori)\n",
    "    oridata.append(ori)\n",
    "    \n",
    "labels = [int(Train['label'][i]) for i in (range(len(Train['pose'])))]\n",
    "labels = torch.Tensor(labels)\n",
    "labels_original = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                   | 0/783 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                           | 1/783 [00:01<21:13,  1.63s/it]\u001b[A\n",
      "  0%|                                           | 2/783 [00:02<14:07,  1.09s/it]\u001b[A\n",
      "  0%|▏                                          | 3/783 [00:03<11:46,  1.10it/s]\u001b[A\n",
      "  1%|▏                                          | 4/783 [00:03<10:53,  1.19it/s]\u001b[A\n",
      "  1%|▎                                          | 5/783 [00:04<10:13,  1.27it/s]\u001b[A\n",
      "  1%|▎                                          | 6/783 [00:05<09:55,  1.31it/s]\u001b[A\n",
      "  1%|▍                                          | 7/783 [00:05<09:38,  1.34it/s]\u001b[A\n",
      "  1%|▍                                          | 8/783 [00:06<09:24,  1.37it/s]\u001b[A\n",
      "  1%|▍                                          | 9/783 [00:07<09:32,  1.35it/s]\u001b[A\n",
      "  1%|▌                                         | 10/783 [00:08<09:29,  1.36it/s]\u001b[A\n",
      "  1%|▌                                         | 11/783 [00:08<09:35,  1.34it/s]\u001b[A\n",
      "  2%|▋                                         | 12/783 [00:09<09:38,  1.33it/s]\u001b[A\n",
      "  2%|▋                                         | 13/783 [00:10<09:31,  1.35it/s]\u001b[A\n",
      "  2%|▊                                         | 14/783 [00:11<09:19,  1.38it/s]\u001b[A\n",
      "  2%|▊                                         | 15/783 [00:11<09:09,  1.40it/s]\u001b[A\n",
      "  2%|▊                                         | 16/783 [00:12<09:05,  1.41it/s]\u001b[A\n",
      "  2%|▉                                         | 17/783 [00:13<09:03,  1.41it/s]\u001b[A\n",
      "  2%|▉                                         | 18/783 [00:13<08:46,  1.45it/s]\u001b[A\n",
      "  2%|█                                         | 19/783 [00:14<08:28,  1.50it/s]\u001b[A\n",
      "  3%|█                                         | 20/783 [00:14<08:15,  1.54it/s]\u001b[A\n",
      "  3%|█▏                                        | 21/783 [00:15<08:03,  1.57it/s]\u001b[A\n",
      "  3%|█▏                                        | 22/783 [00:16<07:53,  1.61it/s]\u001b[A\n",
      "  3%|█▏                                        | 23/783 [00:16<07:59,  1.58it/s]\u001b[A\n",
      "  3%|█▎                                        | 24/783 [00:17<08:11,  1.54it/s]\u001b[A\n",
      "  3%|█▎                                        | 25/783 [00:18<08:23,  1.50it/s]\u001b[A\n",
      "  3%|█▍                                        | 26/783 [00:18<08:17,  1.52it/s]\u001b[A\n",
      "  3%|█▍                                        | 27/783 [00:19<08:09,  1.55it/s]\u001b[A\n",
      "  4%|█▌                                        | 28/783 [00:20<08:17,  1.52it/s]\u001b[A\n",
      "  4%|█▌                                        | 29/783 [00:20<08:19,  1.51it/s]\u001b[A\n",
      "  4%|█▌                                        | 30/783 [00:21<08:13,  1.53it/s]\u001b[A\n",
      "  4%|█▋                                        | 31/783 [00:22<08:24,  1.49it/s]\u001b[A\n",
      "  4%|█▋                                        | 32/783 [00:22<08:31,  1.47it/s]\u001b[A\n",
      "  4%|█▊                                        | 33/783 [00:23<08:51,  1.41it/s]\u001b[A\n",
      "  4%|█▊                                        | 34/783 [00:24<09:07,  1.37it/s]\u001b[A\n",
      "  4%|█▉                                        | 35/783 [00:25<09:15,  1.35it/s]\u001b[A\n",
      "  5%|█▉                                        | 36/783 [00:25<09:25,  1.32it/s]\u001b[A\n",
      "  5%|█▉                                        | 37/783 [00:26<09:26,  1.32it/s]\u001b[A\n",
      "  5%|██                                        | 38/783 [00:27<09:16,  1.34it/s]\u001b[A\n",
      "  5%|██                                        | 39/783 [00:28<09:17,  1.33it/s]\u001b[A\n",
      "  5%|██▏                                       | 40/783 [00:28<09:05,  1.36it/s]\u001b[A\n",
      "  5%|██▏                                       | 41/783 [00:29<09:04,  1.36it/s]\u001b[A\n",
      "  5%|██▎                                       | 42/783 [00:30<09:03,  1.36it/s]\u001b[A\n",
      "  5%|██▎                                       | 43/783 [00:31<09:02,  1.36it/s]\u001b[A\n",
      "  6%|██▎                                       | 44/783 [00:31<09:10,  1.34it/s]\u001b[A\n",
      "  6%|██▍                                       | 45/783 [00:32<09:17,  1.32it/s]\u001b[A\n",
      "  6%|██▍                                       | 46/783 [00:33<09:19,  1.32it/s]\u001b[A\n",
      "  6%|██▌                                       | 47/783 [00:34<09:26,  1.30it/s]\u001b[A\n",
      "  6%|██▌                                       | 48/783 [00:35<09:33,  1.28it/s]\u001b[A\n",
      "  6%|██▋                                       | 49/783 [00:35<09:25,  1.30it/s]\u001b[A\n",
      "  6%|██▋                                       | 50/783 [00:36<09:15,  1.32it/s]\u001b[A\n",
      "  7%|██▋                                       | 51/783 [00:37<09:08,  1.34it/s]\u001b[A\n",
      "  7%|██▊                                       | 52/783 [00:37<09:04,  1.34it/s]\u001b[A\n",
      "  7%|██▊                                       | 53/783 [00:38<08:54,  1.36it/s]\u001b[A\n",
      "  7%|██▉                                       | 54/783 [00:39<08:54,  1.36it/s]\u001b[A\n",
      "  7%|██▉                                       | 55/783 [00:40<09:00,  1.35it/s]\u001b[A\n",
      "  7%|███                                       | 56/783 [00:41<09:14,  1.31it/s]\u001b[A\n",
      "  7%|███                                       | 57/783 [00:41<09:02,  1.34it/s]\u001b[A\n",
      "  7%|███                                       | 58/783 [00:42<09:03,  1.33it/s]\u001b[A\n",
      "  8%|███▏                                      | 59/783 [00:43<09:16,  1.30it/s]\u001b[A\n",
      "  8%|███▏                                      | 60/783 [00:44<09:07,  1.32it/s]\u001b[A\n",
      "  8%|███▎                                      | 61/783 [00:44<08:46,  1.37it/s]\u001b[A\n",
      "  8%|███▎                                      | 62/783 [00:45<08:24,  1.43it/s]\u001b[A\n",
      "  8%|███▍                                      | 63/783 [00:46<08:30,  1.41it/s]\u001b[A\n",
      "  8%|███▍                                      | 64/783 [00:46<08:39,  1.38it/s]\u001b[A\n",
      "  8%|███▍                                      | 65/783 [00:47<08:29,  1.41it/s]\u001b[A\n",
      "  8%|███▌                                      | 66/783 [00:48<08:18,  1.44it/s]\u001b[A\n",
      "  9%|███▌                                      | 67/783 [00:48<08:19,  1.43it/s]\u001b[A\n",
      "  9%|███▋                                      | 68/783 [00:49<08:18,  1.44it/s]\u001b[A\n",
      "  9%|███▋                                      | 69/783 [00:50<08:21,  1.42it/s]\u001b[A\n",
      "  9%|███▊                                      | 70/783 [00:50<08:17,  1.43it/s]\u001b[A\n",
      "  9%|███▊                                      | 71/783 [00:51<08:12,  1.45it/s]\u001b[A\n",
      "  9%|███▊                                      | 72/783 [00:52<08:21,  1.42it/s]\u001b[A\n",
      "  9%|███▉                                      | 73/783 [00:53<08:32,  1.39it/s]\u001b[A\n",
      "  9%|███▉                                      | 74/783 [00:53<08:34,  1.38it/s]\u001b[A\n",
      " 10%|████                                      | 75/783 [00:54<08:36,  1.37it/s]\u001b[A\n",
      " 10%|████                                      | 76/783 [00:55<08:35,  1.37it/s]\u001b[A\n",
      " 10%|████▏                                     | 77/783 [00:56<08:34,  1.37it/s]\u001b[A\n",
      " 10%|████▏                                     | 78/783 [00:56<08:37,  1.36it/s]\u001b[A\n",
      " 10%|████▏                                     | 79/783 [00:57<08:37,  1.36it/s]\u001b[A\n",
      " 10%|████▎                                     | 80/783 [00:58<08:37,  1.36it/s]\u001b[A\n",
      " 10%|████▎                                     | 81/783 [00:58<08:31,  1.37it/s]\u001b[A\n",
      " 10%|████▍                                     | 82/783 [00:59<08:22,  1.40it/s]\u001b[A\n",
      " 11%|████▍                                     | 83/783 [01:00<08:36,  1.36it/s]\u001b[A\n",
      " 11%|████▌                                     | 84/783 [01:01<08:38,  1.35it/s]\u001b[A\n",
      " 11%|████▌                                     | 85/783 [01:01<08:38,  1.35it/s]\u001b[A\n",
      " 11%|████▌                                     | 86/783 [01:02<08:37,  1.35it/s]\u001b[A\n",
      " 11%|████▋                                     | 87/783 [01:03<08:40,  1.34it/s]\u001b[A\n",
      " 11%|████▋                                     | 88/783 [01:04<08:40,  1.34it/s]\u001b[A\n",
      " 11%|████▊                                     | 89/783 [01:04<08:40,  1.33it/s]\u001b[A\n",
      " 11%|████▊                                     | 90/783 [01:05<08:35,  1.34it/s]\u001b[A\n",
      " 12%|████▉                                     | 91/783 [01:06<08:36,  1.34it/s]\u001b[A\n",
      " 12%|████▉                                     | 92/783 [01:07<08:34,  1.34it/s]\u001b[A\n",
      " 12%|████▉                                     | 93/783 [01:07<08:31,  1.35it/s]\u001b[A\n",
      " 12%|█████                                     | 94/783 [01:08<08:33,  1.34it/s]\u001b[A\n",
      " 12%|█████                                     | 95/783 [01:09<08:37,  1.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▏                                    | 96/783 [01:10<08:32,  1.34it/s]\u001b[A\n",
      " 12%|█████▏                                    | 97/783 [01:10<08:36,  1.33it/s]\u001b[A\n",
      " 13%|█████▎                                    | 98/783 [01:11<08:38,  1.32it/s]\u001b[A\n",
      " 13%|█████▎                                    | 99/783 [01:12<08:37,  1.32it/s]\u001b[A\n",
      " 13%|█████▏                                   | 100/783 [01:13<08:38,  1.32it/s]\u001b[A\n",
      " 13%|█████▎                                   | 101/783 [01:13<08:29,  1.34it/s]\u001b[A\n",
      " 13%|█████▎                                   | 102/783 [01:14<08:16,  1.37it/s]\u001b[A\n",
      " 13%|█████▍                                   | 103/783 [01:15<08:21,  1.36it/s]\u001b[A\n",
      " 13%|█████▍                                   | 104/783 [01:16<08:28,  1.34it/s]\u001b[A\n",
      " 13%|█████▍                                   | 105/783 [01:16<08:29,  1.33it/s]\u001b[A\n",
      " 14%|█████▌                                   | 106/783 [01:17<08:27,  1.34it/s]\u001b[A\n",
      " 14%|█████▌                                   | 107/783 [01:18<08:29,  1.33it/s]\u001b[A\n",
      " 14%|█████▋                                   | 108/783 [01:19<08:31,  1.32it/s]\u001b[A\n",
      " 14%|█████▋                                   | 109/783 [01:19<08:30,  1.32it/s]\u001b[A\n",
      " 14%|█████▊                                   | 110/783 [01:20<08:27,  1.33it/s]\u001b[A\n",
      " 14%|█████▊                                   | 111/783 [01:21<08:23,  1.33it/s]\u001b[A\n",
      " 14%|█████▊                                   | 112/783 [01:22<08:23,  1.33it/s]\u001b[A\n",
      " 14%|█████▉                                   | 113/783 [01:22<08:28,  1.32it/s]\u001b[A\n",
      " 15%|█████▉                                   | 114/783 [01:23<08:33,  1.30it/s]\u001b[A\n",
      " 15%|██████                                   | 115/783 [01:24<08:31,  1.31it/s]\u001b[A\n",
      " 15%|██████                                   | 116/783 [01:25<08:25,  1.32it/s]\u001b[A\n",
      " 15%|██████▏                                  | 117/783 [01:25<08:19,  1.33it/s]\u001b[A\n",
      " 15%|██████▏                                  | 118/783 [01:26<08:17,  1.34it/s]\u001b[A\n",
      " 15%|██████▏                                  | 119/783 [01:27<08:15,  1.34it/s]\u001b[A\n",
      " 15%|██████▎                                  | 120/783 [01:28<08:13,  1.34it/s]\u001b[A\n",
      " 15%|██████▎                                  | 121/783 [01:28<08:12,  1.35it/s]\u001b[A\n",
      " 16%|██████▍                                  | 122/783 [01:29<08:11,  1.35it/s]\u001b[A\n",
      " 16%|██████▍                                  | 123/783 [01:30<08:14,  1.34it/s]\u001b[A\n",
      " 16%|██████▍                                  | 124/783 [01:31<08:13,  1.34it/s]\u001b[A\n",
      " 16%|██████▌                                  | 125/783 [01:31<08:19,  1.32it/s]\u001b[A\n",
      " 16%|██████▌                                  | 126/783 [01:32<08:15,  1.33it/s]\u001b[A\n",
      " 16%|██████▋                                  | 127/783 [01:33<08:08,  1.34it/s]\u001b[A\n",
      " 16%|██████▋                                  | 128/783 [01:34<08:03,  1.35it/s]\u001b[A\n",
      " 16%|██████▊                                  | 129/783 [01:34<08:03,  1.35it/s]\u001b[A\n",
      " 17%|██████▊                                  | 130/783 [01:35<08:06,  1.34it/s]\u001b[A\n",
      " 17%|██████▊                                  | 131/783 [01:36<08:06,  1.34it/s]\u001b[A\n",
      " 17%|██████▉                                  | 132/783 [01:37<08:13,  1.32it/s]\u001b[A\n",
      " 17%|██████▉                                  | 133/783 [01:37<08:09,  1.33it/s]\u001b[A\n",
      " 17%|███████                                  | 134/783 [01:38<08:08,  1.33it/s]\u001b[A\n",
      " 17%|███████                                  | 135/783 [01:39<08:05,  1.33it/s]\u001b[A\n",
      " 17%|███████                                  | 136/783 [01:40<08:06,  1.33it/s]\u001b[A\n",
      " 17%|███████▏                                 | 137/783 [01:40<08:05,  1.33it/s]\u001b[A\n",
      " 18%|███████▏                                 | 138/783 [01:41<08:02,  1.34it/s]\u001b[A\n",
      " 18%|███████▎                                 | 139/783 [01:42<08:03,  1.33it/s]\u001b[A\n",
      " 18%|███████▎                                 | 140/783 [01:43<08:05,  1.32it/s]\u001b[A\n",
      " 18%|███████▍                                 | 141/783 [01:43<08:10,  1.31it/s]\u001b[A\n",
      " 18%|███████▍                                 | 142/783 [01:44<08:03,  1.32it/s]\u001b[A\n",
      " 18%|███████▍                                 | 143/783 [01:45<07:58,  1.34it/s]\u001b[A\n",
      " 18%|███████▌                                 | 144/783 [01:46<07:39,  1.39it/s]\u001b[A\n",
      " 19%|███████▌                                 | 145/783 [01:46<07:35,  1.40it/s]\u001b[A\n",
      " 19%|███████▋                                 | 146/783 [01:47<07:29,  1.42it/s]\u001b[A\n",
      " 19%|███████▋                                 | 147/783 [01:48<07:29,  1.42it/s]\u001b[A\n",
      " 19%|███████▋                                 | 148/783 [01:48<07:22,  1.44it/s]\u001b[A\n",
      " 19%|███████▊                                 | 149/783 [01:49<07:09,  1.47it/s]\u001b[A\n",
      " 19%|███████▊                                 | 150/783 [01:50<07:09,  1.47it/s]\u001b[A\n",
      " 19%|███████▉                                 | 151/783 [01:50<07:05,  1.48it/s]\u001b[A\n",
      " 19%|███████▉                                 | 152/783 [01:51<07:09,  1.47it/s]\u001b[A\n",
      " 20%|████████                                 | 153/783 [01:52<07:09,  1.47it/s]\u001b[A\n",
      " 20%|████████                                 | 154/783 [01:52<07:11,  1.46it/s]\u001b[A\n",
      " 20%|████████                                 | 155/783 [01:53<07:15,  1.44it/s]\u001b[A\n",
      " 20%|████████▏                                | 156/783 [01:54<07:12,  1.45it/s]\u001b[A\n",
      " 20%|████████▏                                | 157/783 [01:55<07:35,  1.37it/s]\u001b[A\n",
      " 20%|████████▎                                | 158/783 [01:55<07:49,  1.33it/s]\u001b[A\n",
      " 20%|████████▎                                | 159/783 [01:56<07:31,  1.38it/s]\u001b[A\n",
      " 20%|████████▍                                | 160/783 [01:57<07:47,  1.33it/s]\u001b[A\n",
      " 21%|████████▍                                | 161/783 [01:58<07:37,  1.36it/s]\u001b[A\n",
      " 21%|████████▍                                | 162/783 [01:58<07:31,  1.37it/s]\u001b[A\n",
      " 21%|████████▌                                | 163/783 [01:59<07:38,  1.35it/s]\u001b[A\n",
      " 21%|████████▌                                | 164/783 [02:00<07:34,  1.36it/s]\u001b[A\n",
      " 21%|████████▋                                | 165/783 [02:01<07:33,  1.36it/s]\u001b[A\n",
      " 21%|████████▋                                | 166/783 [02:01<07:25,  1.38it/s]\u001b[A\n",
      " 21%|████████▋                                | 167/783 [02:02<07:25,  1.38it/s]\u001b[A\n",
      " 21%|████████▊                                | 168/783 [02:03<07:30,  1.36it/s]\u001b[A\n",
      " 22%|████████▊                                | 169/783 [02:03<07:32,  1.36it/s]\u001b[A\n",
      " 22%|████████▉                                | 170/783 [02:04<07:30,  1.36it/s]\u001b[A\n",
      " 22%|████████▉                                | 171/783 [02:05<07:28,  1.36it/s]\u001b[A\n",
      " 22%|█████████                                | 172/783 [02:06<07:31,  1.35it/s]\u001b[A\n",
      " 22%|█████████                                | 173/783 [02:06<07:34,  1.34it/s]\u001b[A\n",
      " 22%|█████████                                | 174/783 [02:07<07:27,  1.36it/s]\u001b[A\n",
      " 22%|█████████▏                               | 175/783 [02:08<07:23,  1.37it/s]\u001b[A\n",
      " 22%|█████████▏                               | 176/783 [02:09<07:31,  1.34it/s]\u001b[A\n",
      " 23%|█████████▎                               | 177/783 [02:09<07:30,  1.34it/s]\u001b[A\n",
      " 23%|█████████▎                               | 178/783 [02:10<07:35,  1.33it/s]\u001b[A\n",
      " 23%|█████████▎                               | 179/783 [02:11<07:29,  1.34it/s]\u001b[A\n",
      " 23%|█████████▍                               | 180/783 [02:12<07:24,  1.36it/s]\u001b[A\n",
      " 23%|█████████▍                               | 181/783 [02:12<07:14,  1.39it/s]\u001b[A\n",
      " 23%|█████████▌                               | 182/783 [02:13<07:19,  1.37it/s]\u001b[A\n",
      " 23%|█████████▌                               | 183/783 [02:14<07:12,  1.39it/s]\u001b[A\n",
      " 23%|█████████▋                               | 184/783 [02:14<07:16,  1.37it/s]\u001b[A\n",
      " 24%|█████████▋                               | 185/783 [02:15<07:15,  1.37it/s]\u001b[A\n",
      " 24%|█████████▋                               | 186/783 [02:16<07:28,  1.33it/s]\u001b[A\n",
      " 24%|█████████▊                               | 187/783 [02:17<07:12,  1.38it/s]\u001b[A\n",
      " 24%|█████████▊                               | 188/783 [02:17<06:54,  1.43it/s]\u001b[A\n",
      " 24%|█████████▉                               | 189/783 [02:18<06:42,  1.48it/s]\u001b[A\n",
      " 24%|█████████▉                               | 190/783 [02:19<06:30,  1.52it/s]\u001b[A\n",
      " 24%|██████████                               | 191/783 [02:19<06:20,  1.56it/s]\u001b[A\n",
      " 25%|██████████                               | 192/783 [02:20<06:18,  1.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████                               | 193/783 [02:20<06:12,  1.58it/s]\u001b[A\n",
      " 25%|██████████▏                              | 194/783 [02:21<06:08,  1.60it/s]\u001b[A\n",
      " 25%|██████████▏                              | 195/783 [02:22<06:05,  1.61it/s]\u001b[A\n",
      " 25%|██████████▎                              | 196/783 [02:22<06:04,  1.61it/s]\u001b[A\n",
      " 25%|██████████▎                              | 197/783 [02:23<06:03,  1.61it/s]\u001b[A\n",
      " 25%|██████████▎                              | 198/783 [02:24<06:28,  1.51it/s]\u001b[A\n",
      " 25%|██████████▍                              | 199/783 [02:24<06:36,  1.47it/s]\u001b[A\n",
      " 26%|██████████▍                              | 200/783 [02:25<06:49,  1.42it/s]\u001b[A\n",
      " 26%|██████████▌                              | 201/783 [02:26<06:55,  1.40it/s]\u001b[A\n",
      " 26%|██████████▌                              | 202/783 [02:27<06:52,  1.41it/s]\u001b[A\n",
      " 26%|██████████▋                              | 203/783 [02:27<06:49,  1.42it/s]\u001b[A\n",
      " 26%|██████████▋                              | 204/783 [02:28<06:49,  1.41it/s]\u001b[A\n",
      " 26%|██████████▋                              | 205/783 [02:29<06:48,  1.41it/s]\u001b[A\n",
      " 26%|██████████▊                              | 206/783 [02:29<06:45,  1.42it/s]\u001b[A\n",
      " 26%|██████████▊                              | 207/783 [02:30<06:43,  1.43it/s]\u001b[A\n",
      " 27%|██████████▉                              | 208/783 [02:31<06:42,  1.43it/s]\u001b[A\n",
      " 27%|██████████▉                              | 209/783 [02:31<06:41,  1.43it/s]\u001b[A\n",
      " 27%|██████████▉                              | 210/783 [02:32<06:41,  1.43it/s]\u001b[A\n",
      " 27%|███████████                              | 211/783 [02:33<06:40,  1.43it/s]\u001b[A\n",
      " 27%|███████████                              | 212/783 [02:34<06:40,  1.42it/s]\u001b[A\n",
      " 27%|███████████▏                             | 213/783 [02:34<06:51,  1.39it/s]\u001b[A\n",
      " 27%|███████████▏                             | 214/783 [02:35<06:52,  1.38it/s]\u001b[A\n",
      " 27%|███████████▎                             | 215/783 [02:36<06:52,  1.38it/s]\u001b[A\n",
      " 28%|███████████▎                             | 216/783 [02:36<06:45,  1.40it/s]\u001b[A\n",
      " 28%|███████████▎                             | 217/783 [02:37<06:42,  1.41it/s]\u001b[A\n",
      " 28%|███████████▍                             | 218/783 [02:38<06:40,  1.41it/s]\u001b[A\n",
      " 28%|███████████▍                             | 219/783 [02:39<06:40,  1.41it/s]\u001b[A\n",
      " 28%|███████████▌                             | 220/783 [02:39<06:32,  1.43it/s]\u001b[A\n",
      " 28%|███████████▌                             | 221/783 [02:40<06:28,  1.45it/s]\u001b[A\n",
      " 28%|███████████▌                             | 222/783 [02:41<06:30,  1.44it/s]\u001b[A\n",
      " 28%|███████████▋                             | 223/783 [02:41<06:29,  1.44it/s]\u001b[A\n",
      " 29%|███████████▋                             | 224/783 [02:42<06:26,  1.44it/s]\u001b[A\n",
      " 29%|███████████▊                             | 225/783 [02:43<06:27,  1.44it/s]\u001b[A\n",
      " 29%|███████████▊                             | 226/783 [02:43<06:27,  1.44it/s]\u001b[A\n",
      " 29%|███████████▉                             | 227/783 [02:44<06:27,  1.43it/s]\u001b[A\n",
      " 29%|███████████▉                             | 228/783 [02:45<06:31,  1.42it/s]\u001b[A\n",
      " 29%|███████████▉                             | 229/783 [02:46<06:36,  1.40it/s]\u001b[A\n",
      " 29%|████████████                             | 230/783 [02:46<06:36,  1.40it/s]\u001b[A\n",
      " 30%|████████████                             | 231/783 [02:47<06:35,  1.40it/s]\u001b[A\n",
      " 30%|████████████▏                            | 232/783 [02:48<06:37,  1.39it/s]\u001b[A\n",
      " 30%|████████████▏                            | 233/783 [02:48<06:28,  1.41it/s]\u001b[A\n",
      " 30%|████████████▎                            | 234/783 [02:49<06:27,  1.42it/s]\u001b[A\n",
      " 30%|████████████▎                            | 235/783 [02:50<06:22,  1.43it/s]\u001b[A\n",
      " 30%|████████████▎                            | 236/783 [02:51<06:19,  1.44it/s]\u001b[A\n",
      " 30%|████████████▍                            | 237/783 [02:51<06:16,  1.45it/s]\u001b[A\n",
      " 30%|████████████▍                            | 238/783 [02:52<06:17,  1.44it/s]\u001b[A\n",
      " 31%|████████████▌                            | 239/783 [02:53<06:16,  1.44it/s]\u001b[A\n",
      " 31%|████████████▌                            | 240/783 [02:53<06:14,  1.45it/s]\u001b[A\n",
      " 31%|████████████▌                            | 241/783 [02:54<06:06,  1.48it/s]\u001b[A\n",
      " 31%|████████████▋                            | 242/783 [02:55<06:19,  1.43it/s]\u001b[A\n",
      " 31%|████████████▋                            | 243/783 [02:55<06:22,  1.41it/s]\u001b[A\n",
      " 31%|████████████▊                            | 244/783 [02:56<06:26,  1.39it/s]\u001b[A\n",
      " 31%|████████████▊                            | 245/783 [02:57<06:19,  1.42it/s]\u001b[A\n",
      " 31%|████████████▉                            | 246/783 [02:58<06:25,  1.39it/s]\u001b[A\n",
      " 32%|████████████▉                            | 247/783 [02:58<06:26,  1.39it/s]\u001b[A\n",
      " 32%|████████████▉                            | 248/783 [02:59<06:24,  1.39it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "all_rewards,x_trnew,policy_net=train(oridata,labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "wF3Umjarn2RI",
    "outputId": "ec5ca4b1-6545-4c75-debc-808c4d0993ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_rewards)\n",
    "plt.plot(all_rewards)\n",
    "plt.xlabel('number of epoches')\n",
    "plt.ylabel('the average reward')\n",
    "plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_310_test4.png\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew))):\n",
    "    w_p = np.copy(x_trnew[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it may takes several times to reach the reported performance\n",
    "\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_310_test5.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_2,policy_net=train(x_trnew,labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_rewards)\n",
    "#plt.plot(all_rewards)\n",
    "#plt.xlabel('number of epoches')\n",
    "#plt.ylabel('the average reward')\n",
    "#plt.savefig(\"/data/HRC/paper1-RLDDNet/code/Main/DD-Net-master/FPHAB/images/DRLDDNet_310_test2.png\")\n",
    "#plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_2))):\n",
    "    w_p = np.copy(x_trnew_2[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/DRLDDNet_310_test6.png', labels, ymap=None, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_3,policy_net=train(x_trnew_2,labels_original)\n",
    "print(all_rewards)\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_3))):\n",
    "    w_p = np.copy(x_trnew_3[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl, \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_4,policy_net=train(x_trnew_3,labels_original)\n",
    "print(all_rewards)\n",
    "\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_4))):\n",
    "    w_p = np.copy(x_trnew_4[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards,x_trnew_5,policy_net=train(x_trnew_4,labels_original)\n",
    "print(all_rewards)\n",
    "\n",
    "X_4 =[]\n",
    "X_5 = []\n",
    "labels_rl = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(x_trnew_5))):\n",
    "    w_p = np.copy(x_trnew_5[i]).reshape([-1,20,3]) \n",
    "    w_p = np.array(w_p)\n",
    "    w_p = zoom(w_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    w_p = normlize_range(w_p)\n",
    "\n",
    "    w_M = get_CG(w_p,C)\n",
    "\n",
    "    label_rl = np.zeros(C.clc_coarse)\n",
    "    label_rl[int(labels_original[i]) - 1] = 1 \n",
    "\n",
    "    X_4.append(w_M)\n",
    "    X_5.append(w_p)\n",
    "    labels_rl.append(label_rl)\n",
    "\n",
    "X_4 = np.stack(X_4)\n",
    "X_5 = np.stack(X_5)\n",
    "labels_rl = np.stack(labels_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl, \n",
    "            batch_size=len(labels_rl),\n",
    "            epochs=600, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "labels = ['open juice', 'close juice', 'pour juice', 'open peanut b.', 'close peanut b.', 'prick fork', 'sprinkle spoon', 'scoop spoon', 'put sugar sp.', 'stir spoon', 'open milk', 'close milk', 'pour milk', 'drink mug', 'put tea bag', 'put salt', 'open dish soap','close dish soap','pour dish soap','wash sponge','flip sponge','scratch sponge','sequeeze sponge','open soda can','use spray','write pen','tear paper','squeeze paper','open letter','take out letter','read paper','flip pages','use calculator','light candle','charge cell','unfold glasses','clean glasses','open wallet','pay coin','receive coin','give card','pour wine','toast glass','handshake','high five'] #95.85\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])\n",
    "print('Accuracy:{}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test accuracy   -through RL get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 840/840 [00:06<00:00, 127.62it/s]\n"
     ]
    }
   ],
   "source": [
    "Test = pickle.load(open(C.data_dir+\"test.pkl\",\"rb\"))\n",
    "\n",
    "X_test_2 = []\n",
    "X_test_3 = []\n",
    "labels_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    test_or_p = np.copy(Test['pose'][i]).reshape([-1,22,3])\n",
    "    test_or_p = zoom(test_or_p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    test_or_p = normlize_range(test_or_p)\n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['coarse_label'][i]-1] = 1   \n",
    "    \n",
    "    M = get_CG(p,C)\n",
    "       \n",
    "    p = torch.tensor(p)\n",
    "    M = torch.tensor(M)\n",
    "\n",
    "    X_test_2.append(M)\n",
    "    X_test_3.append(p)\n",
    "    labels_test.append(label)\n",
    "\n",
    "X_test_2 = np.stack(X_test_0) \n",
    "X_test_3 = np.stack(X_test_1)  \n",
    "labels_test  = np.stack(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 840/840 [00:11<00:00, 72.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(X_TEST): #x表示x_main，输入全局帧  label表示标签 \n",
    "#     def train(x_m,x,oridata,labels): #x表示x_main，输入全局帧  label表示标签 \n",
    "    n_states=20 #状态数量 这里应该指关键帧f的数量为30\n",
    "    n_actions=3 #输出动作\n",
    "    # env.seed(random_seed)\n",
    "#     policy_net = Policy(n_states, n_actions, 128) #策略选择网络 12\n",
    "#     Policy  = torch.load('model/policy_net.pth')\n",
    "    policy_net = Policy(n_states, n_actions, 128) #策略选择网络 12\n",
    "    max_steps = 20  #最大\n",
    "    XM=[]\n",
    "    for v in tqdm(range(len(X_TEST))):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        Action=[]\n",
    "        if len(oridata[v])>=10:\n",
    "            rng = default_rng()\n",
    "            MM=np.sort(rng.choice(len(X_TEST[v]),size=10,replace=False))   \n",
    "        else:\n",
    "            MM=np.sort(rng.choice(len(X_TEST[v]),size=10,replace=True))  \n",
    "        XX=X_TEST[v]  #输入数据\n",
    "        for steps in range(len(MM)):\n",
    "            state = XX[MM[steps]]\n",
    "            state = state.reshape(1, 1, 22, 3)\n",
    "            state = torch.Tensor(state)\n",
    "            action, log_prob = policy_net.select_action(state)\n",
    "#             action, log_prob = policy_net(state)\n",
    "            Action.append(action)\n",
    "\n",
    "            if action == 0 :\n",
    "                if steps == 0:\n",
    "                    a = 0\n",
    "                else:\n",
    "                    a = math.ceil(((MM[steps - 1]) + MM[steps]) / 2)\n",
    "                d = min(1, MM[steps] - a)\n",
    "                MM[steps] = MM[steps] - d\n",
    "            if action == 1:\n",
    "                MM[steps] = MM[steps]\n",
    "            if action == 2:\n",
    "                if steps == len(MM) - 1:\n",
    "                    a = len(XX)\n",
    "                else:\n",
    "                    a = math.ceil((MM[steps] + MM[steps + 1]) / 2)\n",
    "                d = min(1, a - MM[steps] - 1)\n",
    "                MM[steps] = MM[steps] + d\n",
    "        xm1 = XX[MM]\n",
    "        XM.append(xm1)\n",
    "                      \n",
    "    return XM\n",
    "    \n",
    "XM = test(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 840/840 [00:05<00:00, 141.03it/s]\n"
     ]
    }
   ],
   "source": [
    "test_0 =[]\n",
    "test_1 = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(XM))):\n",
    "    test_p = np.copy(XM[i]).reshape([-1,22,3]) \n",
    "    test_p = np.array(test_p)\n",
    "    test_p = zoom(test_p,target_l = C.frame_l,joints_num = C.joint_n, joints_dim = C.joint_d)\n",
    "    test_p = normlize_range(test_p)\n",
    "\n",
    "    test_M = get_CG(test_p,C)\n",
    "\n",
    "    test_0.append(test_M)\n",
    "    test_1.append(test_p)\n",
    "\n",
    "test_0 = np.stack(test_0)\n",
    "test_1 = np.stack(test_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/700\n",
      "1960/1960 [==============================] - 4s 2ms/step - loss: 0.0150 - accuracy: 0.9985 - val_loss: 0.4157 - val_accuracy: 0.9167\n",
      "Epoch 2/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.1000 - accuracy: 0.9709 - val_loss: 0.4984 - val_accuracy: 0.9000\n",
      "Epoch 3/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0512 - accuracy: 0.9862 - val_loss: 0.6469 - val_accuracy: 0.8833\n",
      "Epoch 4/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0815 - accuracy: 0.9760 - val_loss: 0.6361 - val_accuracy: 0.8762\n",
      "Epoch 5/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0933 - accuracy: 0.9750 - val_loss: 0.6273 - val_accuracy: 0.8774\n",
      "Epoch 6/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0492 - accuracy: 0.9888 - val_loss: 0.6433 - val_accuracy: 0.8833\n",
      "Epoch 7/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0514 - accuracy: 0.9867 - val_loss: 0.6570 - val_accuracy: 0.8762\n",
      "Epoch 8/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0380 - accuracy: 0.9913 - val_loss: 0.6619 - val_accuracy: 0.8786\n",
      "Epoch 9/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0499 - accuracy: 0.9883 - val_loss: 0.6607 - val_accuracy: 0.8762\n",
      "Epoch 10/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0532 - accuracy: 0.9857 - val_loss: 0.6619 - val_accuracy: 0.8798\n",
      "Epoch 11/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0403 - accuracy: 0.9888 - val_loss: 0.6478 - val_accuracy: 0.8821\n",
      "Epoch 12/700\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0456 - accuracy: 0.9852 - val_loss: 0.6147 - val_accuracy: 0.8940\n",
      "Epoch 13/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0466 - accuracy: 0.9893 - val_loss: 0.5867 - val_accuracy: 0.9000\n",
      "Epoch 14/700\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0421 - accuracy: 0.9883 - val_loss: 0.5682 - val_accuracy: 0.9060\n",
      "Epoch 15/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0360 - accuracy: 0.9893 - val_loss: 0.5501 - val_accuracy: 0.9036\n",
      "Epoch 16/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0370 - accuracy: 0.9908 - val_loss: 0.5421 - val_accuracy: 0.9048\n",
      "Epoch 17/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0348 - accuracy: 0.9949 - val_loss: 0.5357 - val_accuracy: 0.9083\n",
      "Epoch 18/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0325 - accuracy: 0.9923 - val_loss: 0.5287 - val_accuracy: 0.9107\n",
      "Epoch 19/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0261 - accuracy: 0.9954 - val_loss: 0.5214 - val_accuracy: 0.9107\n",
      "Epoch 20/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0303 - accuracy: 0.9939 - val_loss: 0.5144 - val_accuracy: 0.9107\n",
      "Epoch 21/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0258 - accuracy: 0.9944 - val_loss: 0.5093 - val_accuracy: 0.9095\n",
      "Epoch 22/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0307 - accuracy: 0.9944 - val_loss: 0.5047 - val_accuracy: 0.9107\n",
      "Epoch 23/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0252 - accuracy: 0.9959 - val_loss: 0.5015 - val_accuracy: 0.9107\n",
      "Epoch 24/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0189 - accuracy: 0.9974 - val_loss: 0.4990 - val_accuracy: 0.9119\n",
      "Epoch 25/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0314 - accuracy: 0.9949 - val_loss: 0.4967 - val_accuracy: 0.9131\n",
      "Epoch 26/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0315 - accuracy: 0.9918 - val_loss: 0.4953 - val_accuracy: 0.9143\n",
      "Epoch 27/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0244 - accuracy: 0.9954 - val_loss: 0.4945 - val_accuracy: 0.9155\n",
      "Epoch 28/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0330 - accuracy: 0.9929 - val_loss: 0.4953 - val_accuracy: 0.9143\n",
      "Epoch 29/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0287 - accuracy: 0.9949 - val_loss: 0.4957 - val_accuracy: 0.9131\n",
      "Epoch 30/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0216 - accuracy: 0.9964 - val_loss: 0.4959 - val_accuracy: 0.9131\n",
      "Epoch 31/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0239 - accuracy: 0.9964 - val_loss: 0.4955 - val_accuracy: 0.9131\n",
      "Epoch 32/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0197 - accuracy: 0.9969 - val_loss: 0.4952 - val_accuracy: 0.9131\n",
      "Epoch 33/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.4943 - val_accuracy: 0.9119\n",
      "Epoch 34/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0197 - accuracy: 0.9964 - val_loss: 0.4937 - val_accuracy: 0.9119\n",
      "Epoch 35/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0273 - accuracy: 0.9949 - val_loss: 0.4930 - val_accuracy: 0.9119\n",
      "Epoch 36/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0202 - accuracy: 0.9964 - val_loss: 0.4929 - val_accuracy: 0.9107\n",
      "Epoch 37/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0232 - accuracy: 0.9969 - val_loss: 0.4928 - val_accuracy: 0.9131\n",
      "Epoch 38/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0242 - accuracy: 0.9954 - val_loss: 0.4923 - val_accuracy: 0.9131\n",
      "Epoch 39/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0194 - accuracy: 0.9974 - val_loss: 0.4920 - val_accuracy: 0.9131\n",
      "Epoch 40/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0218 - accuracy: 0.9944 - val_loss: 0.4916 - val_accuracy: 0.9119\n",
      "Epoch 41/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0215 - accuracy: 0.9980 - val_loss: 0.4912 - val_accuracy: 0.9119\n",
      "Epoch 42/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0257 - accuracy: 0.9939 - val_loss: 0.4917 - val_accuracy: 0.9119\n",
      "Epoch 43/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.4914 - val_accuracy: 0.9119\n",
      "Epoch 44/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0233 - accuracy: 0.9969 - val_loss: 0.4911 - val_accuracy: 0.9119\n",
      "Epoch 45/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0234 - accuracy: 0.9969 - val_loss: 0.4906 - val_accuracy: 0.9119\n",
      "Epoch 46/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0248 - accuracy: 0.9949 - val_loss: 0.4896 - val_accuracy: 0.9119\n",
      "Epoch 47/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0245 - accuracy: 0.9934 - val_loss: 0.4883 - val_accuracy: 0.9119\n",
      "Epoch 48/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0186 - accuracy: 0.9969 - val_loss: 0.4870 - val_accuracy: 0.9131\n",
      "Epoch 49/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.4857 - val_accuracy: 0.9131\n",
      "Epoch 50/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0191 - accuracy: 0.9974 - val_loss: 0.4846 - val_accuracy: 0.9131\n",
      "Epoch 51/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.4835 - val_accuracy: 0.9143\n",
      "Epoch 52/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0164 - accuracy: 0.9985 - val_loss: 0.4824 - val_accuracy: 0.9155\n",
      "Epoch 53/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0246 - accuracy: 0.9944 - val_loss: 0.4811 - val_accuracy: 0.9155\n",
      "Epoch 54/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0227 - accuracy: 0.9949 - val_loss: 0.4798 - val_accuracy: 0.9167\n",
      "Epoch 55/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0234 - accuracy: 0.9944 - val_loss: 0.4783 - val_accuracy: 0.9167\n",
      "Epoch 56/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0204 - accuracy: 0.9980 - val_loss: 0.4769 - val_accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/700\n",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.4755 - val_accuracy: 0.9167\n",
      "Epoch 58/700\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0197 - accuracy: 0.9969 - val_loss: 0.4742 - val_accuracy: 0.9167\n",
      "Epoch 59/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 0.4730 - val_accuracy: 0.9179\n",
      "Epoch 60/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0266 - accuracy: 0.9949 - val_loss: 0.4718 - val_accuracy: 0.9179\n",
      "Epoch 61/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.4709 - val_accuracy: 0.9179\n",
      "Epoch 62/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0184 - accuracy: 0.9985 - val_loss: 0.4700 - val_accuracy: 0.9179\n",
      "Epoch 63/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0197 - accuracy: 0.9969 - val_loss: 0.4691 - val_accuracy: 0.9179\n",
      "Epoch 64/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0161 - accuracy: 0.9974 - val_loss: 0.4682 - val_accuracy: 0.9179\n",
      "Epoch 65/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.4674 - val_accuracy: 0.9179\n",
      "Epoch 66/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0182 - accuracy: 0.9974 - val_loss: 0.4666 - val_accuracy: 0.9179\n",
      "Epoch 67/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0162 - accuracy: 0.9974 - val_loss: 0.4658 - val_accuracy: 0.9179\n",
      "Epoch 68/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.4651 - val_accuracy: 0.9179\n",
      "Epoch 69/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0179 - accuracy: 0.9974 - val_loss: 0.4643 - val_accuracy: 0.9179\n",
      "Epoch 70/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0187 - accuracy: 0.9974 - val_loss: 0.4635 - val_accuracy: 0.9179\n",
      "Epoch 71/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0201 - accuracy: 0.9969 - val_loss: 0.4628 - val_accuracy: 0.9179\n",
      "Epoch 72/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0169 - accuracy: 0.9985 - val_loss: 0.4621 - val_accuracy: 0.9179\n",
      "Epoch 73/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0189 - accuracy: 0.9969 - val_loss: 0.4615 - val_accuracy: 0.9179\n",
      "Epoch 74/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.4609 - val_accuracy: 0.9179\n",
      "Epoch 75/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.4602 - val_accuracy: 0.9179\n",
      "Epoch 76/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.4596 - val_accuracy: 0.9179\n",
      "Epoch 77/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.4590 - val_accuracy: 0.9179\n",
      "Epoch 78/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0251 - accuracy: 0.9944 - val_loss: 0.4584 - val_accuracy: 0.9179\n",
      "Epoch 79/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0305 - accuracy: 0.9939 - val_loss: 0.4578 - val_accuracy: 0.9179\n",
      "Epoch 80/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0197 - accuracy: 0.9964 - val_loss: 0.4574 - val_accuracy: 0.9179\n",
      "Epoch 81/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0224 - accuracy: 0.9959 - val_loss: 0.4569 - val_accuracy: 0.9179\n",
      "Epoch 82/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 0.4564 - val_accuracy: 0.9179\n",
      "Epoch 83/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0261 - accuracy: 0.9939 - val_loss: 0.4559 - val_accuracy: 0.9190\n",
      "Epoch 84/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0188 - accuracy: 0.9964 - val_loss: 0.4554 - val_accuracy: 0.9190\n",
      "Epoch 85/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0204 - accuracy: 0.9980 - val_loss: 0.4548 - val_accuracy: 0.9190\n",
      "Epoch 86/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0241 - accuracy: 0.9959 - val_loss: 0.4543 - val_accuracy: 0.9190\n",
      "Epoch 87/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0228 - accuracy: 0.9959 - val_loss: 0.4538 - val_accuracy: 0.9190\n",
      "Epoch 88/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0226 - accuracy: 0.9974 - val_loss: 0.4532 - val_accuracy: 0.9190\n",
      "Epoch 89/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0229 - accuracy: 0.9959 - val_loss: 0.4527 - val_accuracy: 0.9202\n",
      "Epoch 90/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0193 - accuracy: 0.9969 - val_loss: 0.4522 - val_accuracy: 0.9202\n",
      "Epoch 91/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0197 - accuracy: 0.9964 - val_loss: 0.4517 - val_accuracy: 0.9202\n",
      "Epoch 92/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0194 - accuracy: 0.9969 - val_loss: 0.4512 - val_accuracy: 0.9202\n",
      "Epoch 93/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 0.4507 - val_accuracy: 0.9202\n",
      "Epoch 94/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0199 - accuracy: 0.9959 - val_loss: 0.4502 - val_accuracy: 0.9202\n",
      "Epoch 95/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0211 - accuracy: 0.9964 - val_loss: 0.4498 - val_accuracy: 0.9214\n",
      "Epoch 96/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0188 - accuracy: 0.9980 - val_loss: 0.4494 - val_accuracy: 0.9214\n",
      "Epoch 97/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0216 - accuracy: 0.9964 - val_loss: 0.4489 - val_accuracy: 0.9214\n",
      "Epoch 98/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0242 - accuracy: 0.9974 - val_loss: 0.4485 - val_accuracy: 0.9214\n",
      "Epoch 99/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0180 - accuracy: 0.9969 - val_loss: 0.4480 - val_accuracy: 0.9214\n",
      "Epoch 100/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0170 - accuracy: 0.9980 - val_loss: 0.4476 - val_accuracy: 0.9214\n",
      "Epoch 101/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0182 - accuracy: 0.9964 - val_loss: 0.4472 - val_accuracy: 0.9214\n",
      "Epoch 102/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0260 - accuracy: 0.9969 - val_loss: 0.4468 - val_accuracy: 0.9214\n",
      "Epoch 103/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0174 - accuracy: 0.9974 - val_loss: 0.4464 - val_accuracy: 0.9214\n",
      "Epoch 104/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0244 - accuracy: 0.9964 - val_loss: 0.4460 - val_accuracy: 0.9226\n",
      "Epoch 105/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.4454 - val_accuracy: 0.9226\n",
      "Epoch 106/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0179 - accuracy: 0.9974 - val_loss: 0.4450 - val_accuracy: 0.9238\n",
      "Epoch 107/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0167 - accuracy: 0.9964 - val_loss: 0.4446 - val_accuracy: 0.9238\n",
      "Epoch 108/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0146 - accuracy: 0.9990 - val_loss: 0.4442 - val_accuracy: 0.9238\n",
      "Epoch 109/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0178 - accuracy: 0.9969 - val_loss: 0.4438 - val_accuracy: 0.9238\n",
      "Epoch 110/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0236 - accuracy: 0.9949 - val_loss: 0.4434 - val_accuracy: 0.9238\n",
      "Epoch 111/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0186 - accuracy: 0.9985 - val_loss: 0.4429 - val_accuracy: 0.9238\n",
      "Epoch 112/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0193 - accuracy: 0.9959 - val_loss: 0.4424 - val_accuracy: 0.9238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0137 - accuracy: 0.9990 - val_loss: 0.4419 - val_accuracy: 0.9238\n",
      "Epoch 114/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0246 - accuracy: 0.9964 - val_loss: 0.4415 - val_accuracy: 0.9238\n",
      "Epoch 115/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0245 - accuracy: 0.9954 - val_loss: 0.4410 - val_accuracy: 0.9238\n",
      "Epoch 116/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.4405 - val_accuracy: 0.9238\n",
      "Epoch 117/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0184 - accuracy: 0.9964 - val_loss: 0.4402 - val_accuracy: 0.9238\n",
      "Epoch 118/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0181 - accuracy: 0.9964 - val_loss: 0.4397 - val_accuracy: 0.9238\n",
      "Epoch 119/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.4394 - val_accuracy: 0.9238\n",
      "Epoch 120/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0215 - accuracy: 0.9949 - val_loss: 0.4390 - val_accuracy: 0.9238\n",
      "Epoch 121/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9969 - val_loss: 0.4387 - val_accuracy: 0.9238\n",
      "Epoch 122/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0216 - accuracy: 0.9949 - val_loss: 0.4384 - val_accuracy: 0.9238\n",
      "Epoch 123/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0211 - accuracy: 0.9964 - val_loss: 0.4381 - val_accuracy: 0.9238\n",
      "Epoch 124/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0173 - accuracy: 0.9974 - val_loss: 0.4377 - val_accuracy: 0.9238\n",
      "Epoch 125/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.4373 - val_accuracy: 0.9238\n",
      "Epoch 126/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.4370 - val_accuracy: 0.9238\n",
      "Epoch 127/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.4367 - val_accuracy: 0.9238\n",
      "Epoch 128/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4364 - val_accuracy: 0.9238\n",
      "Epoch 129/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0227 - accuracy: 0.9964 - val_loss: 0.4361 - val_accuracy: 0.9238\n",
      "Epoch 130/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0193 - accuracy: 0.9974 - val_loss: 0.4358 - val_accuracy: 0.9238\n",
      "Epoch 131/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0191 - accuracy: 0.9974 - val_loss: 0.4355 - val_accuracy: 0.9238\n",
      "Epoch 132/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0181 - accuracy: 0.9985 - val_loss: 0.4352 - val_accuracy: 0.9238\n",
      "Epoch 133/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0162 - accuracy: 0.9974 - val_loss: 0.4348 - val_accuracy: 0.9238\n",
      "Epoch 134/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.4345 - val_accuracy: 0.9238\n",
      "Epoch 135/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0168 - accuracy: 0.9980 - val_loss: 0.4342 - val_accuracy: 0.9238\n",
      "Epoch 136/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0223 - accuracy: 0.9959 - val_loss: 0.4339 - val_accuracy: 0.9238\n",
      "Epoch 137/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0187 - accuracy: 0.9969 - val_loss: 0.4336 - val_accuracy: 0.9238\n",
      "Epoch 138/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0218 - accuracy: 0.9959 - val_loss: 0.4334 - val_accuracy: 0.9238\n",
      "Epoch 139/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0206 - accuracy: 0.9974 - val_loss: 0.4331 - val_accuracy: 0.9238\n",
      "Epoch 140/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9980 - val_loss: 0.4328 - val_accuracy: 0.9238\n",
      "Epoch 141/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0217 - accuracy: 0.9959 - val_loss: 0.4326 - val_accuracy: 0.9238\n",
      "Epoch 142/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4323 - val_accuracy: 0.9238\n",
      "Epoch 143/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0141 - accuracy: 0.9995 - val_loss: 0.4320 - val_accuracy: 0.9238\n",
      "Epoch 144/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0182 - accuracy: 0.9974 - val_loss: 0.4317 - val_accuracy: 0.9238\n",
      "Epoch 145/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.4314 - val_accuracy: 0.9238\n",
      "Epoch 146/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0171 - accuracy: 0.9969 - val_loss: 0.4311 - val_accuracy: 0.9238\n",
      "Epoch 147/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0152 - accuracy: 0.9985 - val_loss: 0.4309 - val_accuracy: 0.9238\n",
      "Epoch 148/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0238 - accuracy: 0.9964 - val_loss: 0.4307 - val_accuracy: 0.9238\n",
      "Epoch 149/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.4306 - val_accuracy: 0.9250\n",
      "Epoch 150/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0180 - accuracy: 0.9969 - val_loss: 0.4305 - val_accuracy: 0.9250\n",
      "Epoch 151/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0172 - accuracy: 0.9980 - val_loss: 0.4303 - val_accuracy: 0.9250\n",
      "Epoch 152/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0258 - accuracy: 0.9964 - val_loss: 0.4302 - val_accuracy: 0.9250\n",
      "Epoch 153/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.4302 - val_accuracy: 0.9250\n",
      "Epoch 154/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0151 - accuracy: 0.9974 - val_loss: 0.4302 - val_accuracy: 0.9250\n",
      "Epoch 155/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0191 - accuracy: 0.9974 - val_loss: 0.4301 - val_accuracy: 0.9250\n",
      "Epoch 156/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0162 - accuracy: 0.9980 - val_loss: 0.4300 - val_accuracy: 0.9250\n",
      "Epoch 157/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.4298 - val_accuracy: 0.9250\n",
      "Epoch 158/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0261 - accuracy: 0.9954 - val_loss: 0.4298 - val_accuracy: 0.9250\n",
      "Epoch 159/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0153 - accuracy: 0.9990 - val_loss: 0.4297 - val_accuracy: 0.9250\n",
      "Epoch 160/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0190 - accuracy: 0.9964 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
      "Epoch 161/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
      "Epoch 162/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0309 - accuracy: 0.9934 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
      "Epoch 163/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
      "Epoch 164/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.4295 - val_accuracy: 0.9250\n",
      "Epoch 165/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0197 - accuracy: 0.9959 - val_loss: 0.4296 - val_accuracy: 0.9250\n",
      "Epoch 166/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0223 - accuracy: 0.9959 - val_loss: 0.4295 - val_accuracy: 0.9250\n",
      "Epoch 167/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0181 - accuracy: 0.9985 - val_loss: 0.4295 - val_accuracy: 0.9262\n",
      "Epoch 168/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.4295 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0164 - accuracy: 0.9995 - val_loss: 0.4294 - val_accuracy: 0.9262\n",
      "Epoch 170/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0155 - accuracy: 0.9980 - val_loss: 0.4294 - val_accuracy: 0.9262\n",
      "Epoch 171/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0161 - accuracy: 0.9974 - val_loss: 0.4293 - val_accuracy: 0.9250\n",
      "Epoch 172/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0179 - accuracy: 0.9985 - val_loss: 0.4293 - val_accuracy: 0.9262\n",
      "Epoch 173/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0206 - accuracy: 0.9969 - val_loss: 0.4293 - val_accuracy: 0.9262\n",
      "Epoch 174/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0209 - accuracy: 0.9980 - val_loss: 0.4292 - val_accuracy: 0.9274\n",
      "Epoch 175/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0240 - accuracy: 0.9954 - val_loss: 0.4291 - val_accuracy: 0.9274\n",
      "Epoch 176/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.4291 - val_accuracy: 0.9274\n",
      "Epoch 177/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0176 - accuracy: 0.9974 - val_loss: 0.4289 - val_accuracy: 0.9274\n",
      "Epoch 178/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0220 - accuracy: 0.9944 - val_loss: 0.4288 - val_accuracy: 0.9274\n",
      "Epoch 179/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.4287 - val_accuracy: 0.9274\n",
      "Epoch 180/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.4285 - val_accuracy: 0.9274\n",
      "Epoch 181/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.4283 - val_accuracy: 0.9274\n",
      "Epoch 182/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0219 - accuracy: 0.9969 - val_loss: 0.4281 - val_accuracy: 0.9274\n",
      "Epoch 183/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4280 - val_accuracy: 0.9286\n",
      "Epoch 184/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0159 - accuracy: 0.9980 - val_loss: 0.4279 - val_accuracy: 0.9286\n",
      "Epoch 185/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0183 - accuracy: 0.9974 - val_loss: 0.4277 - val_accuracy: 0.9274\n",
      "Epoch 186/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0169 - accuracy: 0.9980 - val_loss: 0.4276 - val_accuracy: 0.9274\n",
      "Epoch 187/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0157 - accuracy: 0.9974 - val_loss: 0.4275 - val_accuracy: 0.9274\n",
      "Epoch 188/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0226 - accuracy: 0.9949 - val_loss: 0.4274 - val_accuracy: 0.9274\n",
      "Epoch 189/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0206 - accuracy: 0.9949 - val_loss: 0.4273 - val_accuracy: 0.9274\n",
      "Epoch 190/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.4272 - val_accuracy: 0.9274\n",
      "Epoch 191/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0188 - accuracy: 0.9969 - val_loss: 0.4271 - val_accuracy: 0.9274\n",
      "Epoch 192/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.4270 - val_accuracy: 0.9274\n",
      "Epoch 193/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0165 - accuracy: 0.9980 - val_loss: 0.4269 - val_accuracy: 0.9274\n",
      "Epoch 194/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.4268 - val_accuracy: 0.9274\n",
      "Epoch 195/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0213 - accuracy: 0.9959 - val_loss: 0.4267 - val_accuracy: 0.9274\n",
      "Epoch 196/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0186 - accuracy: 0.9969 - val_loss: 0.4266 - val_accuracy: 0.9274\n",
      "Epoch 197/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0194 - accuracy: 0.9974 - val_loss: 0.4265 - val_accuracy: 0.9274\n",
      "Epoch 198/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.4264 - val_accuracy: 0.9274\n",
      "Epoch 199/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0165 - accuracy: 0.9985 - val_loss: 0.4263 - val_accuracy: 0.9274\n",
      "Epoch 200/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0153 - accuracy: 0.9985 - val_loss: 0.4262 - val_accuracy: 0.9274\n",
      "Epoch 201/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0158 - accuracy: 0.9980 - val_loss: 0.4262 - val_accuracy: 0.9274\n",
      "Epoch 202/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0185 - accuracy: 0.9969 - val_loss: 0.4261 - val_accuracy: 0.9274\n",
      "Epoch 203/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0167 - accuracy: 0.9980 - val_loss: 0.4260 - val_accuracy: 0.9274\n",
      "Epoch 204/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0217 - accuracy: 0.9959 - val_loss: 0.4258 - val_accuracy: 0.9274\n",
      "Epoch 205/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0199 - accuracy: 0.9964 - val_loss: 0.4256 - val_accuracy: 0.9274\n",
      "Epoch 206/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9985 - val_loss: 0.4254 - val_accuracy: 0.9274\n",
      "Epoch 207/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.4252 - val_accuracy: 0.9274\n",
      "Epoch 208/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4250 - val_accuracy: 0.9274\n",
      "Epoch 209/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.4248 - val_accuracy: 0.9274\n",
      "Epoch 210/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.4245 - val_accuracy: 0.9274\n",
      "Epoch 211/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0191 - accuracy: 0.9990 - val_loss: 0.4243 - val_accuracy: 0.9274\n",
      "Epoch 212/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9949 - val_loss: 0.4241 - val_accuracy: 0.9274\n",
      "Epoch 213/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.4239 - val_accuracy: 0.9274\n",
      "Epoch 214/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.4238 - val_accuracy: 0.9274\n",
      "Epoch 215/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0158 - accuracy: 0.9980 - val_loss: 0.4236 - val_accuracy: 0.9274\n",
      "Epoch 216/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.4235 - val_accuracy: 0.9274\n",
      "Epoch 217/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0184 - accuracy: 0.9974 - val_loss: 0.4233 - val_accuracy: 0.9274\n",
      "Epoch 218/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0221 - accuracy: 0.9959 - val_loss: 0.4231 - val_accuracy: 0.9274\n",
      "Epoch 219/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0167 - accuracy: 0.9974 - val_loss: 0.4229 - val_accuracy: 0.9274\n",
      "Epoch 220/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0243 - accuracy: 0.9949 - val_loss: 0.4227 - val_accuracy: 0.9274\n",
      "Epoch 221/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0159 - accuracy: 0.9974 - val_loss: 0.4225 - val_accuracy: 0.9274\n",
      "Epoch 222/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.4224 - val_accuracy: 0.9274\n",
      "Epoch 223/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.4222 - val_accuracy: 0.9274\n",
      "Epoch 224/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0192 - accuracy: 0.9959 - val_loss: 0.4220 - val_accuracy: 0.9274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.4218 - val_accuracy: 0.9274\n",
      "Epoch 226/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9990 - val_loss: 0.4216 - val_accuracy: 0.9274\n",
      "Epoch 227/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0196 - accuracy: 0.9964 - val_loss: 0.4215 - val_accuracy: 0.9274\n",
      "Epoch 228/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.4213 - val_accuracy: 0.9274\n",
      "Epoch 229/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.4211 - val_accuracy: 0.9274\n",
      "Epoch 230/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0205 - accuracy: 0.9969 - val_loss: 0.4209 - val_accuracy: 0.9274\n",
      "Epoch 231/700\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0183 - accuracy: 0.9969 - val_loss: 0.4208 - val_accuracy: 0.9262\n",
      "Epoch 232/700\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 233/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.4204 - val_accuracy: 0.9262\n",
      "Epoch 234/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.4203 - val_accuracy: 0.9262\n",
      "Epoch 235/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0170 - accuracy: 0.9985 - val_loss: 0.4201 - val_accuracy: 0.9262\n",
      "Epoch 236/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9990 - val_loss: 0.4200 - val_accuracy: 0.9262\n",
      "Epoch 237/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0113 - accuracy: 0.9995 - val_loss: 0.4199 - val_accuracy: 0.9262\n",
      "Epoch 238/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.4198 - val_accuracy: 0.9262\n",
      "Epoch 239/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0130 - accuracy: 0.9995 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 240/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0184 - accuracy: 0.9974 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 241/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.4196 - val_accuracy: 0.9262\n",
      "Epoch 242/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0166 - accuracy: 0.9974 - val_loss: 0.4196 - val_accuracy: 0.9262\n",
      "Epoch 243/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 0.4196 - val_accuracy: 0.9262\n",
      "Epoch 244/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0173 - accuracy: 0.9964 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 245/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0198 - accuracy: 0.9959 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 246/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0187 - accuracy: 0.9964 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 247/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0214 - accuracy: 0.9944 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 248/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0206 - accuracy: 0.9959 - val_loss: 0.4198 - val_accuracy: 0.9262\n",
      "Epoch 249/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0199 - accuracy: 0.9949 - val_loss: 0.4198 - val_accuracy: 0.9262\n",
      "Epoch 250/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0126 - accuracy: 0.9990 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 251/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0246 - accuracy: 0.9954 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 252/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0206 - accuracy: 0.9964 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 253/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0130 - accuracy: 0.9995 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 254/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0166 - accuracy: 0.9985 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 255/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 256/700\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0185 - accuracy: 0.9985 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 257/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 258/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0160 - accuracy: 0.9980 - val_loss: 0.4197 - val_accuracy: 0.9250\n",
      "Epoch 259/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0245 - accuracy: 0.9974 - val_loss: 0.4196 - val_accuracy: 0.9250\n",
      "Epoch 260/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.4196 - val_accuracy: 0.9250\n",
      "Epoch 261/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0173 - accuracy: 0.9964 - val_loss: 0.4195 - val_accuracy: 0.9250\n",
      "Epoch 262/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0150 - accuracy: 0.9990 - val_loss: 0.4194 - val_accuracy: 0.9250\n",
      "Epoch 263/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.4193 - val_accuracy: 0.9250\n",
      "Epoch 264/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.4193 - val_accuracy: 0.9250\n",
      "Epoch 265/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9974 - val_loss: 0.4192 - val_accuracy: 0.9250\n",
      "Epoch 266/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0267 - accuracy: 0.9929 - val_loss: 0.4192 - val_accuracy: 0.9250\n",
      "Epoch 267/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0173 - accuracy: 0.9980 - val_loss: 0.4191 - val_accuracy: 0.9250\n",
      "Epoch 268/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.4191 - val_accuracy: 0.9250\n",
      "Epoch 269/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.4191 - val_accuracy: 0.9250\n",
      "Epoch 270/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.4190 - val_accuracy: 0.9262\n",
      "Epoch 271/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0198 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9262\n",
      "Epoch 272/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.4189 - val_accuracy: 0.9262\n",
      "Epoch 273/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0137 - accuracy: 0.9985 - val_loss: 0.4189 - val_accuracy: 0.9262\n",
      "Epoch 274/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 275/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 276/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0180 - accuracy: 0.9974 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 277/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 278/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0234 - accuracy: 0.9949 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 279/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0176 - accuracy: 0.9964 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 280/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0176 - accuracy: 0.9985 - val_loss: 0.4188 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0143 - accuracy: 0.9990 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 282/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0154 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9262\n",
      "Epoch 283/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0218 - accuracy: 0.9959 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 284/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0171 - accuracy: 0.9974 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 285/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 286/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 287/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0190 - accuracy: 0.9974 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 288/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0197 - accuracy: 0.9964 - val_loss: 0.4187 - val_accuracy: 0.9262\n",
      "Epoch 289/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0164 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 290/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 291/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.4185 - val_accuracy: 0.9262\n",
      "Epoch 292/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0216 - accuracy: 0.9964 - val_loss: 0.4184 - val_accuracy: 0.9262\n",
      "Epoch 293/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0255 - accuracy: 0.9954 - val_loss: 0.4183 - val_accuracy: 0.9262\n",
      "Epoch 294/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0164 - accuracy: 0.9974 - val_loss: 0.4182 - val_accuracy: 0.9262\n",
      "Epoch 295/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.4181 - val_accuracy: 0.9262\n",
      "Epoch 296/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0190 - accuracy: 0.9969 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 297/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0140 - accuracy: 0.9969 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 298/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.4179 - val_accuracy: 0.9262\n",
      "Epoch 299/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0165 - accuracy: 0.9980 - val_loss: 0.4179 - val_accuracy: 0.9262\n",
      "Epoch 300/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0169 - accuracy: 0.9985 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 301/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0250 - accuracy: 0.9929 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 302/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0175 - accuracy: 0.9985 - val_loss: 0.4181 - val_accuracy: 0.9250\n",
      "Epoch 303/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.4181 - val_accuracy: 0.9250\n",
      "Epoch 304/700\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.4182 - val_accuracy: 0.9250\n",
      "Epoch 305/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 0.4182 - val_accuracy: 0.9250\n",
      "Epoch 306/700\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0134 - accuracy: 0.9990 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 307/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0128 - accuracy: 0.9990 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 308/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0149 - accuracy: 0.9985 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 309/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9980 - val_loss: 0.4184 - val_accuracy: 0.9250\n",
      "Epoch 310/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 311/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0237 - accuracy: 0.9959 - val_loss: 0.4184 - val_accuracy: 0.9250\n",
      "Epoch 312/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0193 - accuracy: 0.9974 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 313/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0161 - accuracy: 0.9980 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 314/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 315/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.4183 - val_accuracy: 0.9262\n",
      "Epoch 316/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.4184 - val_accuracy: 0.9262\n",
      "Epoch 317/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0185 - accuracy: 0.9964 - val_loss: 0.4185 - val_accuracy: 0.9262\n",
      "Epoch 318/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.4185 - val_accuracy: 0.9262\n",
      "Epoch 319/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0218 - accuracy: 0.9949 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 320/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 321/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.4187 - val_accuracy: 0.9262\n",
      "Epoch 322/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0197 - accuracy: 0.9959 - val_loss: 0.4187 - val_accuracy: 0.9262\n",
      "Epoch 323/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 324/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0138 - accuracy: 0.9985 - val_loss: 0.4186 - val_accuracy: 0.9262\n",
      "Epoch 325/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0182 - accuracy: 0.9969 - val_loss: 0.4185 - val_accuracy: 0.9262\n",
      "Epoch 326/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0136 - accuracy: 0.9990 - val_loss: 0.4185 - val_accuracy: 0.9262\n",
      "Epoch 327/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0139 - accuracy: 0.9995 - val_loss: 0.4184 - val_accuracy: 0.9262\n",
      "Epoch 328/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.4183 - val_accuracy: 0.9262\n",
      "Epoch 329/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0174 - accuracy: 0.9964 - val_loss: 0.4183 - val_accuracy: 0.9262\n",
      "Epoch 330/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0173 - accuracy: 0.9974 - val_loss: 0.4181 - val_accuracy: 0.9262\n",
      "Epoch 331/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0165 - accuracy: 0.9980 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 332/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0163 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 333/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 334/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0206 - accuracy: 0.9964 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 335/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0227 - accuracy: 0.9964 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 336/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/700\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.4180 - val_accuracy: 0.9262\n",
      "Epoch 338/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0186 - accuracy: 0.9974 - val_loss: 0.4179 - val_accuracy: 0.9262\n",
      "Epoch 339/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0209 - accuracy: 0.9964 - val_loss: 0.4178 - val_accuracy: 0.9262\n",
      "Epoch 340/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.4177 - val_accuracy: 0.9262\n",
      "Epoch 341/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.4177 - val_accuracy: 0.9262\n",
      "Epoch 342/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0149 - accuracy: 0.9995 - val_loss: 0.4177 - val_accuracy: 0.9262\n",
      "Epoch 343/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0203 - accuracy: 0.9959 - val_loss: 0.4176 - val_accuracy: 0.9262\n",
      "Epoch 344/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0178 - accuracy: 0.9974 - val_loss: 0.4175 - val_accuracy: 0.9262\n",
      "Epoch 345/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0180 - accuracy: 0.9974 - val_loss: 0.4175 - val_accuracy: 0.9250\n",
      "Epoch 346/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0239 - accuracy: 0.9954 - val_loss: 0.4176 - val_accuracy: 0.9250\n",
      "Epoch 347/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0154 - accuracy: 0.9985 - val_loss: 0.4176 - val_accuracy: 0.9250\n",
      "Epoch 348/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0182 - accuracy: 0.9964 - val_loss: 0.4177 - val_accuracy: 0.9250\n",
      "Epoch 349/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0206 - accuracy: 0.9969 - val_loss: 0.4177 - val_accuracy: 0.9250\n",
      "Epoch 350/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.4177 - val_accuracy: 0.9250\n",
      "Epoch 351/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.4177 - val_accuracy: 0.9250\n",
      "Epoch 352/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.4178 - val_accuracy: 0.9250\n",
      "Epoch 353/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.4178 - val_accuracy: 0.9250\n",
      "Epoch 354/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0240 - accuracy: 0.9944 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 355/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0189 - accuracy: 0.9959 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 356/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0145 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 357/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0226 - accuracy: 0.9954 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 358/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 359/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0153 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 360/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 361/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.4180 - val_accuracy: 0.9238\n",
      "Epoch 362/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 363/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0203 - accuracy: 0.9964 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 364/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0168 - accuracy: 0.9974 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 365/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0148 - accuracy: 0.9990 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 366/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 367/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 368/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 369/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0201 - accuracy: 0.9964 - val_loss: 0.4181 - val_accuracy: 0.9250\n",
      "Epoch 370/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.4182 - val_accuracy: 0.9250\n",
      "Epoch 371/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0192 - accuracy: 0.9964 - val_loss: 0.4182 - val_accuracy: 0.9250\n",
      "Epoch 372/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0191 - accuracy: 0.9969 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 373/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 374/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 375/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 376/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0179 - accuracy: 0.9974 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 377/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0119 - accuracy: 0.9995 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 378/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 379/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0173 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 380/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0216 - accuracy: 0.9959 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 381/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 382/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0176 - accuracy: 0.9974 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 383/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 384/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0150 - accuracy: 0.9985 - val_loss: 0.4186 - val_accuracy: 0.9238\n",
      "Epoch 385/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.4186 - val_accuracy: 0.9238\n",
      "Epoch 386/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0196 - accuracy: 0.9959 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 387/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0223 - accuracy: 0.9949 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 388/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0158 - accuracy: 0.9980 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 389/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0163 - accuracy: 0.9990 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 390/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0174 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 391/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9974 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 392/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0177 - accuracy: 0.9974 - val_loss: 0.4182 - val_accuracy: 0.9238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0185 - accuracy: 0.9980 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 394/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 395/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 396/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0190 - accuracy: 0.9969 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 397/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 398/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0220 - accuracy: 0.9969 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 399/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 400/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0197 - accuracy: 0.9964 - val_loss: 0.4181 - val_accuracy: 0.9238\n",
      "Epoch 401/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.4180 - val_accuracy: 0.9238\n",
      "Epoch 402/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0205 - accuracy: 0.9964 - val_loss: 0.4179 - val_accuracy: 0.9238\n",
      "Epoch 403/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0182 - accuracy: 0.9969 - val_loss: 0.4177 - val_accuracy: 0.9238\n",
      "Epoch 404/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.4176 - val_accuracy: 0.9238\n",
      "Epoch 405/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0232 - accuracy: 0.9959 - val_loss: 0.4175 - val_accuracy: 0.9238\n",
      "Epoch 406/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.4174 - val_accuracy: 0.9238\n",
      "Epoch 407/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0194 - accuracy: 0.9964 - val_loss: 0.4173 - val_accuracy: 0.9238\n",
      "Epoch 408/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.4172 - val_accuracy: 0.9238\n",
      "Epoch 409/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0194 - accuracy: 0.9964 - val_loss: 0.4172 - val_accuracy: 0.9238\n",
      "Epoch 410/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0155 - accuracy: 0.9969 - val_loss: 0.4171 - val_accuracy: 0.9238\n",
      "Epoch 411/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0157 - accuracy: 0.9980 - val_loss: 0.4170 - val_accuracy: 0.9238\n",
      "Epoch 412/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0206 - accuracy: 0.9949 - val_loss: 0.4171 - val_accuracy: 0.9238\n",
      "Epoch 413/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0165 - accuracy: 0.9974 - val_loss: 0.4170 - val_accuracy: 0.9238\n",
      "Epoch 414/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0205 - accuracy: 0.9949 - val_loss: 0.4170 - val_accuracy: 0.9238\n",
      "Epoch 415/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.4169 - val_accuracy: 0.9238\n",
      "Epoch 416/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4168 - val_accuracy: 0.9238\n",
      "Epoch 417/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.4168 - val_accuracy: 0.9238\n",
      "Epoch 418/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0180 - accuracy: 0.9969 - val_loss: 0.4168 - val_accuracy: 0.9238\n",
      "Epoch 419/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.4168 - val_accuracy: 0.9238\n",
      "Epoch 420/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0182 - accuracy: 0.9969 - val_loss: 0.4167 - val_accuracy: 0.9250\n",
      "Epoch 421/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0154 - accuracy: 0.9985 - val_loss: 0.4167 - val_accuracy: 0.9250\n",
      "Epoch 422/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0121 - accuracy: 0.9990 - val_loss: 0.4166 - val_accuracy: 0.9250\n",
      "Epoch 423/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 0.4165 - val_accuracy: 0.9250\n",
      "Epoch 424/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0147 - accuracy: 0.9990 - val_loss: 0.4164 - val_accuracy: 0.9250\n",
      "Epoch 425/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.4164 - val_accuracy: 0.9250\n",
      "Epoch 426/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0128 - accuracy: 0.9995 - val_loss: 0.4164 - val_accuracy: 0.9250\n",
      "Epoch 427/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0168 - accuracy: 0.9974 - val_loss: 0.4163 - val_accuracy: 0.9250\n",
      "Epoch 428/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0130 - accuracy: 0.9995 - val_loss: 0.4163 - val_accuracy: 0.9250\n",
      "Epoch 429/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.4163 - val_accuracy: 0.9250\n",
      "Epoch 430/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4163 - val_accuracy: 0.9250\n",
      "Epoch 431/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0186 - accuracy: 0.9974 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 432/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0155 - accuracy: 0.9980 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 433/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 434/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 435/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 436/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0143 - accuracy: 0.9980 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 437/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9969 - val_loss: 0.4161 - val_accuracy: 0.9250\n",
      "Epoch 438/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.4160 - val_accuracy: 0.9250\n",
      "Epoch 439/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.4160 - val_accuracy: 0.9250\n",
      "Epoch 440/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 0.4159 - val_accuracy: 0.9250\n",
      "Epoch 441/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0167 - accuracy: 0.9974 - val_loss: 0.4159 - val_accuracy: 0.9250\n",
      "Epoch 442/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 443/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0160 - accuracy: 0.9974 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 444/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0174 - accuracy: 0.9980 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 445/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0117 - accuracy: 0.9990 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 446/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.4156 - val_accuracy: 0.9250\n",
      "Epoch 447/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0170 - accuracy: 0.9980 - val_loss: 0.4156 - val_accuracy: 0.9250\n",
      "Epoch 448/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0191 - accuracy: 0.9969 - val_loss: 0.4156 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0159 - accuracy: 0.9974 - val_loss: 0.4156 - val_accuracy: 0.9250\n",
      "Epoch 450/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.4156 - val_accuracy: 0.9250\n",
      "Epoch 451/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0198 - accuracy: 0.9974 - val_loss: 0.4156 - val_accuracy: 0.9250\n",
      "Epoch 452/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0187 - accuracy: 0.9959 - val_loss: 0.4156 - val_accuracy: 0.9238\n",
      "Epoch 453/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0165 - accuracy: 0.9985 - val_loss: 0.4157 - val_accuracy: 0.9238\n",
      "Epoch 454/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 455/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 456/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 457/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0157 - accuracy: 0.9974 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 458/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0205 - accuracy: 0.9959 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 459/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0173 - accuracy: 0.9980 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 460/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0156 - accuracy: 0.9980 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 461/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 462/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0180 - accuracy: 0.9974 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 463/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0146 - accuracy: 0.9974 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 464/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0157 - accuracy: 0.9980 - val_loss: 0.4157 - val_accuracy: 0.9250\n",
      "Epoch 465/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 466/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0131 - accuracy: 0.9980 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 467/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 468/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0178 - accuracy: 0.9974 - val_loss: 0.4158 - val_accuracy: 0.9250\n",
      "Epoch 469/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0167 - accuracy: 0.9964 - val_loss: 0.4159 - val_accuracy: 0.9250\n",
      "Epoch 470/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0175 - accuracy: 0.9980 - val_loss: 0.4159 - val_accuracy: 0.9250\n",
      "Epoch 471/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0169 - accuracy: 0.9980 - val_loss: 0.4160 - val_accuracy: 0.9250\n",
      "Epoch 472/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.4161 - val_accuracy: 0.9250\n",
      "Epoch 473/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0187 - accuracy: 0.9969 - val_loss: 0.4162 - val_accuracy: 0.9250\n",
      "Epoch 474/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0184 - accuracy: 0.9974 - val_loss: 0.4163 - val_accuracy: 0.9250\n",
      "Epoch 475/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0182 - accuracy: 0.9974 - val_loss: 0.4164 - val_accuracy: 0.9250\n",
      "Epoch 476/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0154 - accuracy: 0.9980 - val_loss: 0.4166 - val_accuracy: 0.9250\n",
      "Epoch 477/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4167 - val_accuracy: 0.9250\n",
      "Epoch 478/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 0.4169 - val_accuracy: 0.9250\n",
      "Epoch 479/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0164 - accuracy: 0.9974 - val_loss: 0.4170 - val_accuracy: 0.9250\n",
      "Epoch 480/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0153 - accuracy: 0.9990 - val_loss: 0.4171 - val_accuracy: 0.9250\n",
      "Epoch 481/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 0.4173 - val_accuracy: 0.9250\n",
      "Epoch 482/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9980 - val_loss: 0.4174 - val_accuracy: 0.9250\n",
      "Epoch 483/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0208 - accuracy: 0.9974 - val_loss: 0.4175 - val_accuracy: 0.9250\n",
      "Epoch 484/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.4176 - val_accuracy: 0.9250\n",
      "Epoch 485/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.4177 - val_accuracy: 0.9250\n",
      "Epoch 486/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0163 - accuracy: 0.9964 - val_loss: 0.4179 - val_accuracy: 0.9250\n",
      "Epoch 487/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0155 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9250\n",
      "Epoch 488/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.4182 - val_accuracy: 0.9250\n",
      "Epoch 489/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.4183 - val_accuracy: 0.9250\n",
      "Epoch 490/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 491/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0220 - accuracy: 0.9959 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 492/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 0.4187 - val_accuracy: 0.9238\n",
      "Epoch 493/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0174 - accuracy: 0.9964 - val_loss: 0.4188 - val_accuracy: 0.9238\n",
      "Epoch 494/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0158 - accuracy: 0.9974 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 495/700\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 496/700\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 497/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0193 - accuracy: 0.9974 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 498/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0186 - accuracy: 0.9959 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 499/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0161 - accuracy: 0.9974 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 500/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 501/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0180 - accuracy: 0.9964 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 502/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 503/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0115 - accuracy: 0.9995 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 504/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0195 - accuracy: 0.9969 - val_loss: 0.4189 - val_accuracy: 0.9238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 506/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0221 - accuracy: 0.9954 - val_loss: 0.4189 - val_accuracy: 0.9238\n",
      "Epoch 507/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0213 - accuracy: 0.9964 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 508/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0165 - accuracy: 0.9990 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 509/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 0.4190 - val_accuracy: 0.9238\n",
      "Epoch 510/700\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0171 - accuracy: 0.9980 - val_loss: 0.4190 - val_accuracy: 0.9250\n",
      "Epoch 511/700\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0207 - accuracy: 0.9949 - val_loss: 0.4190 - val_accuracy: 0.9250\n",
      "Epoch 512/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9250\n",
      "Epoch 513/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 0.4189 - val_accuracy: 0.9250\n",
      "Epoch 514/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.4188 - val_accuracy: 0.9250\n",
      "Epoch 515/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.4187 - val_accuracy: 0.9250\n",
      "Epoch 516/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.4187 - val_accuracy: 0.9250\n",
      "Epoch 517/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 518/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0150 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 519/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0159 - accuracy: 0.9964 - val_loss: 0.4185 - val_accuracy: 0.9250\n",
      "Epoch 520/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0184 - accuracy: 0.9964 - val_loss: 0.4185 - val_accuracy: 0.9250\n",
      "Epoch 521/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0160 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 522/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 523/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0097 - accuracy: 0.9995 - val_loss: 0.4187 - val_accuracy: 0.9250\n",
      "Epoch 524/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0146 - accuracy: 0.9985 - val_loss: 0.4188 - val_accuracy: 0.9250\n",
      "Epoch 525/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0143 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9250\n",
      "Epoch 526/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0150 - accuracy: 0.9985 - val_loss: 0.4191 - val_accuracy: 0.9250\n",
      "Epoch 527/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.4192 - val_accuracy: 0.9262\n",
      "Epoch 528/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.4192 - val_accuracy: 0.9262\n",
      "Epoch 529/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0186 - accuracy: 0.9969 - val_loss: 0.4193 - val_accuracy: 0.9262\n",
      "Epoch 530/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0139 - accuracy: 0.9990 - val_loss: 0.4194 - val_accuracy: 0.9262\n",
      "Epoch 531/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.4195 - val_accuracy: 0.9262\n",
      "Epoch 532/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0210 - accuracy: 0.9959 - val_loss: 0.4196 - val_accuracy: 0.9262\n",
      "Epoch 533/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0156 - accuracy: 0.9980 - val_loss: 0.4197 - val_accuracy: 0.9262\n",
      "Epoch 534/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0148 - accuracy: 0.9990 - val_loss: 0.4198 - val_accuracy: 0.9262\n",
      "Epoch 535/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0172 - accuracy: 0.9980 - val_loss: 0.4199 - val_accuracy: 0.9262\n",
      "Epoch 536/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0127 - accuracy: 0.9974 - val_loss: 0.4200 - val_accuracy: 0.9262\n",
      "Epoch 537/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.4200 - val_accuracy: 0.9262\n",
      "Epoch 538/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.4201 - val_accuracy: 0.9262\n",
      "Epoch 539/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0237 - accuracy: 0.9959 - val_loss: 0.4201 - val_accuracy: 0.9262\n",
      "Epoch 540/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0143 - accuracy: 0.9985 - val_loss: 0.4202 - val_accuracy: 0.9262\n",
      "Epoch 541/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.4202 - val_accuracy: 0.9262\n",
      "Epoch 542/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 0.4203 - val_accuracy: 0.9262\n",
      "Epoch 543/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0166 - accuracy: 0.9964 - val_loss: 0.4204 - val_accuracy: 0.9262\n",
      "Epoch 544/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0150 - accuracy: 0.9990 - val_loss: 0.4204 - val_accuracy: 0.9262\n",
      "Epoch 545/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0142 - accuracy: 0.9995 - val_loss: 0.4204 - val_accuracy: 0.9262\n",
      "Epoch 546/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.4204 - val_accuracy: 0.9262\n",
      "Epoch 547/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0137 - accuracy: 0.9985 - val_loss: 0.4205 - val_accuracy: 0.9262\n",
      "Epoch 548/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0178 - accuracy: 0.9974 - val_loss: 0.4206 - val_accuracy: 0.9250\n",
      "Epoch 549/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.4206 - val_accuracy: 0.9250\n",
      "Epoch 550/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 551/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0171 - accuracy: 0.9985 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 552/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 553/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0148 - accuracy: 0.9980 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 554/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 555/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0141 - accuracy: 0.9980 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 556/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0201 - accuracy: 0.9974 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 557/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0126 - accuracy: 0.9990 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 558/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0122 - accuracy: 0.9995 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 559/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.4205 - val_accuracy: 0.9262\n",
      "Epoch 560/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0139 - accuracy: 0.9985 - val_loss: 0.4206 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 562/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 563/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 564/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0151 - accuracy: 0.9980 - val_loss: 0.4208 - val_accuracy: 0.9262\n",
      "Epoch 565/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0165 - accuracy: 0.9980 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 566/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9974 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 567/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 568/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0183 - accuracy: 0.9969 - val_loss: 0.4211 - val_accuracy: 0.9262\n",
      "Epoch 569/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.4211 - val_accuracy: 0.9262\n",
      "Epoch 570/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0135 - accuracy: 0.9990 - val_loss: 0.4211 - val_accuracy: 0.9262\n",
      "Epoch 571/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0132 - accuracy: 0.9990 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 572/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0162 - accuracy: 0.9964 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 573/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0114 - accuracy: 0.9995 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 574/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0195 - accuracy: 0.9974 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 575/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 576/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0140 - accuracy: 0.9985 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 577/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0167 - accuracy: 0.9974 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 578/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0183 - accuracy: 0.9969 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 579/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0117 - accuracy: 0.9995 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 580/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 581/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 582/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0127 - accuracy: 0.9990 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 583/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0185 - accuracy: 0.9964 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 584/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0185 - accuracy: 0.9964 - val_loss: 0.4211 - val_accuracy: 0.9262\n",
      "Epoch 585/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0147 - accuracy: 0.9974 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 586/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0145 - accuracy: 0.9974 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 587/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 588/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 589/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 590/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 591/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0179 - accuracy: 0.9985 - val_loss: 0.4208 - val_accuracy: 0.9262\n",
      "Epoch 592/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0149 - accuracy: 0.9974 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 593/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0202 - accuracy: 0.9964 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 594/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 595/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0121 - accuracy: 0.9990 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 596/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0159 - accuracy: 0.9985 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 597/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 598/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 599/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.4206 - val_accuracy: 0.9262\n",
      "Epoch 600/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.4207 - val_accuracy: 0.9262\n",
      "Epoch 601/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.4208 - val_accuracy: 0.9262\n",
      "Epoch 602/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.4209 - val_accuracy: 0.9262\n",
      "Epoch 603/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.4210 - val_accuracy: 0.9262\n",
      "Epoch 604/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0184 - accuracy: 0.9959 - val_loss: 0.4211 - val_accuracy: 0.9262\n",
      "Epoch 605/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0161 - accuracy: 0.9985 - val_loss: 0.4212 - val_accuracy: 0.9262\n",
      "Epoch 606/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.4212 - val_accuracy: 0.9262\n",
      "Epoch 607/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0137 - accuracy: 0.9980 - val_loss: 0.4213 - val_accuracy: 0.9250\n",
      "Epoch 608/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0154 - accuracy: 0.9980 - val_loss: 0.4214 - val_accuracy: 0.9250\n",
      "Epoch 609/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0193 - accuracy: 0.9964 - val_loss: 0.4214 - val_accuracy: 0.9250\n",
      "Epoch 610/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0184 - accuracy: 0.9959 - val_loss: 0.4215 - val_accuracy: 0.9250\n",
      "Epoch 611/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0157 - accuracy: 0.9990 - val_loss: 0.4216 - val_accuracy: 0.9250\n",
      "Epoch 612/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0175 - accuracy: 0.9980 - val_loss: 0.4216 - val_accuracy: 0.9250\n",
      "Epoch 613/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0119 - accuracy: 0.9980 - val_loss: 0.4217 - val_accuracy: 0.9250\n",
      "Epoch 614/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0187 - accuracy: 0.9959 - val_loss: 0.4216 - val_accuracy: 0.9250\n",
      "Epoch 615/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.4215 - val_accuracy: 0.9250\n",
      "Epoch 616/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0129 - accuracy: 0.9990 - val_loss: 0.4215 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0151 - accuracy: 0.9980 - val_loss: 0.4214 - val_accuracy: 0.9250\n",
      "Epoch 618/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0132 - accuracy: 0.9990 - val_loss: 0.4214 - val_accuracy: 0.9250\n",
      "Epoch 619/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.4213 - val_accuracy: 0.9250\n",
      "Epoch 620/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.4213 - val_accuracy: 0.9250\n",
      "Epoch 621/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.4212 - val_accuracy: 0.9250\n",
      "Epoch 622/700\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0099 - accuracy: 0.9990 - val_loss: 0.4211 - val_accuracy: 0.9250\n",
      "Epoch 623/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0158 - accuracy: 0.9964 - val_loss: 0.4211 - val_accuracy: 0.9250\n",
      "Epoch 624/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0138 - accuracy: 0.9980 - val_loss: 0.4210 - val_accuracy: 0.9250\n",
      "Epoch 625/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9980 - val_loss: 0.4209 - val_accuracy: 0.9250\n",
      "Epoch 626/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.4209 - val_accuracy: 0.9250\n",
      "Epoch 627/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0139 - accuracy: 0.9985 - val_loss: 0.4208 - val_accuracy: 0.9250\n",
      "Epoch 628/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0178 - accuracy: 0.9964 - val_loss: 0.4207 - val_accuracy: 0.9250\n",
      "Epoch 629/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0155 - accuracy: 0.9969 - val_loss: 0.4206 - val_accuracy: 0.9250\n",
      "Epoch 630/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0174 - accuracy: 0.9980 - val_loss: 0.4205 - val_accuracy: 0.9250\n",
      "Epoch 631/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0179 - accuracy: 0.9974 - val_loss: 0.4203 - val_accuracy: 0.9250\n",
      "Epoch 632/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.4202 - val_accuracy: 0.9250\n",
      "Epoch 633/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0169 - accuracy: 0.9974 - val_loss: 0.4201 - val_accuracy: 0.9250\n",
      "Epoch 634/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0166 - accuracy: 0.9969 - val_loss: 0.4200 - val_accuracy: 0.9250\n",
      "Epoch 635/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 636/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.4197 - val_accuracy: 0.9250\n",
      "Epoch 637/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9974 - val_loss: 0.4196 - val_accuracy: 0.9250\n",
      "Epoch 638/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0135 - accuracy: 0.9974 - val_loss: 0.4194 - val_accuracy: 0.9250\n",
      "Epoch 639/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0140 - accuracy: 0.9990 - val_loss: 0.4193 - val_accuracy: 0.9250\n",
      "Epoch 640/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.4192 - val_accuracy: 0.9250\n",
      "Epoch 641/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0168 - accuracy: 0.9974 - val_loss: 0.4190 - val_accuracy: 0.9250\n",
      "Epoch 642/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 0.4189 - val_accuracy: 0.9262\n",
      "Epoch 643/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.4188 - val_accuracy: 0.9262\n",
      "Epoch 644/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.4187 - val_accuracy: 0.9250\n",
      "Epoch 645/700\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0198 - accuracy: 0.9969 - val_loss: 0.4187 - val_accuracy: 0.9250\n",
      "Epoch 646/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0155 - accuracy: 0.9990 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 647/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0143 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 648/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0133 - accuracy: 0.9974 - val_loss: 0.4186 - val_accuracy: 0.9250\n",
      "Epoch 649/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.4185 - val_accuracy: 0.9238\n",
      "Epoch 650/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9238\n",
      "Epoch 651/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 652/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 653/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 654/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 655/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0133 - accuracy: 0.9990 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 656/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0134 - accuracy: 0.9990 - val_loss: 0.4183 - val_accuracy: 0.9238\n",
      "Epoch 657/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0128 - accuracy: 0.9990 - val_loss: 0.4182 - val_accuracy: 0.9238\n",
      "Epoch 658/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0139 - accuracy: 0.9985 - val_loss: 0.4181 - val_accuracy: 0.9226\n",
      "Epoch 659/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0128 - accuracy: 0.9990 - val_loss: 0.4181 - val_accuracy: 0.9226\n",
      "Epoch 660/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0185 - accuracy: 0.9974 - val_loss: 0.4180 - val_accuracy: 0.9226\n",
      "Epoch 661/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 0.4180 - val_accuracy: 0.9226\n",
      "Epoch 662/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0136 - accuracy: 0.9980 - val_loss: 0.4180 - val_accuracy: 0.9226\n",
      "Epoch 663/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0212 - accuracy: 0.9974 - val_loss: 0.4179 - val_accuracy: 0.9226\n",
      "Epoch 664/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.4179 - val_accuracy: 0.9226\n",
      "Epoch 665/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0159 - accuracy: 0.9980 - val_loss: 0.4180 - val_accuracy: 0.9226\n",
      "Epoch 666/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0120 - accuracy: 0.9990 - val_loss: 0.4181 - val_accuracy: 0.9226\n",
      "Epoch 667/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0178 - accuracy: 0.9964 - val_loss: 0.4182 - val_accuracy: 0.9226\n",
      "Epoch 668/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0148 - accuracy: 0.9974 - val_loss: 0.4183 - val_accuracy: 0.9226\n",
      "Epoch 669/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.4183 - val_accuracy: 0.9226\n",
      "Epoch 670/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.4184 - val_accuracy: 0.9226\n",
      "Epoch 671/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.4185 - val_accuracy: 0.9226\n",
      "Epoch 672/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0141 - accuracy: 0.9964 - val_loss: 0.4186 - val_accuracy: 0.9226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.4187 - val_accuracy: 0.9226\n",
      "Epoch 674/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.4187 - val_accuracy: 0.9226\n",
      "Epoch 675/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.4188 - val_accuracy: 0.9226\n",
      "Epoch 676/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.4189 - val_accuracy: 0.9226\n",
      "Epoch 677/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 0.4190 - val_accuracy: 0.9226\n",
      "Epoch 678/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0139 - accuracy: 0.9974 - val_loss: 0.4192 - val_accuracy: 0.9226\n",
      "Epoch 679/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0165 - accuracy: 0.9974 - val_loss: 0.4192 - val_accuracy: 0.9226\n",
      "Epoch 680/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.4193 - val_accuracy: 0.9214\n",
      "Epoch 681/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.4193 - val_accuracy: 0.9214\n",
      "Epoch 682/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0143 - accuracy: 0.9974 - val_loss: 0.4193 - val_accuracy: 0.9214\n",
      "Epoch 683/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.4193 - val_accuracy: 0.9214\n",
      "Epoch 684/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.4192 - val_accuracy: 0.9214\n",
      "Epoch 685/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.4192 - val_accuracy: 0.9214\n",
      "Epoch 686/700\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.4192 - val_accuracy: 0.9214\n",
      "Epoch 687/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0189 - accuracy: 0.9974 - val_loss: 0.4190 - val_accuracy: 0.9214\n",
      "Epoch 688/700\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.4189 - val_accuracy: 0.9214\n",
      "Epoch 689/700\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0135 - accuracy: 0.9980 - val_loss: 0.4187 - val_accuracy: 0.9214\n",
      "Epoch 690/700\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0173 - accuracy: 0.9974 - val_loss: 0.4186 - val_accuracy: 0.9214\n",
      "Epoch 691/700\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0136 - accuracy: 0.9980 - val_loss: 0.4186 - val_accuracy: 0.9214\n",
      "Epoch 692/700\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0137 - accuracy: 0.9985 - val_loss: 0.4186 - val_accuracy: 0.9226\n",
      "Epoch 693/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0129 - accuracy: 0.9980 - val_loss: 0.4185 - val_accuracy: 0.9226\n",
      "Epoch 694/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0113 - accuracy: 0.9995 - val_loss: 0.4185 - val_accuracy: 0.9226\n",
      "Epoch 695/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0160 - accuracy: 0.9980 - val_loss: 0.4184 - val_accuracy: 0.9226\n",
      "Epoch 696/700\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.4183 - val_accuracy: 0.9226\n",
      "Epoch 697/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0160 - accuracy: 0.9974 - val_loss: 0.4184 - val_accuracy: 0.9226\n",
      "Epoch 698/700\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.4183 - val_accuracy: 0.9226\n",
      "Epoch 699/700\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.4183 - val_accuracy: 0.9226\n",
      "Epoch 700/700\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0158 - accuracy: 0.9974 - val_loss: 0.4183 - val_accuracy: 0.9226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_4,X_5],labels_rl,  #history使得训练结果可视化\n",
    "            batch_size=len(Y),\n",
    "            epochs=700, #400\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([test_0,test_1],labels_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_Net.save_weights('weights/coarse_heavy.h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rltest = DD_Net.predict([test_0,test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = ['Grab', 'Tap','Expand','Pinch', 'RC', 'RCC','SR', 'SL', 'SU', 'SD','SX', 'S+', 'SV', 'Shake']\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels_test[i])\n",
    "    \n",
    "y_pred_test = []\n",
    "for i in np.argmax(Y_pred_rltest,axis=1):\n",
    "    y_pred_test.append(labels_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_analysis(y_true,y_pred_test, 'images/rlSHREC_14_test3.png', labels_test, ymap=None, figsize=(12,12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
